[
  {
    "objectID": "present-work-2.html",
    "href": "present-work-2.html",
    "title": "Midterm Project II Proposal and Presentation",
    "section": "",
    "text": "The presentation order is determined by the time you sent me your member info (c(\"John\", \"Jeremy\", \"Praful\") is the first to let me know, followed by c(\"Sajjad\", \"Tanjina\", \"Dewan\"), etc) and random sampling as follows.\n\nteam_lst &lt;- list(c(\"John\", \"Jeremy\", \"Praful\"), \n                 c(\"Sajjad\", \"Tanjina\", \"Dewan\"),\n                 c(\"Ethan\", \"Navid\", \"Sylvester\"), \n                 c(\"Violet\", \"Vanessa\", \"Michele\"), \n                 c(\"Rakesh\", \"Daniel\", \"Jeremy\"), \n                 c(\"Sai\", \"Rohith\", \"Shristi\"))\nset.seed(2025)\nteam_lst[sample(1:6, 6)]\n\n[[1]]\n[1] \"Rakesh\" \"Daniel\" \"Jeremy\"\n\n[[2]]\n[1] \"Violet\"  \"Vanessa\" \"Michele\"\n\n[[3]]\n[1] \"Sai\"     \"Rohith\"  \"Shristi\"\n\n[[4]]\n[1] \"Sajjad\"  \"Tanjina\" \"Dewan\"  \n\n[[5]]\n[1] \"John\"   \"Jeremy\" \"Praful\"\n\n[[6]]\n[1] \"Ethan\"     \"Navid\"     \"Sylvester\"",
    "crumbs": [
      "Midterm Project II",
      "Topics and Works"
    ]
  },
  {
    "objectID": "present-work-2.html#presentation-order",
    "href": "present-work-2.html#presentation-order",
    "title": "Midterm Project II Proposal and Presentation",
    "section": "",
    "text": "The presentation order is determined by the time you sent me your member info (c(\"John\", \"Jeremy\", \"Praful\") is the first to let me know, followed by c(\"Sajjad\", \"Tanjina\", \"Dewan\"), etc) and random sampling as follows.\n\nteam_lst &lt;- list(c(\"John\", \"Jeremy\", \"Praful\"), \n                 c(\"Sajjad\", \"Tanjina\", \"Dewan\"),\n                 c(\"Ethan\", \"Navid\", \"Sylvester\"), \n                 c(\"Violet\", \"Vanessa\", \"Michele\"), \n                 c(\"Rakesh\", \"Daniel\", \"Jeremy\"), \n                 c(\"Sai\", \"Rohith\", \"Shristi\"))\nset.seed(2025)\nteam_lst[sample(1:6, 6)]\n\n[[1]]\n[1] \"Rakesh\" \"Daniel\" \"Jeremy\"\n\n[[2]]\n[1] \"Violet\"  \"Vanessa\" \"Michele\"\n\n[[3]]\n[1] \"Sai\"     \"Rohith\"  \"Shristi\"\n\n[[4]]\n[1] \"Sajjad\"  \"Tanjina\" \"Dewan\"  \n\n[[5]]\n[1] \"John\"   \"Jeremy\" \"Praful\"\n\n[[6]]\n[1] \"Ethan\"     \"Navid\"     \"Sylvester\"",
    "crumbs": [
      "Midterm Project II",
      "Topics and Works"
    ]
  },
  {
    "objectID": "present-work-2.html#project-materials",
    "href": "present-work-2.html#project-materials",
    "title": "Midterm Project II Proposal and Presentation",
    "section": "Project Materials",
    "text": "Project Materials\n\nGroup 1 (Rakesh, Daniel, Jeremy): proposal\nGroup 2 (Violet, Vanessa, Michele): proposal\nGroup 3 (Sai, Rohith, Shristi) : proposal\nGroup 4 (Sajjad, Tanjina, Dewan): proposal\nGroup 5 (John, Jeremy, Praful): proposal\nGroup 6 (Ethan, Navid, Sylvester): proposal",
    "crumbs": [
      "Midterm Project II",
      "Topics and Works"
    ]
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "Final Project Report Guidelines",
    "section": "",
    "text": "Please send me your entire work (written report, code, data, etc) by May 8, 2025 10 AM.\nYou receive 0 point if you miss the deadline.",
    "crumbs": [
      "Final Project",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "project-description.html#deadline",
    "href": "project-description.html#deadline",
    "title": "Final Project Report Guidelines",
    "section": "",
    "text": "Please send me your entire work (written report, code, data, etc) by May 8, 2025 10 AM.\nYou receive 0 point if you miss the deadline.",
    "crumbs": [
      "Final Project",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "project-description.html#project-writing",
    "href": "project-description.html#project-writing",
    "title": "Final Project Report Guidelines",
    "section": "Project Writing",
    "text": "Project Writing\nYour project can be in either of the following categories:\n\nData Analysis (DA) using one or more machine learning methods learned in class.\nIntroduce a new machine learning model/method/algorithm (ML) and compare it with the model/method/algorithms learned in class.\n\n\nStructure\nIf you choose to do DA, your paper should include the following sections:\n\nIntroduction: State why you think the questions you would like to answer are important or interesting, and why you think the method(s) you consider is an appropriate one to answer your questions.\nData: Describe the selected data set. Perform a thorough exploratory data analysis.\nAnalysis:\n\nExplain the chosen model/method.\nShow why the chosen model(s) is appropriate and better than others.\nAnswer your research questions by the analysis result.\n\nConclusion: Restate your research question, and summarize how you learn from data to answer your questions. What is the contribution of this project? Discuss any limitation of your model/method, and how it could be improved for better inference or prediction results.\nReferences/Bibliography: Include a detailed list of references, including papers, books, websites, code, and any idea/work that is not produced by yourself.\n\nIf you choose to do ML, your paper should include the following sections:\n\nIntroduction: State why you choose to learn this new method. Provide an overview and little history of the method. Describe the intuition and idea of the method. What are the pros and cons of the method?\nModel/Method: Provide the mathematical expression of the model. Explain the model and its properties, and how we do the supervised or unsupervised learning with the model.\nSimulation: Do a simulation study, and compare the chosen method with other methods learned in class. Determine which method performs better under what conditions.\nDiscussion: Based on the simulation results, discuss the advantages and disadvantages of the chosen method. Discuss any variants of the chosen method.\nReferences/Bibliography: Include a detailed list of references, including papers, books, websites, code, and any idea/work that is not produced by yourself.\n\n\n\nFormat and Layout\n\nYour project paper is saved as one PDF.\nYour paper should have your project title and your name on the first page. Date, Abstract, Keywords are optional.\nExcept the first title page, the margins should be no larger than one inch.\nExcept the project title and section title, the font size is 12 pt.\nPlease use 1.5 or double line spacing.\nYour report, including everything, should be at least 12 pages long, but no more than 15 pages.\nYour code should NOT be printed in the paper.\n\n\n\nCode\n\nYour code should be able to reproduce all the numerical results, outputs, tables, and figures shown in the report, including the source of the raw data (where you find and load the data) if the project is about data analysis.",
    "crumbs": [
      "Final Project",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "project-description.html#project-evaluation",
    "href": "project-description.html#project-evaluation",
    "title": "Final Project Report Guidelines",
    "section": "Project Evaluation",
    "text": "Project Evaluation\nYour project will be evaluated soley by Dr.Â Yu based on\n\nContent:\n\nThe quality of research question and relevancy of data to those questions? For example, the relationship between human height and weight is a BAD question. An elementary-school height and weight data set is a BAD data set.\nThe quality of the chosen model. For example, one-way ANOVA is a BAD model.\n\nCorrectness, Completeness and Complexity:\n\nAre machine learning methods carried out and explained correctly?\nDoes project include rigorous analysis and models? Simple linear regression model lacks complexity.\n\nWriting: What is the quality of the machine learning model/method presentation, visualization, writing, and explanations.\nFormat: Does the report follow the required format?\nCreativity and Critical Thought: Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\nReproducibility: Can your code reproduce what you show in the paper?\nReference: Do you cite others work properly?",
    "crumbs": [
      "Final Project",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "documents/project_slides/group5_Stats ML Project 1 Visualization.html",
    "href": "documents/project_slides/group5_Stats ML Project 1 Visualization.html",
    "title": "MSSC 6250 - Spring 2025",
    "section": "",
    "text": "import pandas as pd\n# Load Dataset\ndata = pd.read_csv(\"train.csv\")\n\n# Import necessary libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nimport numpy as np\n\n# Set plot style\nsns.set_style(\"whitegrid\")\n\n# 1. Distribution of SalePrice (Histogram + KDE)\nplt.figure(figsize=(10, 5))\nsns.histplot(data['SalePrice'], kde=True, bins=30)\nplt.title(\"Distribution of SalePrice\")\nplt.xlabel(\"SalePrice\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n# 2. Boxplot for detecting outliers in GrLivArea\nplt.figure(figsize=(10, 5))\nsns.boxplot(y=data['GrLivArea'])\nplt.title(\"Boxplot of GrLivArea\")\nplt.ylabel(\"GrLivArea\")\nplt.show()\n\n# 3. Scatterplot of GrLivArea vs. SalePrice\nplt.figure(figsize=(10, 5))\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=data)\nplt.title(\"Scatterplot of GrLivArea vs. SalePrice\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()\n\n# 4. Boxplot for Neighborhood vs. SalePrice\nplt.figure(figsize=(12, 6))\nsns.boxplot(x='Neighborhood', y='SalePrice', data=data)\nplt.xticks(rotation=45)\nplt.title(\"SalePrice Distribution Across Neighborhoods\")\nplt.xlabel(\"Neighborhood\")\nplt.ylabel(\"SalePrice\")\nplt.show()\n\n# 5. Correlation Heatmap\n# Increase the figure size \nplt.figure(figsize=(20, 15))  \n\n# Generate the heatmap with larger font size for annotations\nsns.heatmap(data.select_dtypes(include=[np.number]).corr(), \n            cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.5, \n            annot_kws={\"size\": 10})  # Increase annotation size\n\nplt.title(\"Correlation Heatmap\", fontsize=16)\nplt.xticks(fontsize=12, rotation=45)  # Rotate x-axis labels \nplt.yticks(fontsize=12)  # Adjust y-axis font size\n\nplt.show()\n\n\n# 6. Pairplot for Selected Features\nselected_features = ['SalePrice', 'TotalBsmtSF', 'GrLivArea', 'LotArea']\nsns.pairplot(data[selected_features])\nplt.show()\n\n# 7. Line Plot for SalePrice over Years Sold\nplt.figure(figsize=(10, 5))\nsns.lineplot(x='YrSold', y='SalePrice', data=data)\nplt.title(\"Average SalePrice Over Time\")\nplt.xlabel(\"Year Sold\")\nplt.ylabel(\"SalePrice\")\nplt.show()\n\n# 8. ANOVA Test (Checking if Neighborhood affects SalePrice)\nanova_result = stats.f_oneway(\n    data[data['Neighborhood'] == 'NAmes']['SalePrice'],\n    data[data['Neighborhood'] == 'CollgCr']['SalePrice'],\n    data[data['Neighborhood'] == 'OldTown']['SalePrice']\n)\n\n# 9. Chi-Square Test (Dependency between HouseStyle and SaleCondition)\nchi2_stat, p_value, _, _ = stats.chi2_contingency(pd.crosstab(data['HouseStyle'], data['SaleCondition']))\n\nanova_result, chi2_stat, p_value\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(F_onewayResult(statistic=95.28410630774302, pvalue=1.2519558385662742e-35),\n 88.43116541471059,\n 1.626510734741343e-06)\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Select only numeric features\nnumeric_data = data.select_dtypes(include=['number'])\n\n# Compute correlation with SalePrice\ncorr_matrix = numeric_data.corr()['SalePrice'].sort_values(ascending=False)\n\n# Plot the top 10 correlated features\ntop_features = corr_matrix[1:11]  # Exclude SalePrice itself\n\nplt.figure(figsize=(10, 5))\ntop_features.plot(kind='bar', color='royalblue')\nplt.title(\"Top 10 Features Correlated with Sale Price\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Correlation with SalePrice\")\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "slides/02-overview.html#imageobject-recognition",
    "href": "slides/02-overview.html#imageobject-recognition",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Image/Object Recognition",
    "text": "Image/Object Recognition\n\n\n\n\n\n\nMachine learning is everywhere.\nCan we train a machine to have an ability to recognize dogs and cats?\nhundreds of thousands of objects?"
  },
  {
    "objectID": "slides/02-overview.html#recommender-system",
    "href": "slides/02-overview.html#recommender-system",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Recommender System",
    "text": "Recommender System\n\n\nwatch one video on YouTube, then lots of other videos that never show up pop up.\nCheck some items on Amazon, then it recommends some other items saying âyou may also like XXXâ\nThis kind of recommendation relies on machine learning.\nHow does the model know you may also like those items. Well the model learn this from other customer surfing and purchasing history."
  },
  {
    "objectID": "slides/02-overview.html#covid-detection",
    "href": "slides/02-overview.html#covid-detection",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "COVID Detection",
    "text": "COVID Detection"
  },
  {
    "objectID": "slides/02-overview.html#stock-price-forecasting",
    "href": "slides/02-overview.html#stock-price-forecasting",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Stock Price Forecasting",
    "text": "Stock Price Forecasting"
  },
  {
    "objectID": "slides/02-overview.html#risk-factors-for-cancer",
    "href": "slides/02-overview.html#risk-factors-for-cancer",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Risk Factors for Cancer",
    "text": "Risk Factors for Cancer"
  },
  {
    "objectID": "slides/02-overview.html#how-we-solve-real-life-problems",
    "href": "slides/02-overview.html#how-we-solve-real-life-problems",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "How We Solve Real-Life Problems",
    "text": "How We Solve Real-Life Problems\n\nWe describe and formulate our problems/questions by models.\n\n\n\nSolve problems/Answer questions by doing inference and/or predictions from the built model using the information from the data and computer algorithms.\n\n\n\n\nSo what is (Statistical) Machine Learning?\n\n\nMathematical/statistical or machine learning models.\nwe know the eating habits from a data of 100 american people. How do we know the the eating habits of all the people in the united states.\nIf we know someoneâs GPA and major, how can we predict his salary after graduation?"
  },
  {
    "objectID": "slides/02-overview.html#machine-learning-vs.-statistical-learning",
    "href": "slides/02-overview.html#machine-learning-vs.-statistical-learning",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Machine Learning vs.Â Statistical Learning",
    "text": "Machine Learning vs.Â Statistical Learning\n\n\nMachine learning (ML) is a field in Computer Science (CS).\n\n\nâA computer program is said to learn from experience (E) with respect to some class of tasks (T) and performance measure (P) if its performance at tasks in T, as measured by P, improves with experience E.â â Tom Mitchell, Professor of ML at Carnegie Mellon University\n\n\ncomputer programs = computing software and system\nexperience = data (objective and/or subjective)\ntasks = problems being solved by the computing system\n\n\n\nThe ML algorithms are mainly for predictive modeling problems, and many have been borrowed from Statistics, for example, linear regression.\nML is a CS perspective on modeling data with a focus on algorithmic methods.\n\n\n\n\nWait, I am a statistical modeler (computational statistician), and this is exactly what I am doing!\n\nMachine Learning is the study of computer algorithms that improve automatically through experience."
  },
  {
    "objectID": "slides/02-overview.html#machine-learning-vs.-statistical-learning-1",
    "href": "slides/02-overview.html#machine-learning-vs.-statistical-learning-1",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Machine Learning vs.Â Statistical Learning",
    "text": "Machine Learning vs.Â Statistical Learning\n\n\nStatistical learning, used to be called Applied Statistics (what a âfancyâ name, ð¤£), arose as a subfield of Statistics.\n\n\n\nStatistical learning refers to a set of tools for modeling and understanding complex datasets. â An Introduction to Statistical Learning\n\n\nâ¦to extract important patterns and trends, and understand âwhat the data says.â We call this learning from data. â The Elements of Statistical Learning\n\n\ntools = mathematics, computing hardware/software/architecture, programming languages, algorithms, etc.\nStatistical learning is a mathematical perspective on modeling data with a focus on goodness of fit.\n\nhttps://machinelearningmastery.com/relationship-between-applied-statistics-and-machine-learning/"
  },
  {
    "objectID": "slides/02-overview.html#machine-learning-vs.-statistical-learning-2",
    "href": "slides/02-overview.html#machine-learning-vs.-statistical-learning-2",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Machine Learning vs.Â Statistical Learning",
    "text": "Machine Learning vs.Â Statistical Learning\n\nMachine learning emphasizes algorithms and automation.\nStatistical learning emphasizes modeling, interpretability, and uncertainty.\n\n\n\nThe distinction is blur:\n\nA machine learner needs a well-built statistical model that quantifies uncertainty about prediction\nA statistical learner needs a computationally efficient algorithm to deal with large complex data.\n\n\n\n\n\n\nBUT, one thing for sure. Machine learning has the upper hand in marketing!"
  },
  {
    "objectID": "slides/02-overview.html#fancy-terms-and-larger-grant",
    "href": "slides/02-overview.html#fancy-terms-and-larger-grant",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Fancy Terms and Larger Grant!",
    "text": "Fancy Terms and Larger Grant!\n\nSource: http://statweb.stanford.edu/~tibs/stat315a/glossary.pdf"
  },
  {
    "objectID": "slides/02-overview.html#section-1",
    "href": "slides/02-overview.html#section-1",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "",
    "text": "Source: All of Statistics"
  },
  {
    "objectID": "slides/02-overview.html#types-of-learning",
    "href": "slides/02-overview.html#types-of-learning",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Types of Learning",
    "text": "Types of Learning\n\nSource: https://towardsdatascience.com/machine-learning-algorithms-in-laymans-terms-part-1-d0368d769a7b"
  },
  {
    "objectID": "slides/02-overview.html#supervised-learning",
    "href": "slides/02-overview.html#supervised-learning",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nResponse Y (output, outcome, target, label, dependent/endogenous variable)\nVector of p predictors X = (X_1, X_2, \\dots, X_p) (inputs, features, regressors, covariates, explanatory/exogenous/independent variable).\n\n\n\nRegression: Y is numeric (e.g price, blood pressure). (if p = 1, simple regression; if p &gt; 1, multiple regression)\nClassification: Y is categorical (e.g survived/died, digit 0-9 (MNIST), cancer class of tissue sample).\n\n\n\n\nTraining data \\mathcal{D} = \\{(x_1, y_1), \\dots ,(x_n , y_n)\\} =\\{(x_i, y_i)\\}_{i=1}^n, x_i \\in \\mathbf{R}^p, y_i \\in \\mathbf{R}^d.\n\n\n\nGoal: Use training data (E) to train our model for better (w.r.t. P) inference/prediction (T) on the response.\n\nLearn a mapping from inputs to outputs Y = f(X)."
  },
  {
    "objectID": "slides/02-overview.html#regression-example",
    "href": "slides/02-overview.html#regression-example",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Regression Example",
    "text": "Regression Example\n\nGoal: Establish the relationship between salary and demographic variables.\n\n\nSource: ISL Fig. 1.1"
  },
  {
    "objectID": "slides/02-overview.html#statistics-nah-machine-learning-neat",
    "href": "slides/02-overview.html#statistics-nah-machine-learning-neat",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Statistics Nahâ¦ Machine Learning Neat!",
    "text": "Statistics Nahâ¦ Machine Learning Neat!\n\nSource: https://towardsdatascience.com/the-actual-difference-between-statistics-and-machine-learning-64b49f07ea3"
  },
  {
    "objectID": "slides/02-overview.html#statistics-nah-machine-learning-neat-1",
    "href": "slides/02-overview.html#statistics-nah-machine-learning-neat-1",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Statistics Nahâ¦ Machine Learning Neat!",
    "text": "Statistics Nahâ¦ Machine Learning Neat!"
  },
  {
    "objectID": "slides/02-overview.html#classification-example",
    "href": "slides/02-overview.html#classification-example",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Classification Example",
    "text": "Classification Example\n\nGoal: Build a customized spam filtering system\n\n\nSource: http://penplusbytes.org/strategies-for-dealing-with-e-mail-spam/"
  },
  {
    "objectID": "slides/02-overview.html#classification-example-1",
    "href": "slides/02-overview.html#classification-example-1",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Classification Example",
    "text": "Classification Example\n\n\n\n\nSource: https://www.yesware.com/blog/email-spam/\n\n\n\n\nData: 4601 emails sent to George at HP, before 2000. Each is labeled as spam or email.\nInputs: relative frequencies of 57 of the commonly occurring words and punctuation marks.\n\n\n\n\n\nSource: ESL Tbl. 1.1"
  },
  {
    "objectID": "slides/02-overview.html#objectives-of-supervised-learning",
    "href": "slides/02-overview.html#objectives-of-supervised-learning",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Objectives of Supervised Learning",
    "text": "Objectives of Supervised Learning\nBased on the training data (E) weâd like to :\n\n(T) Prediction: Accurately predict unseen test cases.\n\nGiven a new test input (age, year, education), what is predicted salary level?\nGiven a new bunch of words in the email, the email is a spam or normal message?\n\n\n\n\n\n(T) Inference Understand which inputs affect the outcome, and how.\n\nIf education is up one level, how much salary will increase on average?\nIf â!â increases one more time, how much percentage does the probability of being labeled as spam go up?\n\n\n\n\n\n\n(P) Assess the quality of our predictions and inferences.\n\nUse evalution metrics to assess the performance of a machine learning model/algorithm."
  },
  {
    "objectID": "slides/02-overview.html#unsupervised-learning",
    "href": "slides/02-overview.html#unsupervised-learning",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nNo outcome variable, just a set of features measured on a set of samples.\nOur training sample is \\mathcal{D} = \\{x_1, \\dots, x_n\\}, x_i \\in \\mathbf{R}^p.\n\n\n\nObjective is more fuzzy\n\nfind groups of samples or features that behave similarly (Clustering)\n\nfind linear combinations of features with the most variation (Dimension Reduction).\n\n\n\n\n\n\nDifficult to know how well you are doing.\nCan be useful as a pre-processing step for supervised learning."
  },
  {
    "objectID": "slides/02-overview.html#clustering-example",
    "href": "slides/02-overview.html#clustering-example",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Clustering Example",
    "text": "Clustering Example\n\nCustomer Segmentation: dividing customers into groups or clusters on the basis of common characteristics.\n\n\nSource: https://www.datacamp.com/community/tutorials/introduction-customer-segmentation-python"
  },
  {
    "objectID": "slides/02-overview.html#deep-learning",
    "href": "slides/02-overview.html#deep-learning",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Deep Learning",
    "text": "Deep Learning\n\nAn (artifical) neural network is a machine learning model inspired by the biological neural networks that constitute animal brains.\n\n\n\n\n\nSource: Wiki"
  },
  {
    "objectID": "slides/02-overview.html#section-2",
    "href": "slides/02-overview.html#section-2",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "",
    "text": "A neural network with several hidden layers is called a deep neural network, or deep learning.\n\nSource: ISL Ch 10"
  },
  {
    "objectID": "slides/02-overview.html#accuracy-and-interpretability-trade-off",
    "href": "slides/02-overview.html#accuracy-and-interpretability-trade-off",
    "title": "Overview of Statistical Machine Learning ð»",
    "section": "Accuracy and Interpretability Trade-Off",
    "text": "Accuracy and Interpretability Trade-Off\n\n\n\n\nSource: ISL Fig. 2.7\n\nflexibility: a model is flexible if the model performs quite good for various types of data.\n\n\n\n\nmssc6250-s25.github.io/website"
  },
  {
    "objectID": "slides/12-gp.html#univariate-guassian",
    "href": "slides/12-gp.html#univariate-guassian",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Univariate Guassian",
    "text": "Univariate Guassian\n\n\\(Y \\sim N(\\mu, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/12-gp.html#multivariate-guassian",
    "href": "slides/12-gp.html#multivariate-guassian",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Multivariate Guassian",
    "text": "Multivariate Guassian\n\nâMultivariateâ \\(=\\) two or more random variables\n\\(\\mathbf{Y}\\in \\mathbb{R}^d \\sim N_d\\left(\\boldsymbol \\mu, \\boldsymbol \\Sigma\\right)\\)\n\n\n\nBivariate Gaussian (\\(d=2\\)):\n\n\\[\\mathbf{Y}= \\begin{pmatrix} Y_1 \\\\ Y_2 \\end{pmatrix}\\] \\[\\boldsymbol \\mu= \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}\\] \\[\\boldsymbol \\Sigma= \\begin{pmatrix} \\sigma_1^2  & \\rho_{12}\\sigma_1\\sigma_2\\\\ \\rho_{12}\\sigma_1\\sigma_2 & \\sigma_2^2 \\end{pmatrix}\\]"
  },
  {
    "objectID": "slides/12-gp.html#section-1",
    "href": "slides/12-gp.html#section-1",
    "title": "Gaussian Processes â¾ï¸",
    "section": "",
    "text": "\\[\\boldsymbol \\mu= \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\] \\[\\boldsymbol \\Sigma= \\begin{pmatrix}1 & 0\\\\ 0 & 1 \\end{pmatrix}\\]"
  },
  {
    "objectID": "slides/12-gp.html#section-2",
    "href": "slides/12-gp.html#section-2",
    "title": "Gaussian Processes â¾ï¸",
    "section": "",
    "text": "\\[\\boldsymbol \\mu= \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\] \\[\\boldsymbol \\Sigma= \\begin{pmatrix}1 & 0\\\\ 0 & 0.2 \\end{pmatrix}\\]"
  },
  {
    "objectID": "slides/12-gp.html#section-3",
    "href": "slides/12-gp.html#section-3",
    "title": "Gaussian Processes â¾ï¸",
    "section": "",
    "text": "\\[\\boldsymbol \\mu= \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\] \\[\\boldsymbol \\Sigma= \\begin{pmatrix}1 & 0.9\\\\ 0.9 & 1 \\end{pmatrix}\\]"
  },
  {
    "objectID": "slides/12-gp.html#three-or-more-variables",
    "href": "slides/12-gp.html#three-or-more-variables",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Three or More Variables",
    "text": "Three or More Variables\n\nHard to visualize in dimensions \\(&gt; 2\\), so stack points next to each other.\n\n\ny1 and y2 are highly correlated, y1 and y2 have similar values, and so when we put them next to each other, the lines do not go up or down that much. Instead, the lines are quite horizontal."
  },
  {
    "objectID": "slides/12-gp.html#section-4",
    "href": "slides/12-gp.html#section-4",
    "title": "Gaussian Processes â¾ï¸",
    "section": "",
    "text": "\\(d = 5\\)\n\n\\[\\boldsymbol \\mu= \\begin{pmatrix} 0 \\\\ 0 \\\\ 0\\\\0\\\\0\\end{pmatrix} \\quad \\quad \\boldsymbol \\Sigma= \\begin{pmatrix}1 & 0.99 & 0.98 & 0.97 & 0.96\\\\ 0.99 & 1 & 0.99 & 0.98 & 0.97 \\\\ 0.98 & 0.99 & 1 & 0.99 & 0.98 \\\\ 0.97 & 0.98 & 0.97 & 1 & 0.99\\\\0.96 & 0.97 & 0.98 & 0.99 & 1\n\\end{pmatrix}\\]\n\n\nEach line is one sample (path)."
  },
  {
    "objectID": "slides/12-gp.html#section-5",
    "href": "slides/12-gp.html#section-5",
    "title": "Gaussian Processes â¾ï¸",
    "section": "",
    "text": "\\(d = 50\\)\n\n\\[\\boldsymbol \\mu= \\begin{pmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\0\\\\0\\end{pmatrix} \\quad \\quad \\boldsymbol \\Sigma= \\begin{pmatrix}\n1 & 0.99 & 0.98 & 0.97 & 0.96 & \\cdots \\\\\n0.99 & 1 & 0.99 & 0.98 & 0.97 & \\cdots\\\\\n0.98 & 0.99 & 1 & 0.99 & 0.98 & \\cdots\\\\\n0.97 & 0.98 & 0.97 & 1 & 0.99 & \\cdots\\\\\n0.96 & 0.97 & 0.98 & 0.99 & 1 & \\cdots \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots &  \\vdots &\n\\end{pmatrix}\\]\n\n\nEach line is one sample (path).\n\nThink of Gaussian processes as an infinite dimensional distribution over functions\n\nall we need to do is change notation"
  },
  {
    "objectID": "slides/12-gp.html#gaussian-processes-1",
    "href": "slides/12-gp.html#gaussian-processes-1",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Gaussian Processes",
    "text": "Gaussian Processes\n\nA stochastic process \\(f(x), x \\in \\mathcal{X} \\subset \\mathbb{R}^D\\), is a function whose values are random variables, for any value of \\(x\\).\nUsually for \\(D = 1\\), the process is a temporal process, and for \\(D &gt; 1\\), it is referred to as a spatial process.\n\n\n\nA Gaussian process (GP) is a process where all finite-dimensional distributions are multivariate Gaussian, for any choice of \\(n\\) and \\(x_1\\dots, x_n \\in \\mathbb{R}^D\\):\n\n\\[f(x_1), \\dots, f(x_n) \\sim N_n\\left(\\boldsymbol \\mu, \\boldsymbol \\Sigma\\right)\\]\n\nWrite \\(f(\\cdot) \\sim GP\\) to denote that the function \\(f()\\) is a GP."
  },
  {
    "objectID": "slides/12-gp.html#mean-and-covariance-function",
    "href": "slides/12-gp.html#mean-and-covariance-function",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Mean and Covariance Function",
    "text": "Mean and Covariance Function\n\nTo fully specify a Gaussian distribution we need the mean and covariance, \\(Y \\sim N(\\mu, \\Sigma)\\)\n\n\n\n\nTo fully specify a Gaussian process we need the mean and covariance function, \\[f(\\cdot) \\sim GP\\left(m(\\cdot), k(\\cdot, \\cdot)\\right)\\] where\n\n\\[m(x) = \\text{E}(f(x))\\] \\[k(x, x') = \\text{Cov}(f(x), f(x'))\\]\n\n\n\nPopular choices of \\(m(\\cdot)\\) are \\(m(x) = 0\\) or \\(m(x) = \\text{const}\\) for all \\(x\\), or \\(m(x) = \\beta'x\\)\nCare more about the covariance function or kernel function as it governs the how the process looks like by defining the similarity between data points."
  },
  {
    "objectID": "slides/12-gp.html#covariance-function-kernel",
    "href": "slides/12-gp.html#covariance-function-kernel",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Covariance Function Kernel",
    "text": "Covariance Function Kernel\n\\[\\text{Cov}(f(x), f(x')) = k(x, x')\\]\n\n\n\\(k(x, x')\\) must be a positive semi-definite function, leading to valid covariance matrices\n\nGiven locations \\(x_1, \\dots, x_n\\), the \\(n \\times n\\) Gram matrix \\(K\\) with \\(K_{ij} = k(x_i, x_j)\\) must be a positive semi-definite matrix.\n\n\n\n\n\n\n\nOften assume \\(k(x, x')\\) is a function of only the distance between locations: \\[\\text{Cov}(f(x), f(x')) = k(\\|x-x'\\|) = k(r)\\]\n\nthe GP is a stationary process.\nthe covariance function is isotropic.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe covariance function determines the nature of the GP, hypothesis space of functions."
  },
  {
    "objectID": "slides/12-gp.html#squared-exponential-se-kernel",
    "href": "slides/12-gp.html#squared-exponential-se-kernel",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Squared Exponential (SE) Kernel",
    "text": "Squared Exponential (SE) Kernel\n\\[k(x, x' \\mid \\tau, h) = \\tau^2 \\exp\\left(-\\frac{(x - x')^2}{2h^2} \\right)\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[k(x, x') = \\exp\\left(-\\frac{1}{2}(x - x')^2 \\right)\\]"
  },
  {
    "objectID": "slides/12-gp.html#squared-exponential-se-kernel-1",
    "href": "slides/12-gp.html#squared-exponential-se-kernel-1",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Squared Exponential (SE) Kernel",
    "text": "Squared Exponential (SE) Kernel\n\n\n\n\n\n\n\n\n\n\n\n\\[k(x, x') = \\exp\\left(-\\frac{1}{2}\\frac{(x - x')^2}{ {\\color{red}{0.25}}^2} \\right)\\]\n\nThe parameter \\(h\\) is the characteristic length-scale that controls the number of level-zero upcrossings."
  },
  {
    "objectID": "slides/12-gp.html#squared-exponential-se-kernel-2",
    "href": "slides/12-gp.html#squared-exponential-se-kernel-2",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Squared Exponential (SE) Kernel",
    "text": "Squared Exponential (SE) Kernel\n\n\n\n\n\n\n\n\n\n\n\n\\[k(x, x') = \\exp\\left(-\\frac{1}{2}\\frac{(x - x')^2}{ {\\color{red}{4}}^2} \\right)\\]"
  },
  {
    "objectID": "slides/12-gp.html#squared-exponential-se-kernel-3",
    "href": "slides/12-gp.html#squared-exponential-se-kernel-3",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Squared Exponential (SE) Kernel",
    "text": "Squared Exponential (SE) Kernel\n\n\n\n\n\n\n\n\n\n\n\n\\[k(x, x') = {\\color{red}{10^2}}\\exp\\left(-\\frac{1}{2}(x - x')^2 \\right)\\]\n\nThe parameter \\(\\tau\\) is the variance of \\(f(x)\\) that controls the vertical variation of the process."
  },
  {
    "objectID": "slides/12-gp.html#exponential-kernel",
    "href": "slides/12-gp.html#exponential-kernel",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Exponential Kernel",
    "text": "Exponential Kernel\n\\[k(x, x' \\mid \\tau, h) = \\tau^2 \\exp\\left(-\\frac{|x - x'|}{h} \\right)\\]"
  },
  {
    "objectID": "slides/12-gp.html#brownian-motion",
    "href": "slides/12-gp.html#brownian-motion",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Brownian Motion",
    "text": "Brownian Motion\n\\[k(x, x') = \\min(x, x')\\]"
  },
  {
    "objectID": "slides/12-gp.html#white-noise",
    "href": "slides/12-gp.html#white-noise",
    "title": "Gaussian Processes â¾ï¸",
    "section": "White Noise",
    "text": "White Noise\n\\[k(x, x') = \\begin{cases} 1 & \\quad \\text{if } x = x' \\\\\n0 & \\quad \\text{otherwise}\n\\end{cases}\\]"
  },
  {
    "objectID": "slides/12-gp.html#why-gp",
    "href": "slides/12-gp.html#why-gp",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Why GP?",
    "text": "Why GP?\n\nThe GP inherits its properties primarily from the covariance function \\(k\\):\n\nSmoothness\nDifferentiability\nVariance\n\n\n\n\n\nSums of Gaussians are Gaussian.\n\n\n\n\nMarginal distributions of multivariate Gaussians are still Gaussian.\n\n\n\n\nAny affine transformation of a Gaussian is a Gaussian.\n\n\n\n\nConditional distributions are still Gaussian.\n\n\n\\[\\mathbf{Y}= \\begin{pmatrix} \\mathbf{Y}_1 \\\\ \\mathbf{Y}_2 \\end{pmatrix} \\sim N\\left(\\boldsymbol \\mu,  \\boldsymbol \\Sigma\\right), \\quad \\boldsymbol \\mu= \\begin{pmatrix} \\boldsymbol \\mu_1 \\\\ \\boldsymbol \\mu_2 \\end{pmatrix},  \\quad \\boldsymbol \\Sigma= \\begin{pmatrix} \\Sigma_{11}  & \\Sigma_{12}\\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}\\] \\[(\\mathbf{Y}_2 \\mid \\mathbf{y}_1 = \\mathbf{y}_1) \\sim N\\left(\\boldsymbol \\mu_2 + \\Sigma_{21} \\Sigma_{11}^{-1} (\\mathbf{y}_1 - \\boldsymbol \\mu_1), \\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12}\\right)\\]"
  },
  {
    "objectID": "slides/12-gp.html#conditional-updates-of-gaussian-processes",
    "href": "slides/12-gp.html#conditional-updates-of-gaussian-processes",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Conditional Updates of Gaussian Processes",
    "text": "Conditional Updates of Gaussian Processes\n\nIf \\(f(\\cdot) \\sim GP\\),\n\n\\[f(x_1), \\dots, f(x_n), f(x^*) \\sim N(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\\]\n\nIf we observe its value at \\(x_1, \\dots, x_n\\), then\n\n\\[f(x^*) \\mid f(x_1), \\dots, f(x_n) \\sim N(\\boldsymbol \\mu^*, \\boldsymbol \\Sigma^*)\\] where \\(\\boldsymbol \\mu^*\\) and \\(\\boldsymbol \\Sigma^*\\) are as on the previous slide.\n\nWe still believe \\(f\\) is a GP even weâve observed its value at a number of locations."
  },
  {
    "objectID": "slides/12-gp.html#bayesian-conditioning-updates-of-gp-prior",
    "href": "slides/12-gp.html#bayesian-conditioning-updates-of-gp-prior",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Bayesian Conditioning Updates of GP: Prior",
    "text": "Bayesian Conditioning Updates of GP: Prior\n\nInstead of assigning priors to parameters in the regression function, we assign a function prior to the regression function:\n\n\\[f(\\cdot) \\sim GP(0, k(\\cdot, \\cdot))\\]\n\nFor any points \\(x_1, \\dots, x_n, x^*\\),\n\n\\[f(x_1), \\dots, f(x_n), f(x^*) \\sim N\\left(0, \\boldsymbol \\Sigma\\right)\\]\n\\[\\boldsymbol \\Sigma= \\left(\\begin{array}{ccc|c}\nk(x_1, x_1) & \\cdots & k(x_1, x_n) & k(x_1, x^*) \\\\\n\\vdots &  & \\vdots & \\vdots \\\\\nk(x_n, x_1) & \\cdots & k(x_n, x_n) & k(x_n, x^*) \\\\\n\\hline\nk(x^*, x_1) & \\cdots & k(x^*, x_n) & k(x^*, x^*)\n\\end{array} \\right) =\n\\left(\\begin{array}{ccc|c}\n&  &  &  \\\\\n& K &  & K_* \\\\\n&  &  &  \\\\\n\\hline\n& K_*^T &  & K_{**}\n\\end{array} \\right) \\]"
  },
  {
    "objectID": "slides/12-gp.html#bayesian-conditioning-updates-of-gp-posterior",
    "href": "slides/12-gp.html#bayesian-conditioning-updates-of-gp-posterior",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Bayesian Conditioning Updates of GP: Posterior",
    "text": "Bayesian Conditioning Updates of GP: Posterior\n\nGiven observed information \\(f(x_1), \\dots, f(x_n)\\)\n\n\n\\[f(x^*) \\mid f(x_1), \\dots, f(x_n) \\sim N\\left( \\boldsymbol \\mu^*, \\boldsymbol \\Sigma^*\\right)\\] where\n\\[\\boldsymbol \\mu^* = K_{*}^TK^{-1}{\\bf f}\\] with \\({\\bf f} = \\left(f(x_1), \\dots, f(x_n)\\right)^T\\)\n\\[\\boldsymbol \\Sigma^* = K_{**} - K_{*}^TK^{-1}K_{*}\\]"
  },
  {
    "objectID": "slides/12-gp.html#no-noisenugget---interpolation",
    "href": "slides/12-gp.html#no-noisenugget---interpolation",
    "title": "Gaussian Processes â¾ï¸",
    "section": "No Noise/Nugget - Interpolation",
    "text": "No Noise/Nugget - Interpolation\n\n\nââ: posterior mean \\(\\boldsymbol \\mu^*\\)\nââ: 95% posterior predictive interval \\(\\boldsymbol \\mu^* \\pm 1.96 \\boldsymbol \\Sigma^*\\)\nSo far we treat \\(f(x)\\), the function values as data points.\nThere is no noise, and every posterior curve interpolates data points."
  },
  {
    "objectID": "slides/12-gp.html#noisy-observationswith-nugget---gp-regression-gpr",
    "href": "slides/12-gp.html#noisy-observationswith-nugget---gp-regression-gpr",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Noisy Observations/with Nugget - GP Regression (GPR)",
    "text": "Noisy Observations/with Nugget - GP Regression (GPR)\n\nIn reality, we donât or canât observe \\(f(x)\\) and like to estimate it.\n\n\\[\\begin{align} y_i &= f(x_i) + \\epsilon_i, \\quad \\epsilon_i\\sim N(0, \\sigma^2) \\\\\nf(\\cdot) &\\sim GP(0, k(\\cdot, \\cdot; \\theta))\\end{align}\\]\n\n\\[y_1, \\dots, y_n, f(x^*) \\sim N\\left(0, \\boldsymbol \\Sigma\\right), \\quad \\boldsymbol \\Sigma=\n\\left(\\begin{array}{ccc|c}\n&  &  &  \\\\\n& K + \\sigma^2I &  & K_* \\\\\n&  &  &  \\\\\n\\hline\n& K_*^T &  & K_{**}\n\\end{array} \\right) \\]\n\n\n\\[f(x^*) \\mid y_1, \\dots, y_n \\sim N\\left( \\boldsymbol \\mu^*, \\boldsymbol \\Sigma^*\\right), \\quad \\boldsymbol \\mu^* = K_{*}^T{\\color{red}{(K + \\sigma^2I)^{-1}}}{\\bf y}, \\quad \\boldsymbol \\Sigma^* = K_{**} - K_{*}^T{\\color{red}{(K + \\sigma^2I)^{-1}}}K_{*}\\] with \\({\\bf y} = \\left(y_1, \\dots, y_n\\right)^T\\)"
  },
  {
    "objectID": "slides/12-gp.html#noisy-observationswith-nugget---gp-regression-gpr-1",
    "href": "slides/12-gp.html#noisy-observationswith-nugget---gp-regression-gpr-1",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Noisy Observations/with Nugget - GP Regression (GPR)",
    "text": "Noisy Observations/with Nugget - GP Regression (GPR)"
  },
  {
    "objectID": "slides/12-gp.html#hyperparameter-tuning",
    "href": "slides/12-gp.html#hyperparameter-tuning",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning"
  },
  {
    "objectID": "slides/12-gp.html#empirical-bayes",
    "href": "slides/12-gp.html#empirical-bayes",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Empirical Bayes",
    "text": "Empirical Bayes\n\nUses the observed data to estimate parameters \\(\\theta = (\\sigma^2, \\tau^2, h)\\)\nFind an empirical Bayes estimate for \\(\\theta\\) from the marginal likelihood\n\n\\[ p\\left( \\mathbf{y}\\mid \\theta \\right) = \\int p\\left( \\mathbf{y}\\mid {\\bf f} \\right) p\\left( {\\bf f} \\mid \\theta \\right) \\, d {\\bf f} = N\\left(\\mathbf{0}, K(\\tau, h)+\\sigma^2I \\right)\\]\n\n\n\\(\\hat{\\theta}_{EB} = \\mathop{\\mathrm{arg\\,max}}_{\\theta}\\log p\\left( \\mathbf{y}\\mid \\theta \\right)\\)."
  },
  {
    "objectID": "slides/12-gp.html#full-bayesian-inference",
    "href": "slides/12-gp.html#full-bayesian-inference",
    "title": "Gaussian Processes â¾ï¸",
    "section": "Full Bayesian Inference",
    "text": "Full Bayesian Inference\n\\[\\begin{align}\ny_i &= f(x_i) + \\epsilon_i, \\, \\, \\epsilon_i \\stackrel{\\rm iid}{\\sim} N(0, \\sigma^2), \\quad i = 1, \\dots, n,\\\\\nf(\\cdot) &\\sim GP\\left(\\mu, k(\\cdot, \\cdot)\\right),\\,\\, \\text{Cov}(f(x_i), f(x_j)) = k(x_i, x_j)\\\\\n\\sigma^2 &\\sim IG(a_{\\sigma}, b_{\\sigma})\\\\\n\\tau^2 &\\sim IG(a_{\\tau}, b_{\\tau})\\\\\nh &\\sim Ga(a_{h}, b_{h})\\\\\n\\mu & \\sim N(0, b_{\\mu})\n\\end{align}\\]\n\nThe model is Gibbsable, or the Metropolis-Hastings algorithm can be used when \\({\\bf f}\\) is integrated out."
  },
  {
    "objectID": "slides/08-bayes.html#thinking-like-a-bayesian",
    "href": "slides/08-bayes.html#thinking-like-a-bayesian",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Thinking like a Bayesian",
    "text": "Thinking like a Bayesian\n\n\nWhen flipping a fair coin, we say that âthe probability of flipping Heads is 0.5.â How do you interpret this probability?\n\nIf I flip this coin over and over, roughly 50% will be Heads.\nHeads and Tails are equally plausible.\nBoth a. and b. make sense.\n\n\n\n\n\n\nAn election is coming up and a pollster claims that candidate Yu has a 0.9 probability of winning. How do you interpret this probability?\n\nIf we observe the election over and over, candidate Yu will win roughly 90% of the time.\nCandidate Yu is much more likely to win than to lose.\nThe pollsterâs calculation is wrong. Candidate Yu will either win or lose, thus their probability of winning can only be 0 or 1.\n\n\n\n\n\n\n\n1. a = 1 pt, b = 3 pts, c = 2 pts\n\n2. a = 1 pt, b = 3 pts, c = 1 pt"
  },
  {
    "objectID": "slides/08-bayes.html#thinking-like-a-bayesian-1",
    "href": "slides/08-bayes.html#thinking-like-a-bayesian-1",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Thinking like a Bayesian",
    "text": "Thinking like a Bayesian\n\n\nTwo claims.\n(1) Ben claims he can predict the coin flip outcome. To test his claim, you flip a fair coin 8 times and he correctly predicts all.\n(2) Emma claims she can distinguish natural and artificial sweeteners. To test her claim, you give her 8 samples and she correctly identifies each.\nIn light of these experiments, what do you conclude?\n\nYouâre more confident in Emmaâs claim than Benâs claim.\nThe evidence supporting Benâs claim is just as strong as the evidence supporting Emmaâs claim.\n\n\n\n\n\n\nSuppose that during a doctorâs visit, you tested positive for COVID. If you only get to ask the doctor one question, which would it be?\n\nWhatâs the chance that I actually have COVID?\nIf in fact I donât have COVID, whatâs the chance that I wouldâve gotten this positive test result?\n\n\n\n\n\n\n\n3. a = 3 pts, b = 1 pt\n\n4. a = 3 pts, b = 1 pt\n\nTotals from 4â5 indicate that your current thinking is fairly frequentist, whereas totals from 9â12 indicate alignment with the Bayesian philosophy. In between these extremes, totals from 6â8 indicate that you see strengths in both philosophies."
  },
  {
    "objectID": "slides/08-bayes.html#frequentist-or-bayesian",
    "href": "slides/08-bayes.html#frequentist-or-bayesian",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Frequentist or Bayesian?",
    "text": "Frequentist or Bayesian?\n\n\n\nTotals 4-5: your thinking is frequentist\n\n\n\n\n\n\n\n\n\n\n\n\nTotals 9-12: your thinking is Bayesian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTotals 6-8: you see strengths in both philosophies"
  },
  {
    "objectID": "slides/08-bayes.html#the-meaning-of-probability-relative-frequency",
    "href": "slides/08-bayes.html#the-meaning-of-probability-relative-frequency",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "The Meaning of Probability: Relative Frequency\n",
    "text": "The Meaning of Probability: Relative Frequency\n\n\nThe frequentist interprets probability as the long-run relative frequency of a repeatable experiment.\n\n\nThe probability that some outcome of a process will be obtained is defined as\n\nthe relative frequency with which that outcome would be obtained if the process were repeated a large number of times independently under similar conditions.\n\n\n\n\n\n\n\n      Frequency Relative Frequency\nHeads         4                0.4\nTails         6                0.6\nTotal        10                1.0\n---------------------\n      Frequency Relative Frequency\nHeads       512              0.512\nTails       488              0.488\nTotal      1000              1.000\n---------------------\n\n\n\nIf we repeat tossing the coin 10 times, the probability of obtaining heads is 40%.\nIf 1000 times, the probability is 51.2%.\n\n\n\n\n\n\n\n\n\n\n\n\nsource: usplash"
  },
  {
    "objectID": "slides/08-bayes.html#issues-of-relative-frequency",
    "href": "slides/08-bayes.html#issues-of-relative-frequency",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Issues of Relative Frequency\n",
    "text": "Issues of Relative Frequency\n\n\nð How large of a number is large enough?\n\n\n\nð Meaning of âunder similar conditionsâ\n\n\n\n\nð The relative frequency is reliable under identical conditions?\n\n\n\n\nð We only obtain an approximation instead of exact value.\n\n\n\n\nð How do you compute the probability that Chicago Cubs wins the World Series next year?"
  },
  {
    "objectID": "slides/08-bayes.html#the-meaning-of-probability-relative-plausibility",
    "href": "slides/08-bayes.html#the-meaning-of-probability-relative-plausibility",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "The Meaning of Probability: Relative Plausibility\n",
    "text": "The Meaning of Probability: Relative Plausibility\n\n\nIn the Bayesian philosophy, a probability measures the relative plausibility of an event.\n\n\n\n\n\nFor the statement âcandidate A has a 0.9 probability of winningâ\n\nA frequentist might say\n\nthe conclusion is wrong\n(weirdly) in long-run hypothetical repetitions of the election, candidate A would win roughly 90% of the time.\n\n\nA Bayesian would say based on analysis the candidate A is 9 times more likely to win than to lose.\n\n\n\n\n\n\n\n\n\nSource: https://twitter.com/nytgraphics/status/796195155158171648/photo/1\n\n\n\n\n\n\n\nFor the statement âthe probability of flipping Heads is 0.5â\n\nA frequentist would conclude that if we flip the coin over and over, roughly 1/2 of these flips will be Heads.\nA Bayesian would conclude that Heads and Tails are equally likely."
  },
  {
    "objectID": "slides/08-bayes.html#your-degree-of-uncertainty-or-ignorance",
    "href": "slides/08-bayes.html#your-degree-of-uncertainty-or-ignorance",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Your Degree of Uncertainty or Ignorance",
    "text": "Your Degree of Uncertainty or Ignorance\n\n\n\n\n\n\n\n\nSource: Defence Intelligence â communicating probability\n\n\n\n\n\n\n\nIâm gonna flip a coin, and ask you the probability itâll come up heads. You say â50â50â.\n\n\n\nI then flip the coin, take a peek, but cover it up, and ask: whatâs your probability itâs heads now?\n\n\n\n\nThe event has happened, and no randomness left - just your ignorance!\n\n\n\n\n\n\n\nDavid Spiegelhalter (2024). Why probability probably doesnât exist (but it is useful to act like it does). Nature, 636, p.Â 560 - 563.\nit handles both chance and ignorance.\nI will argue â whether in a scientific paper, as part of weather forecasts, predicting the outcome of a sports competition or quantifying a health risk â is not an objective property of the world, but a construction based on personal or collective judgements and (often doubtful) assumptions. Furthermore, in most circumstances, it is not even estimating some underlying âtrueâ quantity. Probability, indeed, can only rarely be said to âexistâ at all.\nTo get a handle on probabilityâs slipperiness, consider how the concept is used in modern weather forecasts. Meteorologists make predictions of temperature, wind speed and quantity of rain, and often also the probability of rain â say 70% for a given time and place. The first three can be compared with their âtrueâ values; you can go out and measure them. But there is no âtrueâ probability to compare the last with the forecasterâs assessment. There is no âprobability-ometerâ. It either rains or it doesnât.\nThere is another lesson in here. Even if there is a statistical model for what should happen, this is always based on subjective assumptions\nMy argument is that any practical use of probability involves subjective judgements. This doesnât mean that I can put any old numbers on my thoughts â I would be proved a poor probability assessor if I claimed with 99.9% certainty that I can fly off my roof, for example. The objective world comes into play when probabilities, and their underlying assumptions, are tested against reality (see âHow ignorant am I?â); but that doesnât mean the probabilities themselves are objective."
  },
  {
    "objectID": "slides/08-bayes.html#everybody-changes-their-mind",
    "href": "slides/08-bayes.html#everybody-changes-their-mind",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Everybody Changes Their Mind",
    "text": "Everybody Changes Their Mind\n\nHow can we live if we donât change? - BeyoncÃ©. Lyric from âSatellites.â\n\n\nUsing data and prior beliefs to update our knowledge (posterior), and repeating.\nWe continuously update our knowledge about the world as we accumulate lived experiences, or collect data.\n\n\n\n\n\n\nFig 1.1 of https://www.bayesrulesbook.com/. The figures not being sourced come from this book too.\n\n\n\n\nEverybody changes their mind. You likely even changed your mind in the last minute. For example, suppose thereâs a new Italian restaurant in your town. It has a 5-star online rating and you love Italian food! Thus, prior to ever stepping foot in the restaurant, you anticipate that it will be quite delicious. On your first visit, you collect some edible data: your pasta dish arrives a soggy mess. Weighing the stellar online rating against your own terrible meal (which might have just been a fluke), you update your knowledge: this is a 3-star not 5-star restaurant. Willing to give the restaurant another chance, you make a second trip. On this visit, youâre pleased with your Alfredo and increase the restaurantâs rating to 4 stars. You continue to visit the restaurant, collecting edible data and updating your knowledge each time."
  },
  {
    "objectID": "slides/08-bayes.html#bayesian-knowledge-building-process",
    "href": "slides/08-bayes.html#bayesian-knowledge-building-process",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Bayesian Knowledge-building Process",
    "text": "Bayesian Knowledge-building Process\n\nIf youâre an environmental scientist, yours might be an analysis of the human role in climate change. You donât walk into such an inquiry without context â you carry a degree of incoming or prior information based on previous research and experience. Naturally, itâs in light of this information that you interpret new data, weighing both in developing your updated or posterior information."
  },
  {
    "objectID": "slides/08-bayes.html#frequentist-relys-on-limited-data-only",
    "href": "slides/08-bayes.html#frequentist-relys-on-limited-data-only",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Frequentist Relys on (Limited) Data Only",
    "text": "Frequentist Relys on (Limited) Data Only\n\nIn Question 3, in a frequentist analysis, â8 out of 8â is â8 out of 8â no matter if itâs in the context of Benâs coins or Emmaâs sweeteners.\n\n\nEqually confident conclusions that Ben can predict coin flips and Emma can distinguish between natural and artificial sweeteners.\n\n\n\n\n\n\nBut do you really believe Benâs claim 100%? ð¤ ð\n\n\n\n\nIn fact, we judge their claim before evidence are collected, donât we? ð¤\n\n\n\n\nYou probably think Ben overstates his ability but Emmaâs claim sounds relatively reasonable, right?"
  },
  {
    "objectID": "slides/08-bayes.html#the-bayesian-balancing",
    "href": "slides/08-bayes.html#the-bayesian-balancing",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "The Bayesian Balancing",
    "text": "The Bayesian Balancing\n\nFrequentist throws out all prior knowledge in favor of a mere 8 data points.\n\n\n\nBayesian analyses balance and weight our prior experience/knowledge/belief and new data/evidence to judge a claim or make a conclusion.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe are not stubborn! If Ben had correctly predicted the outcome of 1 million coin flips, the strength of this data would far surpass that of our prior judgement, leading to a posterior conclusion that perhaps Ben is psychic!\n\n\n\n\n\n\n\n\n\n\n\n\nSeeing is believing"
  },
  {
    "objectID": "slides/08-bayes.html#asking-different-questions",
    "href": "slides/08-bayes.html#asking-different-questions",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Asking Different Questions",
    "text": "Asking Different Questions\n\n\nIn Question 4,\n\nBayesians answer (a) whatâs the chance that I actually have COVID?\n\nFrequentists answer (b) if in fact I do not have COVID, whatâs the chance that I wouldâve gotten this positive test result?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest Positive\nTest Negative\nTotal\n\n\n\nCOVID\n3\n1\n4\n\n\nNo COVID\n9\n87\n96\n\n\nTotal\n12\n88\n100\n\n\n\n\n\\(H_0\\): Do not have COVID vs.Â \\(H_1\\): Have COVID\nA frequestist assesses the uncertainty of the observed data in light of an assumed hypothesis \\(P(Data \\mid H_0) = 9/96\\)\nA Bayesian assesses the uncertainty of the hypothesis in light of the observed data \\(P(H_0 \\mid Data) = 9/12\\)\n\na Bayesian analysis would ask: Given my positive test result, whatâs the chance that I actually have the disease? Since only 3 of the 12 people that tested positive have the disease (Table 1.1), thereâs only a 25% chance that you have the disease. Thus, when we take into account the diseaseâs rarity and the relatively high false positive rate, itâs relatively unlikely that you actually have the disease. What a relief.\nsince disease status isnât repeatable, the probability you have the disease is either 1 or 0 â you have it or you donât. To the contrary, medical testing (and data collection in general) is repeatable. You can get tested for the disease over and over and over. Thus, a frequentist analysis would ask: If I donât actually have the disease, whatâs the chance that I wouldâve tested positive? Since only 9 of the 96 people without the disease tested positive, thereâs a roughly 10% (9/96) chance that you wouldâve tested positive even if you didnât have the disease."
  },
  {
    "objectID": "slides/08-bayes.html#fake-news",
    "href": "slides/08-bayes.html#fake-news",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Fake News",
    "text": "Fake News\n\n\n\nTo do: Predict whether or not if an incoming article is fake.\n\n\n\n\nPrior info: 40% of the articles are fake\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData come in: Check several fake and real articles, and found ! is more consistent with fake news (Evidence).\n\n\n\n title_has_excl fake real\n          FALSE   44   88\n           TRUE   16    2\n          Total   60   90\n\n\nThe usage of an ! might seem odd for a real article. The exclamation point data is more consistent with fake news."
  },
  {
    "objectID": "slides/08-bayes.html#bayesian-updating-rule",
    "href": "slides/08-bayes.html#bayesian-updating-rule",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Bayesian Updating Rule",
    "text": "Bayesian Updating Rule\n\n\n\\(F\\): an article is fake.\nThe prior probability model\n\n\n\n\n\n\n\n\n\nEvent\n\\(F\\)\n\\(F^c\\)\nTotal\n\n\nProbability \\(P(\\cdot)\\)\n\n0.4\n0.6\n1"
  },
  {
    "objectID": "slides/08-bayes.html#bayesian-model-for-events",
    "href": "slides/08-bayes.html#bayesian-model-for-events",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Bayesian Model for Events",
    "text": "Bayesian Model for Events\n\n\n\n title_has_excl fake real\n          FALSE   44   88\n           TRUE   16    2\n          Total   60   90\n\n\n\n\\(D\\): an article title has exclamation mark.\nConditional probability: \\(P(D \\mid {\\color{blue}{F}}) = 16/60 = 27\\%\\); \\(P(D \\mid {\\color{blue}{F^c}}) = 2/90 = 2\\%\\).\n\n\n\n\nOpposite position:\n\nKnow the incoming article used ! (observed data \\({\\color{blue}{D}}\\))\nDonât know whether or not the article is fake \\(F\\) (what we want to decide).\n\n\n\n\n\n\nCompare \\(P(D \\mid F)\\) and \\(P(D \\mid F^c)\\) to ascertain the relative likelihoods of observed data \\({\\color{blue}{D}}\\) under different scenarios of the uncertain article status.\n\nSince exclamation point usage is so much more likely among fake news than real news, this data provides some evidence that the article is fake To help distinguish this application of conditional probability calculations from that when\nD is uncertain and F is known, weâll utilize the following likelihood function notation."
  },
  {
    "objectID": "slides/08-bayes.html#likelihood-function",
    "href": "slides/08-bayes.html#likelihood-function",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Likelihood Function",
    "text": "Likelihood Function\n\n\n\nLikelihood function \\(L(\\cdot\\mid {\\color{blue}{D}})\\):\n\n\\[L(F \\mid {\\color{blue}{D}}) = P({\\color{blue}{D}} \\mid F) \\text{ and } L(F^c \\mid {\\color{blue}{D}}) = P({\\color{blue}{D}} \\mid F^c)\\]\n\n\n\n\nWhen \\({\\color{blue}{F}}\\) is known, the conditional probability function \\(P(\\cdot \\mid {\\color{blue}{F}})\\) compares the probabilities of an unknown event \\(D\\), \\(D^c\\), occurring with \\(F\\): \\[P(D \\mid {\\color{blue}{F}}) \\text{  vs. } P(D^c \\mid {\\color{blue}{F}})\\]\n\n\n\n\n\n\n\nWhen \\({\\color{blue}{D}}\\) is known, the likelihood function \\(L(\\cdot \\mid {\\color{blue}{D}}) = P({\\color{blue}{D}} \\mid \\cdot)\\) evaluates the relative compatibility of data \\(D\\) with \\(F\\) or \\(F^c\\): \\[L(F \\mid {\\color{blue}{D}}) \\text{  vs. } L(F^c \\mid {\\color{blue}{D}})\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvent\n\\(F\\)\n\\(F^c\\)\nTotal\n\n\n\nProbability \\(P(\\cdot)\\)\n\n0.4\n0.6\n1\n\n\nLikelihood \\(L(\\cdot \\mid D)\\)\n\n0.27\n0.02\n0.29\n\n\n\n\n\n\n\nThe likelihood function is not a probability function!"
  },
  {
    "objectID": "slides/08-bayes.html#bayes-rule",
    "href": "slides/08-bayes.html#bayes-rule",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Bayesâ Rule",
    "text": "Bayesâ Rule\n\\[\\begin{align*} P(F \\mid D) &= \\frac{P(F \\cap D)}{P(D)}\\\\ &= \\frac{L(F \\mid D)P(F)}{P(D)}\\end{align*}\\]\n\n\\[\\text{posterior = } \\frac{\\text{likelihood} \\cdot \\text{prior }}{ \\text{normalizing constant}} \\]\n\nThe normalizing constant \\(P(D)\\) is known as marginal likelihood or evidence.\n\n\\[\\begin{align*} P(F \\mid D) &= \\frac{P(F \\cap D)}{P(D)}\\\\ &= \\frac{P(D \\mid F)P(F)}{P(D)} \\\\ &= \\frac{P(D \\mid F)P(F)}{P(D \\mid F)P(F) + P(D \\mid F^c)P(F^c)}\\\\ &= \\frac{L(F \\mid D)P(F)}{L(F \\mid D)P(F) + L(F^c \\mid D)P(F^c)}\\end{align*}\\]"
  },
  {
    "objectID": "slides/08-bayes.html#posterior",
    "href": "slides/08-bayes.html#posterior",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Posterior",
    "text": "Posterior\n\nStarted with a prior understanding that thereâs a 40% chance that the incoming article would be fake.\n\n\n\nYet upon observing the use of an exclamation point in the title\n\n\n\nâThe president has a funny secret!â\n\n\na feature thatâs more common to fake news.\n\nOur posterior understanding evolved quite a bit â the chance that the article is fake jumped to 89%.\n\n\n\n\n\n\n\n\n\nEvent\n\\(F\\)\n\\(F^c\\)\nTotal\n\n\n\nPrior prob \\(P(\\cdot)\\)\n\n0.4\n0.6\n1\n\n\nPosterior prob \\(P(\\cdot \\mid D)\\)\n\n0.89\n0.11\n1"
  },
  {
    "objectID": "slides/08-bayes.html#bayesian-inference-for-random-variables",
    "href": "slides/08-bayes.html#bayesian-inference-for-random-variables",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Bayesian Inference for Random Variables",
    "text": "Bayesian Inference for Random Variables\nFor any random variables parameter \\(\\theta\\) and data \\({\\bf Y} = (Y_1, \\dots, Y_n)\\),\n\n\\(\\pi(\\theta)\\): the prior pmf/pdf of \\(\\theta\\)\n\\(L(\\theta \\mid y_1,\\dots, y_n)\\): the likelihood of \\(\\theta\\) given observed data \\(\\mathbf{y}= \\{y_i \\}_{i = 1}^n\\).\nThe posterior distribution of \\(\\theta\\) given \\(\\mathbf{y}\\) is\n\n\\[\\pi(\\theta \\mid \\mathbf{y}) = \\frac{L(\\theta \\mid \\mathbf{y})\\pi(\\theta)}{p(\\mathbf{y})}\\] where \\[p(\\mathbf{y}) = \\begin{cases} \\int_{\\Theta} L(\\theta \\mid \\mathbf{y})\\pi(\\theta) ~ d\\theta & \\text{if } \\theta \\text{ is continuous }\\\\\n\\sum_{\\theta \\in \\Theta} L(\\theta \\mid \\mathbf{y})\\pi(\\theta) & \\text{if } \\theta \\text{ is discrete }\n\\end{cases}\\]"
  },
  {
    "objectID": "slides/08-bayes.html#proportionality",
    "href": "slides/08-bayes.html#proportionality",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Proportionality",
    "text": "Proportionality\n\\[\\pi(\\theta \\mid \\mathbf{y}) = \\frac{L(\\theta \\mid \\mathbf{y})\\pi(\\theta)}{p(\\mathbf{y})} \\propto_{\\theta} L(\\theta \\mid \\mathbf{y})\\pi(\\theta)\\]\n\\[\\text{posterior } \\propto \\text{ likelihood } \\cdot \\text{ prior } \\]"
  },
  {
    "objectID": "slides/08-bayes.html#motivation-example",
    "href": "slides/08-bayes.html#motivation-example",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Motivation Example",
    "text": "Motivation Example\n\n\n\nMichelle has decided to run for governor of Wisconsin.\n\n\nAccording to previous 30 polls,\n\nMichelleâs support is centered round 45%\nshe polled at around 35% in the dreariest days and around 55% in the best days\n\n\nWith this prior information, weâd like to estimate/update Michelleâs support by conducting a new poll.\n\n\n\n\n\n\n\n\n\n\n\n\nKey: Describe prior and data information using probabilistic models.\n\n\nThe parameter to be estimated is \\(\\theta\\), the Michelleâs support, which is between 0 and 1."
  },
  {
    "objectID": "slides/08-bayes.html#prior-distribution",
    "href": "slides/08-bayes.html#prior-distribution",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Prior Distribution",
    "text": "Prior Distribution\n\nA popular probability distribution for probability is beta distribution, \\(\\text{beta}(\\alpha, \\beta)\\), where \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\) are shape parameters.\n\n\\[\\pi(\\theta \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\theta^{\\alpha - 1}(1-\\theta)^{\\beta-1}\\]"
  },
  {
    "objectID": "slides/08-bayes.html#prior-distribution-1",
    "href": "slides/08-bayes.html#prior-distribution-1",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Prior Distribution",
    "text": "Prior Distribution\n\n\\(\\theta \\sim \\text{beta}(\\alpha, \\beta)\\)\nIn the prior model, \\(\\alpha\\) and \\(\\beta\\) are hyperparameters to be chosen to reflect our prior information.\n\n\n\nMichelleâs support is centered round 45%, and she polled at around 35% in the dreariest days and around 55% in the best days.\n\n\nChoose \\(\\alpha\\) and \\(\\beta\\) so that the prior mean is about 0.45 and the range is from 0.35 to 0.55.\n\\(\\mathrm{E}(\\theta) = \\frac{\\alpha}{\\alpha + \\beta}\\)\n\\(\\mathrm{Var}(\\theta) = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\)"
  },
  {
    "objectID": "slides/08-bayes.html#prior-distribution-2",
    "href": "slides/08-bayes.html#prior-distribution-2",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Prior Distribution",
    "text": "Prior Distribution"
  },
  {
    "objectID": "slides/08-bayes.html#likelihood",
    "href": "slides/08-bayes.html#likelihood",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Likelihood",
    "text": "Likelihood\n\nYou plan to conduct a new poll of \\(n = 50\\) Cheeseheads and record \\(Y\\), the number that support Michelle.\n\n\nWhat distribution can be used for modeling likelihood connecting the data \\(y\\) and the parameter we are interested, \\(\\theta\\)?\n\n\n\nIf voters answer the poll independently, and the probability that any polled voter supports Michelle is \\(\\theta\\), we could consider\n\n\\[Y \\mid \\theta \\sim \\text{binomial}(n=50, \\theta)\\]\n\n\n\nThe poll result is \\(y = 30\\), the likelihood is\n\n\\[L(\\theta \\mid y = 30) = {50 \\choose 30}\\theta^{30}(1-\\theta)^{20}, \\quad \\theta \\in (0, 1)\\]"
  },
  {
    "objectID": "slides/08-bayes.html#likelihood-1",
    "href": "slides/08-bayes.html#likelihood-1",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Likelihood",
    "text": "Likelihood"
  },
  {
    "objectID": "slides/08-bayes.html#bayesian-model",
    "href": "slides/08-bayes.html#bayesian-model",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Bayesian Model",
    "text": "Bayesian Model\n\\[\\begin{align}Y \\mid \\theta &\\sim \\text{binomial}(n=50, \\theta)\\\\ \\theta &\\sim \\text{beta}(45, 55)\n\\end{align}\\]\nGoal: Obtain the posterior distribution \\(\\pi(\\theta \\mid y)\\).\n\n\\[\n\\begin{align} \\pi(\\theta \\mid y) &\\propto_{\\theta} L(\\theta \\mid y)\\pi(\\theta) \\\\\n&= {50 \\choose 30}\\theta^{30}(1-\\theta)^{20} \\times \\frac{\\Gamma(100)}{\\Gamma(45)\\Gamma(55)}\\theta^{44}(1-\\theta)^{54}\\\\\n&\\propto_{\\theta} \\theta^{74}(1-\\theta)^{74}\\\\\n&= \\frac{\\Gamma(150)}{\\Gamma(75)\\Gamma(75)} \\theta^{74}(1-\\theta)^{74} \\\\\n&= \\text{beta}(75, 75)\\end{align}\n\\]\nusing the fact that \\(\\int_{\\mathcal{X}} f(x) dx = 1\\) for any pdf \\(f(x)\\)."
  },
  {
    "objectID": "slides/08-bayes.html#posterior-distribution",
    "href": "slides/08-bayes.html#posterior-distribution",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Posterior Distribution",
    "text": "Posterior Distribution"
  },
  {
    "objectID": "slides/08-bayes.html#take-home-message",
    "href": "slides/08-bayes.html#take-home-message",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Take-home Message",
    "text": "Take-home Message\n\n\nBayesian/Probabilistic\n\nThe parameter is random\n\nThe parameter keeps changing.\nEven the parameter is fixed and constant, the probability distribution associated with it reflects our ignorance, uncertainty, and plausibility of its value, the probability we really want!\n\n\n\n\nFrequentist/Classical\n\nThe parameter is fixed and constant\n\nThe probability is approximated by relative frequency.\nThe uncertainty is from the randomness of data."
  },
  {
    "objectID": "slides/08-bayes.html#normal-data-model",
    "href": "slides/08-bayes.html#normal-data-model",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Normal Data Model",
    "text": "Normal Data Model\nIn simple linear regression,\n\\[Y_i \\mid \\beta_0, \\beta_1, \\sigma \\stackrel{ind}{\\sim}  N\\left(\\mu_i, \\sigma^2 \\right) \\quad \\text{with} \\quad \\mu_i = \\beta_0 + \\beta_1 X_i\\]\nThis normal data model is our likelihood \\(L(\\boldsymbol \\theta= (\\beta_0, \\beta_1, \\sigma) \\mid \\mathcal{D} = \\{\\mathbf{y}, \\mathbf{x}\\})\\), as it evaluates how the data are compatible with different possible values of parameters.\n\nTo be a Bayesian, what do we do next?\n\n\n\nAssign priors to the unknown parameters, then do the posterior inference using Bayesâ rule!\n\n\n\n\nBig question is how?"
  },
  {
    "objectID": "slides/08-bayes.html#prior-models",
    "href": "slides/08-bayes.html#prior-models",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Prior Models",
    "text": "Prior Models\n\nThere are countless approaches to construct priors for \\(\\beta_0, \\beta_1\\), and \\(\\sigma\\).\n\n\n\nFor simplicity, assume \\(\\beta_0, \\beta_1\\), and \\(\\sigma\\) are independent, \\(\\pi(\\boldsymbol \\theta) = \\pi(\\beta_0, \\beta_1, \\sigma) = \\pi(\\beta_0)\\pi(\\beta_1)\\pi(\\sigma)\\)\n\n\n\n\n\n\n\\(\\beta_0\\) and \\(\\beta_1\\) can technically take any values in the real line.\n\n\\[\\begin{align} \\beta_0 \\sim N(m_0, s_0^2) \\\\\n\\beta_1 \\sim N(m_1, s_1^2) \\end{align}\\]\n\n\n\n\n\\(\\sigma\\) must be positive.\n\n\\[\\begin{align} \\sigma \\sim \\text{Exp}(\\lambda) \\end{align}\\] \\(\\pi(\\sigma) = \\lambda e^{-\\lambda \\sigma}\\), \\(\\lambda &gt; 0\\) and \\(\\mathrm{E}(\\sigma) = 1/\\lambda\\), and \\(\\mathrm{Var}(\\sigma) = 1/\\lambda^2\\)\nhttps://mc-stan.org/rstanarm/articles/priors.html"
  },
  {
    "objectID": "slides/08-bayes.html#bayesian-simple-linear-regression-model",
    "href": "slides/08-bayes.html#bayesian-simple-linear-regression-model",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Bayesian Simple Linear Regression Model",
    "text": "Bayesian Simple Linear Regression Model\n\\[\\begin{align} Y_i \\mid \\beta_0, \\beta_1, \\sigma &\\stackrel{ind}{\\sim}  N\\left(\\mu_i, \\sigma^2 \\right) \\quad \\text{with} \\quad \\mu_i = \\beta_0 + \\beta_1 X_i \\\\\n\\beta_0 &\\sim N(m_0, s_0^2) \\\\\n\\beta_1 &\\sim N(m_1, s_1^2) \\\\\n\\sigma &\\sim \\text{Exp}(\\lambda) \\end{align}\\]"
  },
  {
    "objectID": "slides/08-bayes.html#capital-bikeshare-bayesrulesbikes-data-in-washington-d.c.",
    "href": "slides/08-bayes.html#capital-bikeshare-bayesrulesbikes-data-in-washington-d.c.",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Capital Bikeshare bayesrules::bikes Data in Washington, D.C.\n",
    "text": "Capital Bikeshare bayesrules::bikes Data in Washington, D.C.\n\n\n\nRows: 500\nColumns: 2\n$ rides     &lt;int&gt; 654, 1229, 1454, 1518, 1362, 891, 1280, 1220, 1137, 1368, 13â¦\n$ temp_feel &lt;dbl&gt; 64.7, 49.0, 51.1, 52.6, 50.8, 46.6, 45.6, 49.2, 46.4, 45.6, â¦"
  },
  {
    "objectID": "slides/08-bayes.html#tuning-prior-models",
    "href": "slides/08-bayes.html#tuning-prior-models",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Tuning Prior Models",
    "text": "Tuning Prior Models\n\n\nPrior understanding 1:\n\nOn an average temperature day, say 65 or 70 degrees, there are typically around 5000 riders, though this average could be somewhere between 3000 and 7000.\n\n\n\n\n\nThe prior information tells us something about \\(\\beta_0\\), but the information has been centered.\n\nThe centered intercept, \\(\\beta_{0c}\\), reflects the typical ridership at the typical temperature.\n\n\n\n\\(\\beta_{0c} \\sim N(5000, 1000^2)\\)"
  },
  {
    "objectID": "slides/08-bayes.html#tuning-prior-models-1",
    "href": "slides/08-bayes.html#tuning-prior-models-1",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Tuning Prior Models",
    "text": "Tuning Prior Models\n\n\nPrior understanding 2:\n\nFor every one degree increase in temperature, ridership typically increases by 100 rides, though this average increase could be as low as 20 or as high as 180.\n\n\n\n\n\\(\\beta_{1} \\sim N(100, 40^2)\\)"
  },
  {
    "objectID": "slides/08-bayes.html#tuning-prior-models-2",
    "href": "slides/08-bayes.html#tuning-prior-models-2",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Tuning Prior Models",
    "text": "Tuning Prior Models\n\n\nPrior understanding 3:\n\nAt any given temperature, daily ridership will tend to vary with a moderate standard deviation of 1250 rides.\n\n\n\n\n\\(\\sigma \\sim \\text{Exp}(0.0008)\\) because \\(\\mathrm{E}(\\sigma) = 1/\\lambda = 1/0.0008 = 1250\\)"
  },
  {
    "objectID": "slides/08-bayes.html#prior-model-simulation",
    "href": "slides/08-bayes.html#prior-model-simulation",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Prior Model Simulation",
    "text": "Prior Model Simulation\n\n100 prior plausible model lines \\(\\mu_{Y|X} = \\beta_0 + \\beta_1 X\\)"
  },
  {
    "objectID": "slides/08-bayes.html#posterior-inference",
    "href": "slides/08-bayes.html#posterior-inference",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Posterior Inference",
    "text": "Posterior Inference\n\nUpdate our prior understanding of the relationship between ridership and temperature using data information provided by likelihood.\nThe joint posterior distribution is\n\n\\[\\pi(\\beta_0, \\beta_1, \\sigma \\mid \\mathbf{y}) = \\frac{L(\\beta_0, \\beta_1, \\sigma \\mid \\mathbf{y})\\pi(\\beta_0, \\beta_1, \\sigma)}{p(\\mathbf{y})}\\] where\n\n\\(L(\\beta_0, \\beta_1, \\sigma \\mid \\mathbf{y}) = p(\\mathbf{y}\\mid \\beta_0, \\beta_1, \\sigma) = \\prod_{i=1}^np(y_i \\mid \\beta_0, \\beta_1, \\sigma)\\)\n\\(\\pi(\\beta_0, \\beta_1, \\sigma) = \\pi(\\beta_0)\\pi(\\beta_1)\\pi(\\sigma)\\)\n\\(p(\\mathbf{y}) = \\int \\int \\int \\left[\\prod_{i=1}^np(y_i \\mid \\beta_0, \\beta_1, \\sigma)\\right]\\pi(\\beta_0)\\pi(\\beta_1)\\pi(\\sigma) ~ d\\beta_0d\\beta_1d\\sigma\\)\nThere are lots of ways to generate/approximate the posterior distribution of parameters. One method is Markov chain Monte Carlo (MCMC)."
  },
  {
    "objectID": "slides/08-bayes.html#rstanarmstan_glm",
    "href": "slides/08-bayes.html#rstanarmstan_glm",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "rstanarm::stan_glm()",
    "text": "rstanarm::stan_glm()\n\n\nrstanarm uses RStan syntax1 to do Bayesian inference for applied regression models (arm).\n\n\nbike_model &lt;- rstanarm::stan_glm(rides ~ temp_feel, data = bikes,\n                                 family = gaussian,\n                                 prior_intercept = normal(5000, 1000),\n                                 prior = normal(100, 40), \n                                 prior_aux = exponential(0.0008),\n                                 chains = 4, iter = 5000*2, seed = 2025)\n\n\nGenerate 4 Monte Carlo chains (chains = 4), each having 10000 iterations (iter = 5000*2).\nEach iteration draw a posterior sample of the \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma\\).\nAfter tossing out the first half of Markov chain values from the warm-up or burn-in phase, stan_glm() simulation produces four parallel chains of length 5000 for each model parameter:\n\n\\(\\{ \\beta_0^{(1)}, \\beta_0^{(2)}, \\dots, \\beta_0^{(5000)} \\}\\), \\(\\{ \\beta_1^{(1)}, \\beta_1^{(2)}, \\dots, \\beta_1^{(5000)} \\}\\), \\(\\{ \\sigma^{(1)}, \\sigma^{(2)}, \\dots, \\sigma^{(5000)} \\}\\)\nRStan is the R interface to Stan."
  },
  {
    "objectID": "slides/08-bayes.html#convergence-diagnostics",
    "href": "slides/08-bayes.html#convergence-diagnostics",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics\n\n# Trace plots of parallel chains\nbayesplot::mcmc_trace(bike_model, size = 0.1)"
  },
  {
    "objectID": "slides/08-bayes.html#convergence-diagnostics-1",
    "href": "slides/08-bayes.html#convergence-diagnostics-1",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics\n\n\n\n\n\nSource: https://blog.stata.com/2016/11/15/introduction-to-bayesian-statistics-part-2-mcmc-and-the-metropolis-hastings-algorithm/"
  },
  {
    "objectID": "slides/08-bayes.html#convergence-diagnostics-2",
    "href": "slides/08-bayes.html#convergence-diagnostics-2",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics"
  },
  {
    "objectID": "slides/08-bayes.html#convergence-diagnostics-3",
    "href": "slides/08-bayes.html#convergence-diagnostics-3",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics\n\n# Density plots of parallel chains\nbayesplot::mcmc_dens_overlay(bike_model)\n\n\n\nquantifies the number of independent samples it would take to produce an equivalently accurate posterior approximation.\nwe observe that these four chains produce nearly indistinguishable posterior approximations. This provides evidence that our simulation is stable and sufficiently long â running the chains for more iterations likely wouldnât produce drastically different or improved posterior approximations."
  },
  {
    "objectID": "slides/08-bayes.html#convergence-diagnostics-4",
    "href": "slides/08-bayes.html#convergence-diagnostics-4",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics\n\n# Effective sample size ratio\nbayesplot::neff_ratio(bike_model)\n\n(Intercept)   temp_feel       sigma \n      0.995       0.992       0.950 \n\n# Rhat\nbayesplot::rhat(bike_model)\n\n(Intercept)   temp_feel       sigma \n          1           1           1 \n\n\n\nBoth effective sample size and R-hat are close to 1, indicating that the chains are stable, mixing quickly, and behaving much like an independent sample.\n\n\nThereâs no magic rule for interpreting this ratio, and it should be utilized alongside other diagnostics such as the trace plot. That said, we might be suspicious of a Markov chain for which the effective sample size ratio is less than 0.1, i.e., the effective sample size is less than 10% of the actual sample size.\nan R-hat ratio greater than 1.05 raises some red flags about the stability of the simulation."
  },
  {
    "objectID": "slides/08-bayes.html#convergence-issues",
    "href": "slides/08-bayes.html#convergence-issues",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Convergence Issues",
    "text": "Convergence Issues\n\n\n\n\nHighly Autocorrelated Chain: Effective size is small, not many independent samples that are representative of the true posterior distribution.\n\nRun longer and thinning the chain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlow Convergence: Need wait longer to have the chain reached a stable mixing zone that are representative of the true posterior distribution.\n\nSet a longer burn-in or warm-up period"
  },
  {
    "objectID": "slides/08-bayes.html#interpreting-the-posterior",
    "href": "slides/08-bayes.html#interpreting-the-posterior",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Interpreting the Posterior",
    "text": "Interpreting the Posterior\n\n# Posterior summary statistics\ntidy(bike_model, effects = c(\"fixed\", \"aux\"),\n     conf.int = TRUE, conf.level = 0.80)\n\n# A tibble: 4 Ã 5\n  term        estimate std.error conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  -2191.     355.    -2653.    -1735. \n2 temp_feel       82.1      5.07     75.7      88.7\n3 sigma         1282.      41.1    1231.     1336. \n4 mean_PPD      3487.      80.3    3385.     3590. \n\n\n\nThe posterior relationship is\n\n\\[-2196 + 82.2 X\\]\n\nThe 80% credible interval for \\(\\beta_1\\) is \\((75.7, 88.5)\\).\nGiven the data, the probability that \\(\\beta_1\\) is between 75.7 and 88.5 is 80%., i.e., \\(P(\\beta_1 \\in(75.7, 88.5) \\mid \\mathbf{y}, \\mathbf{x}) = 80\\%\\)."
  },
  {
    "objectID": "slides/08-bayes.html#posterior-samples",
    "href": "slides/08-bayes.html#posterior-samples",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Posterior Samples",
    "text": "Posterior Samples\n\n# Store the 4 chains for each parameter in 1 data frame\nbike_model_df &lt;- as.data.frame(bike_model)\nnrow(bike_model_df)\n\n[1] 20000\n\nhead(bike_model_df)\n\n  (Intercept) temp_feel sigma\n1       -2318      83.0  1304\n2       -2107      81.9  1261\n3       -2460      85.9  1246\n4       -2419      84.7  1394\n5       -2067      79.5  1202\n6       -1806      75.6  1224\n\n\nHow to obtain \\(P(\\beta_1 &gt; 0 \\mid \\mathbf{y}, \\mathbf{x})\\)?\n\n\\(P(\\beta_1 &gt; 0 \\mid \\mathbf{y}, \\mathbf{x}) \\approx \\frac{1}{20000}\\sum_{t=1}^{20000} \\mathbf{1}\\left(\\beta_1^{(t)} &gt; 0\\right)\\)\n\nsum(bike_model_df$temp_feel &gt; 0) / nrow(bike_model_df)\n\n[1] 1"
  },
  {
    "objectID": "slides/08-bayes.html#posterior-regression-lines",
    "href": "slides/08-bayes.html#posterior-regression-lines",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Posterior Regression Lines",
    "text": "Posterior Regression Lines"
  },
  {
    "objectID": "slides/08-bayes.html#posterior-predictive-draws",
    "href": "slides/08-bayes.html#posterior-predictive-draws",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Posterior Predictive Draws",
    "text": "Posterior Predictive Draws\nFor each posterior draw \\(\\{\\beta_0^{(t)}, \\beta_1^{(t)}, \\sigma^{(t)} \\}_{t = 1}^{20000}\\), we have the posterior predictive distribution \\[Y_i^{(t)} \\sim N\\left(\\beta_0^{(t)} + \\beta_1^{(t)}X_i, \\, (\\sigma^{(t)})^2\\right)\\]"
  },
  {
    "objectID": "slides/08-bayes.html#bayesian-interpretation-for-ridge-and-lasso",
    "href": "slides/08-bayes.html#bayesian-interpretation-for-ridge-and-lasso",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Bayesian Interpretation for Ridge and Lasso",
    "text": "Bayesian Interpretation for Ridge and Lasso\n\nLasso and Ridge regression can be interpreted as a Bayesian regression model.\nThe same data-level likelihood \\[Y_i \\mid \\boldsymbol \\beta, \\sigma \\stackrel{ind}{\\sim}  N\\left(\\mu_i, \\sigma^2 \\right) \\quad \\text{with} \\quad \\mu_i = \\mathbf{x}_i'\\boldsymbol \\beta\\]\nWe use prior distributions to regularize how parameters behave.\n\n\n\nThe two methods can assign the same prior distributions to \\(\\beta_0\\) and \\(\\sigma\\), but they use different priors on \\(\\{\\beta_j\\}_{j = 1}^p\\).1\n\nLasso: \\(\\beta_j \\stackrel{iid}{\\sim} \\text{Laplace}\\left(0, \\tau(\\lambda)\\right)\\)\nRidge: \\(\\beta_j \\stackrel{iid}{\\sim} N\\left(0, \\tau(\\lambda)\\right)\\)\n\n\n\n\nRemember we usually standardize coefficients before implementing Ridge and Lasso."
  },
  {
    "objectID": "slides/08-bayes.html#ridge-and-lasso-priors",
    "href": "slides/08-bayes.html#ridge-and-lasso-priors",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Ridge and Lasso Priors",
    "text": "Ridge and Lasso Priors\n\n\nLasso\n\n\\[\\beta_j \\stackrel{iid}{\\sim} \\text{Laplace}\\left(0, \\tau(\\lambda)\\right)\\]\nLasso solution is the posterior mode of \\(\\boldsymbol \\beta\\)\n\\[\\boldsymbol \\beta^{(l)} = \\mathop{\\mathrm{arg\\,max}}_{\\boldsymbol \\beta} \\,\\,\\, \\pi(\\boldsymbol \\beta\\mid \\mathbf{y}, \\mathbf{x})\\]\n\n\n\n\n\n\n\n\n\n\nRidge\n\n\\[\\beta_j \\stackrel{iid}{\\sim} N\\left(0, \\tau(\\lambda)\\right)\\]\nRidge solution is the posterior mode/mean of \\(\\boldsymbol \\beta\\)\n\\[\\boldsymbol \\beta^{(r)} = \\mathop{\\mathrm{arg\\,max}}_{\\boldsymbol \\beta} \\,\\, \\, \\pi(\\boldsymbol \\beta\\mid \\mathbf{y}, \\mathbf{x})\\]"
  },
  {
    "objectID": "slides/08-bayes.html#resources",
    "href": "slides/08-bayes.html#resources",
    "title": "Bayesian Inference and Linear Regression \n",
    "section": "Resources",
    "text": "Resources\n\nBayes Rules! An Introduction to Applied Bayesian Modeling\nStatistical Rethinking\nA Studentâs Guide to Bayesian Statistics\nBayesian Data Analysis\n\n\n\n\n\nmssc6250-s25.github.io/website"
  },
  {
    "objectID": "slides/01-syllabus.html#my-journey",
    "href": "slides/01-syllabus.html#my-journey",
    "title": "Welcome Aboard ð",
    "section": "My Journey",
    "text": "My Journey\n\nAssistant Professor (2020/08 - )\n\n\n\n\n\n\n\n\n\n\nPostdoctoral Fellow\n\n\n\n\n\n\n\n\n\n\nPhD in Statistics and Applied Mathematics\n\n\n\n\n\n\n\n\n\n\nMA in Economics/PhD program in Statistics\n\n\n\n\n\n\n\n\n\nAfter college, working and doing military service for several years, I came to the US for my PhD degree. Originally I would like to study economics, but then I switched my major to statistics.\n\nI got my master degree in economics from Indiana University Bloomington, then I transferred to UC Santa Cruz to finish my PhD studies.\nThen I spent two years doing my postdoctoral research at Rice University in Houston, Texas.\nFinally, in fall 2020, I came to Marquette as an assistant professor.\nMidwest/Indiana-West/California-South/Texas-Midwest/Wisconsin\nBeen to any one of these universities/cities?\nThe most beautiful campus.\nWho are international students? I can totally understand how hard studying and living in another country. Feel free to share your stories or difficulties, and I am more than happy to help you if you have any questions.\nPoor listening and speaking skills. I was shy.\nOK so, this is my background. How about you introducing yourself as well. You can share anything, your major, hobbies, your favorite food, what do you want to do after graduation, anything,\nI have the class list. Iâd like to learn your face and remember your name. You know, you all wear a mask. Itâs hard to recognize you and connect your name and your face.\nWhen I call your name, you can say something about yourself. No need to be long, couple of seconds are good."
  },
  {
    "objectID": "slides/01-syllabus.html#my-research",
    "href": "slides/01-syllabus.html#my-research",
    "title": "Welcome Aboard ð",
    "section": "My Research",
    "text": "My Research\n\nBayesian spatio-temporal modeling and computation in neuroimaging/epidemiology\nBayesian deep learning for image classification\nEfficient MCMC for high dimensional regression\nData science education\n\n\n\nfMRI\n\n\n\n\n\n\n\n\n\nEEG"
  },
  {
    "objectID": "slides/01-syllabus.html#how-to-reach-me",
    "href": "slides/01-syllabus.html#how-to-reach-me",
    "title": "Welcome Aboard ð",
    "section": "How to Reach Me",
    "text": "How to Reach Me\n\nOffice hours TuTh 4:50 - 5:50 PM and Wed 12 - 1 PM in Cudahy Hall 353.\nð§ cheng-han.yu@marquette.edu\n\nAnswer your question within 24 hours.\nExpect a reply on Monday if shoot me a message on weekends.\nStart your subject line with [mssc6250] followed by a clear description of your question.\n\n\n\n\n\nI will NOT reply your e-mail if â¦ Check the email policy in the syllabus!"
  },
  {
    "objectID": "slides/01-syllabus.html#prerequisites",
    "href": "slides/01-syllabus.html#prerequisites",
    "title": "Welcome Aboard ð",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nOn bulletin: MATH 4720 (Intro to Statistics), MATH 3100 (Linear Algebra) and/or MSSC 5780 (Regression Analysis)\nProgramming experience (Who does machine learning without coding?)\n\n\n\nHaving taken MSSC 5700 (Probability) and MSSC 5710 (Stats Inference) or other math and statistics courses (Stats Computing, etc) is recommended."
  },
  {
    "objectID": "slides/01-syllabus.html#textbook",
    "href": "slides/01-syllabus.html#textbook",
    "title": "Welcome Aboard ð",
    "section": "Textbook",
    "text": "Textbook\n\n\n\n\n(ISL) An Introduction to Statistical Learning, by James et al.Â Publisher: Springer. \n\nDiscuss all chapters except Chapter 11 (survival analysis) and 13 (multiple testing).\nR and Python code\n\nIn the Preface,\n\nâ¦ for advanced undergraduates or masterâs students in Statistics or related quantitative fields,\n\n\nâ¦ concentrate more on the applications of the methods and less on the mathematical details."
  },
  {
    "objectID": "slides/01-syllabus.html#optional-references-1",
    "href": "slides/01-syllabus.html#optional-references-1",
    "title": "Welcome Aboard ð",
    "section": "Optional References",
    "text": "Optional References\n\n\n\n(MML) Mathematics for Machine Learning, by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. Publisher: Cambridge University Press.\nCollege level mathematics for machine learning\nMathematical concepts behind models and algorithms"
  },
  {
    "objectID": "slides/01-syllabus.html#optional-references-2",
    "href": "slides/01-syllabus.html#optional-references-2",
    "title": "Welcome Aboard ð",
    "section": "Optional References",
    "text": "Optional References\n\n\n\n(PMLI) Probabilistic Machine Learning: An Introduction, by Kevin Murphy. Publisher: MIT Press.\nSelf-contained with lots of mathematics foundations\nPython code"
  },
  {
    "objectID": "slides/01-syllabus.html#optional-references-3",
    "href": "slides/01-syllabus.html#optional-references-3",
    "title": "Welcome Aboard ð",
    "section": "Optional References",
    "text": "Optional References\n\n\n\n(PMLA) Probabilistic Machine Learning: Advanced Topics, by Kevin Murphy. Publisher: MIT Press.\nPhD level\nProbabilistic or distributional-based"
  },
  {
    "objectID": "slides/01-syllabus.html#optional-references-4",
    "href": "slides/01-syllabus.html#optional-references-4",
    "title": "Welcome Aboard ð",
    "section": "Optional References",
    "text": "Optional References\n\n\n\n(ESL) The Elements of Statistical Learning, 2nd edition, by Hastie et. al.Â Publisher: Springer.\nFor PhD students or researchers in mathematical sciences\nFrequentist-based"
  },
  {
    "objectID": "slides/01-syllabus.html#course-website---httpsmssc6250-s25.github.iowebsite",
    "href": "slides/01-syllabus.html#course-website---httpsmssc6250-s25.github.iowebsite",
    "title": "Welcome Aboard ð",
    "section": "Course Website - https://mssc6250-s25.github.io/website/\n",
    "text": "Course Website - https://mssc6250-s25.github.io/website/"
  },
  {
    "objectID": "slides/01-syllabus.html#learning-management-system-d2l",
    "href": "slides/01-syllabus.html#learning-management-system-d2l",
    "title": "Welcome Aboard ð",
    "section": "Learning Management System (D2L)",
    "text": "Learning Management System (D2L)\n\n\n\n\n\n\nSubmit your homework Assessments &gt; Dropbox.\nCheck your grade Assessments &gt; Grades."
  },
  {
    "objectID": "slides/01-syllabus.html#grading-policy",
    "href": "slides/01-syllabus.html#grading-policy",
    "title": "Welcome Aboard ð",
    "section": "Grading Policy â¨",
    "text": "Grading Policy â¨\n\nThe grade is earned out of 1000 total points distributed as follows:\n\nHomework: 500 pts\nMidterm mini-project presentations: 300 pts\nFinal project: 200 pts\n\n\n\nâ No extra credit projects/homework/exam to compensate for a poor grade."
  },
  {
    "objectID": "slides/01-syllabus.html#grade-percentage-conversion",
    "href": "slides/01-syllabus.html#grade-percentage-conversion",
    "title": "Welcome Aboard ð",
    "section": "Grade-Percentage Conversion",
    "text": "Grade-Percentage Conversion\n\nYour final grade is based on your percentage of pts earned out of 1000 pts.\n\n[x, y) means greater than or equal to x and less than y.\n\n\n\n\n\n\nGrade\nPercentage\n\n\n\nA\n[94, 100]\n\n\nA-\n[90, 94)\n\n\nB+\n[87, 90)\n\n\nB\n[83, 87)\n\n\nB-\n[80, 83)\n\n\nC+\n[77, 80)\n\n\nC\n[70, 77)\n\n\nF\n[0, 70)"
  },
  {
    "objectID": "slides/01-syllabus.html#homework-500-pts",
    "href": "slides/01-syllabus.html#homework-500-pts",
    "title": "Welcome Aboard ð",
    "section": "Homework (500 pts)",
    "text": "Homework (500 pts)\n\n\nAssessments &gt; Dropbox and upload your homework in PDF format.\nâ No make-up homework.\nDue Friday 11:59 PM  (Hard deadline and no late submission).\nYou have at least one week to finish your homework."
  },
  {
    "objectID": "slides/01-syllabus.html#mini-project-presentation-300-pts",
    "href": "slides/01-syllabus.html#mini-project-presentation-300-pts",
    "title": "Welcome Aboard ð",
    "section": "Mini-Project Presentation (300 pts)",
    "text": "Mini-Project Presentation (300 pts)\n\nThere will be 2 in-class mini-project presentations (150 pts each).\nLearn from each other by presenting and discussing the assigned topics.\nMore details about the activity will be released later."
  },
  {
    "objectID": "slides/01-syllabus.html#final-project-200-pts",
    "href": "slides/01-syllabus.html#final-project-200-pts",
    "title": "Welcome Aboard ð",
    "section": "Final Project (200 pts)",
    "text": "Final Project (200 pts)\n\nThe final project is submitted as a paper (and some relevant work?)\nThe project submission deadline is Thursday, 5/8, 10 AM.\nMore details about the project will be released later."
  },
  {
    "objectID": "slides/01-syllabus.html#which-programming-language",
    "href": "slides/01-syllabus.html#which-programming-language",
    "title": "Welcome Aboard ð",
    "section": "Which Programming Language?",
    "text": "Which Programming Language?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse any language you prefer!"
  },
  {
    "objectID": "slides/01-syllabus.html#generative-ai-and-sharingreusing-code-policy",
    "href": "slides/01-syllabus.html#generative-ai-and-sharingreusing-code-policy",
    "title": "Welcome Aboard ð",
    "section": "Generative AI and Sharing/Reusing Code Policy",
    "text": "Generative AI and Sharing/Reusing Code Policy\n\n\n\nYou are responsible for the content of all work submitted for this course.\nYou may use generative AI tools such as ChatGPT or DALL-E to generate a first draft of text for your assignments, provided that this use is appropriately documented and cited.\nLearn how to cite the use of AI in MLA and APA format, and more\n\n\n\n\n\n\n\n\n\n\n\n\n\nSharing/Reusing Code\n\nUnless explicitly stated otherwise, you may make use of any online resources, but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solutions.\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source.\n\n\n\n\n\nmssc6250-s25.github.io/website"
  },
  {
    "objectID": "slides/10-generative.html#bayes-classifier",
    "href": "slides/10-generative.html#bayes-classifier",
    "title": "Generative Models \n",
    "section": "Bayes Classifier",
    "text": "Bayes Classifier\n\nThe test error rate is minimized, on average, by a classifier that assigns each observation to the most likely category, given its predictor values.\n\nThis fact leads to\n\n\nBayes (Optimal) Classifier: Assign a test observation \\(y_j\\) with predictor \\(x_j\\) to class \\(k\\) for which the conditional probability \\(P(Y_j = k \\mid X = x_j)\\) is largest.\nThe Bayes classifier produces the lowest possible test error rate, called Bayes error rate.\n\n\n\nThe problem is\n\nWe never know \\(P(Y_j = k \\mid X = x_j)\\) ð­\nThe goal of soft classifiers is to estimate it, like logistic regression:\n\n\\[ \\widehat{\\pi}(\\mathbf{x}) = \\widehat{P}(Y = 1 \\mid \\mathbf{x}) = \\frac{1}{1+e^{-\\mathbf{x}'\\widehat{\\boldsymbol{\\beta}}}}\\]\n[What is ``Bayes regression functionâ? In statistics, it is called oracle when a method/model has access to the ground truth.]"
  },
  {
    "objectID": "slides/10-generative.html#what-is-a-generative-model",
    "href": "slides/10-generative.html#what-is-a-generative-model",
    "title": "Generative Models \n",
    "section": "What is a Generative Model?",
    "text": "What is a Generative Model?\n\nSo far predictors are assumed fixed and known, and we care about the conditional distribution, \\(p(y \\mid \\mathbf{x})\\)\n\nLogistic regression (LR)\n\n\n\n\n\nA generative model considers the joint distribution of the response and predictors, i.e., \\(p(y, \\mathbf{x})\\)\n\nDiscriminant analysis\nNaive Bayes (NB)\n\n\n\n\n\nWhy not just logistic regression?\n\nWhen the classes are substaintially separated, the parameter estimates for the logistic regression are unstable.\nIf the distribution of the predictors is approximately normal in each of the classes and the sample size is small, the generative models could perform better and more stable.\n\n\\(\\mathbf{x}\\) is variable whose value is generated from some distribution given a value of \\(y\\)"
  },
  {
    "objectID": "slides/10-generative.html#generative-modeling",
    "href": "slides/10-generative.html#generative-modeling",
    "title": "Generative Models \n",
    "section": "Generative Modeling",
    "text": "Generative Modeling\n\nTo infer/predict \\(p(y = k \\mid \\mathbf{x})\\) from a generative model, Bayes theorem comes into play:\n\n\n\\[\\begin{align} p(y = k \\mid \\mathbf{x}) &= \\frac{p(y = k, \\mathbf{x})}{p(\\mathbf{x})} = \\frac{p(y = k)p(\\mathbf{x}\\mid y = k) }{p(\\mathbf{x})} = \\frac{p(y = k)p(\\mathbf{x}\\mid y = k)}{\\sum_{l=1}^K p(y = k)p(\\mathbf{x}\\mid y = l)}\\\\\n&=\\frac{\\pi_kf_k(\\mathbf{x})}{\\sum_{l=1}^K\\pi_lf_l(\\mathbf{x})}\\end{align}\\]\n\n\nTo get the posterior \\(p(y = k \\mid \\mathbf{x})\\), we need\n\n\nprior \\(\\pi_k = p(y = k)\\)\n\n\nlikelihood \\(f_k(\\mathbf{x}) = p(\\mathbf{x}\\mid y = k)\\)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDifferent generative models use different priors and likelihoods, resulting in different strengths and weakness."
  },
  {
    "objectID": "slides/10-generative.html#bayes-decision-boundary",
    "href": "slides/10-generative.html#bayes-decision-boundary",
    "title": "Generative Models \n",
    "section": "Bayes Decision Boundary",
    "text": "Bayes Decision Boundary\n\nHow do we find/estimate the Bayes decision boundary generated by the Bayes classifier?"
  },
  {
    "objectID": "slides/10-generative.html#linear-discriminant-analysis-lda",
    "href": "slides/10-generative.html#linear-discriminant-analysis-lda",
    "title": "Generative Models \n",
    "section": "Linear Discriminant Analysis (LDA)",
    "text": "Linear Discriminant Analysis (LDA)\n\n\\[\\begin{align} p(y = k \\mid \\mathbf{x})  = \\frac{p(y = k)p(\\mathbf{x}\\mid y = k)}{\\sum_{l=1}^K p(y = k)p(\\mathbf{x}\\mid y = l)} =\\frac{\\pi_kf_k(\\mathbf{x})}{\\sum_{l=1}^K\\pi_lf_l(\\mathbf{x})}\\end{align}\\]\n\n\nEstimate the prior \\(\\pi_k = p(y = k)\\) using the proportion of the training labels that belong to the \\(k\\)th class: \\[\\hat{\\pi}_k = n_k / n \\]\n\n\n\ntable(Default$default)/length(Default$default)\n\n\n    No    Yes \n0.9667 0.0333 \n\n\n\n\nLDA assumption for \\(f_k(\\mathbf{x})\\): \\(\\color{blue}{(\\mathbf{x}\\mid y = k) \\sim N_p\\left(\\boldsymbol{\\mu}_k, \\Sigma \\right)}\\)\n\n\n\\(\\boldsymbol{\\mu}_k\\): class-specific mean vector\n\n\\(\\Sigma\\): covariance matrix common to all \\(K\\) classes"
  },
  {
    "objectID": "slides/10-generative.html#two-multivariate-normals-with-common-covariance",
    "href": "slides/10-generative.html#two-multivariate-normals-with-common-covariance",
    "title": "Generative Models \n",
    "section": "Two Multivariate Normals with Common Covariance",
    "text": "Two Multivariate Normals with Common Covariance\n\n\n\\(\\color{blue}{(\\mathbf{x}\\mid y = k) \\sim N_p\\left(\\boldsymbol{\\mu}_k, \\Sigma \\right)}\\)\n\n\\(\\boldsymbol{\\mu}_1 = (0.5, -1)'\\), \\(\\boldsymbol{\\mu}_2 = (-0.5, 0.5)'\\)\n\\(\\Sigma = \\begin{pmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{pmatrix}\\)\n\\(\\pi_1 = 0.4\\), \\(\\pi_2 = 0.6\\)\nEach class has its own predictor mean.\nThe predictorâs variability are the same for the two classes.\n\n\n\n\n\n\n\n\n\n\nplot_ly has mu1 mu2 problems! Be careful!"
  },
  {
    "objectID": "slides/10-generative.html#parameter-estimation-for-lda",
    "href": "slides/10-generative.html#parameter-estimation-for-lda",
    "title": "Generative Models \n",
    "section": "Parameter Estimation for LDA",
    "text": "Parameter Estimation for LDA\n\\(\\color{blue}{(\\mathbf{x}\\mid y = k) \\sim N_p\\left(\\boldsymbol{\\mu}_k, \\Sigma \\right)}\\)\n\nEstimating \\(p(\\mathbf{x}\\mid y = k)\\) is reduced to estimating \\(\\boldsymbol{\\mu}_1, \\dots, \\boldsymbol{\\mu}_K\\) and \\(\\Sigma\\).\n\n\n\n\n\nCentroid/sample mean: \\(\\widehat{\\boldsymbol{\\mu}}_k = \\frac{1}{n_k} \\sum_{i: \\,y_i = k} \\mathbf{x}_i\\)\n\nPooled covariance matrix: \\[\\widehat \\Sigma = \\frac{1}{n-K} \\sum_{k=1}^K \\sum_{i : \\, y_i = k} (\\mathbf{x}_i - \\widehat{\\boldsymbol{\\mu}}_k) (\\mathbf{x}_i - \\widehat{\\boldsymbol{\\mu}}_k)' = \\sum_{k=1}^K \\frac{n_k-1}{n-K}\\widehat \\Sigma_k\\] where \\(\\widehat \\Sigma_k = \\frac{1}{n_k-1}\\sum_{i : \\, y_i = k} (\\mathbf{x}_i - \\widehat{\\boldsymbol{\\mu}}_k) (\\mathbf{x}_i - \\widehat{\\boldsymbol{\\mu}}_k)'\\)\n\n\n\n\n\nmu1_hat =\n\n\n[1]  0.47 -1.01\n\n\nmu2_hat =\n\n\n[1] -0.54  0.51\n\n\nSigma_hat = \n\n\n     [,1] [,2]\n[1,] 1.13 0.54\n[2,] 0.54 1.02"
  },
  {
    "objectID": "slides/10-generative.html#decision-boundary-by-lda",
    "href": "slides/10-generative.html#decision-boundary-by-lda",
    "title": "Generative Models \n",
    "section": "Decision Boundary by LDA",
    "text": "Decision Boundary by LDA"
  },
  {
    "objectID": "slides/10-generative.html#discriminant-function",
    "href": "slides/10-generative.html#discriminant-function",
    "title": "Generative Models \n",
    "section": "Discriminant Function",
    "text": "Discriminant Function\nThe likelihood is Gaussian\n\n\\[f_k(x) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} \\exp\\left[ -\\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu}_k)' \\Sigma^{-1} (\\mathbf{x}- \\boldsymbol{\\mu}_k) \\right]\\]\n\n\n\nThe log-likelihood is\n\\[\\begin{align}\n\\log f_k(\\mathbf{x}) =&~ -\\log \\big((2\\pi)^{p/2} |\\Sigma|^{1/2} \\big) - \\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu}_k)'\\Sigma^{-1} (\\mathbf{x}- \\boldsymbol{\\mu}_k) \\\\\n    =&~ - \\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu}_k)' \\Sigma^{-1} (\\mathbf{x}- \\boldsymbol{\\mu}_k) + \\text{Constant}\n\\end{align}\\]\n\n\n\nThe maximum a posteriori estimate is\n\n\\[\\begin{align}\n\\widehat f(\\mathbf{x}) =& ~\\underset{k}{\\arg\\max} \\,\\, \\log \\big( \\pi_k f_k(\\mathbf{x}) \\big) \\\\\n    =& ~\\underset{k}{\\arg\\max} \\,\\, - \\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu}_k)' \\Sigma^{-1} (\\mathbf{x}- \\boldsymbol{\\mu}_k) + \\log(\\pi_k)\n\\end{align}\\]"
  },
  {
    "objectID": "slides/10-generative.html#discriminant-function-1",
    "href": "slides/10-generative.html#discriminant-function-1",
    "title": "Generative Models \n",
    "section": "Discriminant Function",
    "text": "Discriminant Function\n\n\\[\\begin{align}\n\\widehat f(\\mathbf{x}) =& ~\\underset{k}{\\arg\\max} \\,\\, - \\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu}_k)' \\Sigma^{-1} (\\mathbf{x}- \\boldsymbol{\\mu}_k) + \\log(\\pi_k)\n\\end{align}\\]\n\n\n\n\\((\\mathbf{x}- \\boldsymbol{\\mu}_k)' \\Sigma^{-1} (\\mathbf{x}- \\boldsymbol{\\mu}_k)\\) is the Mahalanobis distance between \\(\\mathbf{x}\\) and the centroid \\(\\boldsymbol{\\mu}_k\\).\n\nClassify \\(y\\) so that its \\(\\mathbf{x}\\) and the centroid of the labeled class is the closest (after adjusting for the prior). \\[\\begin{align}\n& - \\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu}_k)' \\Sigma^{-1} (\\mathbf{x}- \\boldsymbol{\\mu}_k) + \\log(\\pi_k)\\\\\n&=~ \\mathbf{x}' \\underbrace{\\Sigma^{-1} \\boldsymbol{\\mu}_k}_{\\mathbf{w}_k} \\underbrace{- \\frac{1}{2}\\boldsymbol{\\mu}_k' \\Sigma^{-1} \\boldsymbol{\\mu}_k + \\log(\\pi_k)}_{b_k} + \\text{const of } k \\\\\n&=~ \\mathbf{x}' \\mathbf{w}_k + b_k + \\text{const of } k\\\\\n&=~ \\delta_k(\\mathbf{x}) + \\text{const of } k\n\\end{align}\\] where \\(\\delta_k(\\mathbf{x})\\) the discriminant function which is linear in \\(\\mathbf{x}\\).\n\nby incorporating the covariance matrix and adjusting the for prior. Note that anything that does not depends on the class index \\(k\\) is irrelevant to the decision."
  },
  {
    "objectID": "slides/10-generative.html#discriminant-function-2",
    "href": "slides/10-generative.html#discriminant-function-2",
    "title": "Generative Models \n",
    "section": "Discriminant Function",
    "text": "Discriminant Function\n\n\n\nThe decision boundary of LDA is a linear function of \\(\\mathbf{x}\\).\nThe boundary between two classes \\(k\\) and \\(l\\) is where they have the same density value.\n\n\\[\\begin{align}\n&\\delta_k(\\mathbf{x}) = \\delta_l(\\mathbf{x}) \\\\\n&\\Leftrightarrow \\quad \\mathbf{x}' \\mathbf{w}_k + b_k = \\mathbf{x}' \\mathbf{w}_l + b_l \\\\\n&\\Leftrightarrow \\quad \\mathbf{x}' (\\mathbf{w}_k - \\mathbf{w}_l) + (b_k - b_l) = 0. \\\\\n\\end{align}\\]\n\n\\[\\begin{align}\n&\\delta_k(\\mathbf{x}) - \\delta_l(\\mathbf{x}) \\\\\n\\propto & \\quad \\log \\left(\\pi_k f_k(\\mathbf{x})\\right) -  \\log \\left(\\pi_l f_l(\\mathbf{x}))\\right)\\\\\n\\propto & \\quad \\log \\left(P(Y = k \\mid \\mathbf{x})\\right) -  \\log \\left(P(Y = l \\mid \\mathbf{x})\\right) \\\\\n\\propto & \\quad \\log \\left( \\frac{P(Y = k \\mid \\mathbf{x})}{P(Y = l \\mid \\mathbf{x})}\\right)\n\\end{align}\\]"
  },
  {
    "objectID": "slides/10-generative.html#interpretation-of-lda",
    "href": "slides/10-generative.html#interpretation-of-lda",
    "title": "Generative Models \n",
    "section": "Interpretation of LDA",
    "text": "Interpretation of LDA\n\n\n\n\n\nESL Fig 4.9\n\n\n\n\nCheck ESL\nFind the linear combination \\(Z = a'X\\) such that the between class variance is maximized relative to the within-class variance.\nAgain, the between class variance is the variance of the class means of Z, and the within class variance is the pooled variance about the means. Figure 4.9 shows why this criterion makes sense. Although the direction joining the centroids separates the means as much as possible (i.e., maximizes the between-class variance), there is considerable overlap between the projected classes due to the nature of the covariances. By taking the covariance into account as well, a direction with minimum overlap can be found.\nGaussian classification with common covariances leads to linear decision boundaries. Classification can be achieved by sphering the data with respect to W, and classifying to the closest centroid (modulo log Ïk) in the sphered space. Since only the relative distances to the centroids count, one can confine the data to the subspace spanned by the centroids in the sphered space. This subspace can be further decomposed into successively optimal subspaces in term of centroid separation. This decomposition is identical to the decomposition due to Fisher."
  },
  {
    "objectID": "slides/10-generative.html#masslda",
    "href": "slides/10-generative.html#masslda",
    "title": "Generative Models \n",
    "section": "\nMASS::lda() 1\n",
    "text": "MASS::lda() 1\n\n\n\n\nLDA\n\nlda_fit &lt;- MASS::lda(default ~ balance, data = Default)\nlda_pred &lt;- predict(lda_fit, data = Default)\n(lda_conf &lt;- table(lda_pred$class, Default$default, \n                   dnn = c(\"Predicted\", \"Actual\")))\n\n         Actual\nPredicted   No  Yes\n      No  9643  257\n      Yes   24   76\n\n\n\ntraining accuracy is 0.972\nsensitivity is 0.228\n\n\nLogistic Regression\n\n\n       \n          No  Yes\n  FALSE 9625  233\n  TRUE    42  100\n\n\n\ntraining accuracy is 0.973\nsensitivity is 0.3\n\n\n\nIn python, from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
  },
  {
    "objectID": "slides/10-generative.html#quadratic-discriminant-analysis-qda",
    "href": "slides/10-generative.html#quadratic-discriminant-analysis-qda",
    "title": "Generative Models \n",
    "section": "Quadratic Discriminant Analysis (QDA)",
    "text": "Quadratic Discriminant Analysis (QDA)\n\n\n\n\nLDA assumption for \\(f_k(\\mathbf{x})\\): \\(\\color{blue}{(\\mathbf{x}\\mid y = k) \\sim N_p\\left(\\boldsymbol{\\mu}_k, \\Sigma \\right)}\\)\n\n\n\\(\\Sigma\\): covariance matrix common to all K classes\n\n\n\n\n\n\nQDA assumption for \\(f_k(\\mathbf{x})\\): \\(\\color{blue}{(\\mathbf{x}\\mid y = k) \\sim N_p\\left(\\boldsymbol{\\mu}_k, \\Sigma_k \\right)}\\)\n\n\n\\(\\Sigma_k\\): each class has its own covariance matrix\n\n\n\n\n\n\nThe discriminant function for QDA is\n\\[\\begin{align} \\delta_k(\\mathbf{x}) &= - \\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu}_k)' \\Sigma_k^{-1} (\\mathbf{x}- \\boldsymbol{\\mu}_k) - \\frac{1}{2} \\log |\\Sigma_k |+ \\log(\\pi_k) \\\\\n&= \\mathbf{x}' \\mathbf{W}_k \\mathbf{x}+ \\mathbf{x}'\\mathbf{w}_k + b_k \\end{align}\\] which is a quadratic function of \\(\\mathbf{x}\\).\n\n\nThe boundary between class \\(k\\) and \\(l\\) for QDA is\n\\[\\begin{align}\n\\mathbf{x}' (\\mathbf{W}_k - \\mathbf{W}_l) \\mathbf{x}+ \\mathbf{x}' (\\mathbf{w}_k - \\mathbf{w}_l) + (b_k - b_l) = 0\\\\\n\\end{align}\\]"
  },
  {
    "objectID": "slides/10-generative.html#parameter-estimation-for-qda",
    "href": "slides/10-generative.html#parameter-estimation-for-qda",
    "title": "Generative Models \n",
    "section": "Parameter Estimation for QDA",
    "text": "Parameter Estimation for QDA\nThe estimation procedure is almost the same as LDA.\n\n\\(\\hat{\\pi}_k = n_k / n\\)\nCentroids: \\(\\widehat{\\boldsymbol{\\mu}}_k = \\frac{1}{n_k} \\sum_{i: \\,y_i = k} \\mathbf{x}_i\\)\nIndividual covariance matrix: \\[\\widehat \\Sigma_k = \\frac{1}{n_k-1}\\sum_{i : \\, y_i = k} (\\mathbf{x}_i - \\widehat{\\boldsymbol{\\mu}}_k) (\\mathbf{x}_i - \\widehat{\\boldsymbol{\\mu}}_k)'\\]"
  },
  {
    "objectID": "slides/10-generative.html#two-multivariate-normals-with-different-covariance-matrices",
    "href": "slides/10-generative.html#two-multivariate-normals-with-different-covariance-matrices",
    "title": "Generative Models \n",
    "section": "Two Multivariate Normals with Different Covariance Matrices",
    "text": "Two Multivariate Normals with Different Covariance Matrices\n\n\n\\(\\color{blue}{(\\mathbf{x}\\mid y = k) \\sim N_p\\left(\\boldsymbol{\\mu}_k, \\Sigma_k \\right)}\\)\n\n\\(\\boldsymbol{\\mu}_1 = (0.5, -1)'\\)\n\\(\\boldsymbol{\\mu}_2 = (-0.5, 0.5)'\\)\n\\(\\Sigma_1 = \\begin{pmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{pmatrix}\\)\n\\(\\Sigma_2 = \\begin{pmatrix} 1 & -0.5 \\\\ -0.5 & 1 \\end{pmatrix}\\)\n\\(\\pi_1 = 0.4\\), \\(\\pi_2 = 0.6\\)\nEach class has its own predictor mean and covariance matrix."
  },
  {
    "objectID": "slides/10-generative.html#decision-boundary-by-qda",
    "href": "slides/10-generative.html#decision-boundary-by-qda",
    "title": "Generative Models \n",
    "section": "Decision Boundary by QDA",
    "text": "Decision Boundary by QDA\n\nhttps://freakonometrics.hypotheses.org/tag/qda"
  },
  {
    "objectID": "slides/10-generative.html#massqda",
    "href": "slides/10-generative.html#massqda",
    "title": "Generative Models \n",
    "section": "\nMASS::qda1\n",
    "text": "MASS::qda1\n\n\nLDA\n\n\n         Actual\nPredicted   No  Yes\n      No  9643  257\n      Yes   24   76\n\n\nQDA\n\nqda_fit &lt;- MASS::qda(default ~ balance, data = Default)\nqda_pred &lt;- predict(qda_fit, data = Default)\n(qda_conf &lt;- table(qda_pred$class, Default$default, dnn = c(\"Predicted\", \"Actual\")))\n\n         Actual\nPredicted   No  Yes\n      No  9639  246\n      Yes   28   87\n\n\n\ntraining accuracy is 0.973\nsensitivity is 0.261\nIn python, from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
  },
  {
    "objectID": "slides/10-generative.html#naive-bayes-nb",
    "href": "slides/10-generative.html#naive-bayes-nb",
    "title": "Generative Models \n",
    "section": "Naive Bayes (NB)",
    "text": "Naive Bayes (NB)\n\n\n\n\n\nLDA assumption for \\(f_k(\\mathbf{x})\\): \\(\\color{blue}{(\\mathbf{x}\\mid y = k) \\sim N_p\\left(\\boldsymbol{\\mu}_k, \\Sigma \\right)}\\)\n\n\n\\(\\Sigma\\): covariance matrix common to all K classes\n\n\n\n\n\n\nQDA assumption for \\(f_k(\\mathbf{x})\\): \\(\\color{blue}{(\\mathbf{x}\\mid y = k) \\sim N_p\\left(\\boldsymbol{\\mu}_k, \\Sigma_k \\right)}\\)\n\n\n\\(\\Sigma_k\\): each class has its own covariance matrix\n\n\n\n\n\n\nNB assumption for \\(f_k(\\mathbf{x})\\):\n\nfeatures within each class are independent!\n\\(f_k(\\mathbf{x}) = f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\cdots \\times f_{kp}(x_p)\\)\n\n\n\n\n\n\n\n\nConsidering the joint distribution of predictors is a pain when \\(p\\) is large.\nWe donât believe \\(p\\) predictors are not associated at all, but this ânaiveâ assumption leads to decent results when \\(n\\) is not large enough relative to \\(p\\).\nThe naive independence assumption introduces some bias, but reduces variance.\n\n\n\n\n\\[\\begin{align} p(y = k \\mid \\mathbf{x})  =\\frac{\\pi_kf_k(\\mathbf{x})}{\\sum_{l=1}^K\\pi_lf_l(\\mathbf{x})} = \\frac{\\pi_k\\prod_{j=1}^p f_{kj(x_j)}}{\\sum_{l=1}^K\\pi_l\\prod_{j=1}^p f_{lj(x_j)}} \\end{align}\\]\n\nestimating a joint distribution requires such a huge amount of data"
  },
  {
    "objectID": "slides/10-generative.html#options-for-estimating-f_kjx_j",
    "href": "slides/10-generative.html#options-for-estimating-f_kjx_j",
    "title": "Generative Models \n",
    "section": "Options for Estimating \\(f_{kj}(x_j)\\)\n",
    "text": "Options for Estimating \\(f_{kj}(x_j)\\)\n\n\nNumerical \\(X_j\\)\n\nParametric: \\[(X_j \\mid Y = k) \\sim N\\left(\\mu_{jk}, \\sigma_{jk}^2 \\right).\\] Itâs QDA with additional assumption that \\(\\Sigma_k\\) is diagonal.\nNon-parametric: Estimate \\(f_{kj}(x_j)\\) by (discrete) histogram, (continuous) kernel density estimator, etc.\n\n\n\n\n\n\nCategorical \\(X_j\\)\n\n\n\\(f_{kj}(x_j) \\approx\\) the proportion of training data for \\(x_j\\) corresponding to class \\(k\\)."
  },
  {
    "objectID": "slides/10-generative.html#section-1",
    "href": "slides/10-generative.html#section-1",
    "title": "Generative Models \n",
    "section": "",
    "text": "Class 1\n\n\n\n\n\n\n\n\n\n\nClass 2\n\n\n\n\n\n\nISL Fig. 4.10. Left to right: estimated density of \\(x_1\\), \\(x_2\\) and estimated probability of \\(x_3\\)."
  },
  {
    "objectID": "slides/10-generative.html#e1071naivebayes-1",
    "href": "slides/10-generative.html#e1071naivebayes-1",
    "title": "Generative Models \n",
    "section": "\ne1071::naiveBayes() 1\n",
    "text": "e1071::naiveBayes() 1\n\nQDA\n\n\n         Actual\nPredicted   No  Yes\n      No  9639  246\n      Yes   28   87\n\n\nNaive Bayes\n\nBy default, each feature is Gaussian distributed.\n\n\nnb_fit &lt;- e1071::naiveBayes(default ~ balance, data = Default)\nnb_pred &lt;- predict(nb_fit, Default)\n(nb_conf &lt;- table(nb_pred, Default$default))\n\n       \nnb_pred   No  Yes\n    No  9639  246\n    Yes   28   87\n\n\n\ntraining accuracy is 0.973\nsensitivity is 0.261\nIn Python, from sklearn.naive_bayes import GaussianNB"
  },
  {
    "objectID": "slides/10-generative.html#log-odds-of-lr-and-lda",
    "href": "slides/10-generative.html#log-odds-of-lr-and-lda",
    "title": "Generative Models \n",
    "section": "Log Odds of LR and LDA",
    "text": "Log Odds of LR and LDA\nSuppose class \\(K\\) is the baseline.\n\nLR\n\n\\[\\log \\left( \\frac{P(Y = k \\mid \\mathbf{x})}{P(Y = K \\mid \\mathbf{x})}\\right) = \\beta_{k0} + \\beta_{k1}x_{1} + \\dots + \\beta_{kp}x_{p}\\]\n\n\nLDA\n\n\\[\\log \\left( \\frac{P(Y = k \\mid \\mathbf{x})}{P(Y = K \\mid \\mathbf{x})}\\right) = a_k + \\sum_{j=1}^pc_{kj}x_j\\] where \\(a_k = \\log(\\pi_k / \\pi_K) - \\frac{1}{2}(\\boldsymbol{\\mu}_k + \\boldsymbol{\\mu}_K)'\\Sigma^{-1}(\\boldsymbol{\\mu}_k - \\boldsymbol{\\mu}_K)\\) and \\(c_{kj}\\) is the \\(j\\)th element of \\(\\Sigma^{-1}(\\boldsymbol{\\mu}_k - \\boldsymbol{\\mu}_K)\\).\nBoth log odds are linear in \\(\\mathbf{x}\\)."
  },
  {
    "objectID": "slides/10-generative.html#log-odds-of-qda-and-nb",
    "href": "slides/10-generative.html#log-odds-of-qda-and-nb",
    "title": "Generative Models \n",
    "section": "Log Odds of QDA and NB",
    "text": "Log Odds of QDA and NB\n\nQDA\n\n\\[\\log \\left( \\frac{P(Y = k \\mid \\mathbf{x})}{P(Y = K \\mid \\mathbf{x})}\\right) = a_k + \\sum_{j=1}^pc_{kj}x_j + \\sum_{j=1}^p\\sum_{l=1}^p d_{kjl}x_jx_l\\] The log odds is quadratic in \\(\\mathbf{x}\\), where \\(a_k\\), \\(c_{kj}\\) and \\(d_{kjl}\\) are functions of \\(\\pi_k\\), \\(\\pi_K\\), \\(\\boldsymbol{\\mu}_k\\), \\(\\boldsymbol{\\mu}_K\\), \\(\\Sigma_k\\) and \\(\\Sigma_K\\).\n\n\nNB\n\n\\[\\log \\left( \\frac{P(Y = k \\mid \\mathbf{x})}{P(Y = K \\mid \\mathbf{x})}\\right) = a_k + \\sum_{j=1}^pg_{kj}(x_j)\\] where \\(a_k = \\log(\\pi_k / \\pi_K)\\) and \\(g_{kj}(x_j) = \\log\\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)}\\right)\\).\nThe log odds takes the form of a generalized additive model!"
  },
  {
    "objectID": "slides/10-generative.html#comparison-of-log-odds",
    "href": "slides/10-generative.html#comparison-of-log-odds",
    "title": "Generative Models \n",
    "section": "Comparison of Log Odds",
    "text": "Comparison of Log Odds\n\nLDA is a special case of QDA with \\(d_{kjl} = 0\\) for all \\(k, j\\) and \\(l\\). \\((\\Sigma_1 = \\cdots = \\Sigma_k = \\Sigma)\\)\n\nAny classifier with a linear decision boundary is a special case of NB with \\(g_{kj}(x_j) = c_{kj}x_j.\\)\n\nLDA is a special case of NB. Really !? ð²\n\n\nNB is a special case of LDA if \\(f_{kj}(x_j) = N\\left(\\mu_{jk}, \\sigma_{j}^2 \\right)\\). \\((\\Sigma = \\text{diag}(\\sigma_1^2, \\dots, \\sigma_p^2))\\)\n\nNeither QDA nor NB is a special case of the other\n\nNB is more\n\n\nflexible for any transformation of \\(x_j\\), \\(g_{kj}(x_j)\\).\n\nrestrictive because of its pure additive fit with no quadratic interactions \\(d_{kjl}x_jx_l\\)."
  },
  {
    "objectID": "slides/10-generative.html#which-method-to-use",
    "href": "slides/10-generative.html#which-method-to-use",
    "title": "Generative Models \n",
    "section": "Which Method to Use",
    "text": "Which Method to Use\n\nNone of these methods uniformly dominates the others.\n\nThe choice of method depends on\n\nthe true distribution of \\(X_j\\)s in each class\nthe sizes of \\(n\\) and \\(p\\) that controls bias-variance trade-off\n\n\n\n\n\nLDA outperforms LR when predictors are approximately normal, but use LR when inputs are away from normal.\nUse LDA but not QDA when variability of predictors are similar among all \\(K\\) classes.\nDo not use naive Bayes when predictors are clearly correlated.\nUse QDA when predictors clearly have different covariance structure for each class.\n\n\nLDA outperforms LR when predictors are approximately normal, but use LR when data are away from normal.\nUse LDA but not QDA when variability of predictors are similar among all \\(K\\) classes. QDA fits a more flexible classifier than necessary.\nDo not use naive Bayes when predictors are clearly correlated.\nUse QDA when predictors clearly have different covariance structure for each class."
  },
  {
    "objectID": "slides/10-generative.html#which-method-to-use-1",
    "href": "slides/10-generative.html#which-method-to-use-1",
    "title": "Generative Models \n",
    "section": "Which Method to Use",
    "text": "Which Method to Use\n\nQDA is more flexible than LDA (more parameters to be estimated), and hence low bias and high variance.\nLDA tends to be better than QDA if \\(n\\) is relatively small, and so reducing variance is crucial.\nQDA is recommended if \\(n\\) is relatively large so that the variance of the classifier is not a major concern.\nNaive Bayes works when \\(n\\) is not large enough relative to \\(p\\).\n\n\\(n\\) small more bias; \\(p\\) small less bias small \\(n\\) large \\(p\\): low bias high variance, so reducing variance by using less flexible method\n\n\n\n\nmssc6250-s25.github.io/website"
  },
  {
    "objectID": "slides/13-svm.html#support-vector-machines-svms",
    "href": "slides/13-svm.html#support-vector-machines-svms",
    "title": "Support Vector Machines \n",
    "section": "Support Vector Machines (SVMs)",
    "text": "Support Vector Machines (SVMs)\n\nSVMs have been shown to perform well in a variety of settings, and are often considered one of the best âout of the boxâ classifiers.\nStart with the maximal margin classifier (1960s), then the support vector classifier (1990s), and then the support vector machine."
  },
  {
    "objectID": "slides/13-svm.html#classifier",
    "href": "slides/13-svm.html#classifier",
    "title": "Support Vector Machines \n",
    "section": "Classifier",
    "text": "Classifier\n\n\\({\\cal D}_n = \\{\\mathbf{x}_i, y_i\\}_{i=1}^n\\)\nIn SVM, we code the binary outcome \\(y\\) as 1 or -1, representing one class and the other.\nThe goal is to find a linear classifier \\(f(\\mathbf{x}) = \\beta_0 + \\mathbf{x}' \\boldsymbol \\beta\\) so that the classification rule is the sign of \\(f(\\mathbf{x})\\):\n\n\\[\n\\hat{y} =\n\\begin{cases}\n        +1, \\quad \\text{if} \\quad f(\\mathbf{x}) &gt; 0\\\\\n        -1, \\quad \\text{if} \\quad f(\\mathbf{x}) &lt; 0\n\\end{cases}\n\\]"
  },
  {
    "objectID": "slides/13-svm.html#separating-hyperplane",
    "href": "slides/13-svm.html#separating-hyperplane",
    "title": "Support Vector Machines \n",
    "section": "Separating Hyperplane",
    "text": "Separating Hyperplane\n\nThe \\(f(\\mathbf{x}) = \\beta_0 + \\mathbf{x}' \\boldsymbol \\beta= 0\\) is a hyperplane, which is a subspace of dimension \\(p-1\\) in the \\(p\\)-dimensional space.\n\\(f(\\mathbf{x}) = \\beta_0 + \\beta_1X_1+\\beta_2X_2 = 0\\) is a straight line (hyperplane of dimension one) in the 2-dimensional space.\nThe classification rule is \\(y_i f(\\mathbf{x}_i) &gt;0\\)."
  },
  {
    "objectID": "slides/13-svm.html#maximum-margin-classifier",
    "href": "slides/13-svm.html#maximum-margin-classifier",
    "title": "Support Vector Machines \n",
    "section": "Maximum-margin Classifier",
    "text": "Maximum-margin Classifier\n\nIf our data can be perfectly separated using a hyperplane, there exists an infinite number of such hyperplanes. But which one is the best?\nA natural choice is the maximal margin hyperplane (optimal separating hyperplane), which is the separating hyperplane that is farthest from the training points.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthe maximal margin hyperplane depends on the support vectors. If the support vectors are moved, the maximal margin hyperplane would move too."
  },
  {
    "objectID": "slides/13-svm.html#maximum-margin-classifier-1",
    "href": "slides/13-svm.html#maximum-margin-classifier-1",
    "title": "Support Vector Machines \n",
    "section": "Maximum-margin Classifier",
    "text": "Maximum-margin Classifier\n\nlibrary(e1071)\nsvm_fit &lt;- svm(y ~ ., data = data.frame(x, y), type = 'C-classification', \n               kernel = 'linear', scale = FALSE, cost = 10000)\n\n\n\n\n\nThe training points lied on the dashed lines are support vectors:\n\nif they were moved, the maximal margin hyperplane would move too.\nthe hyperplane depends directly on the support vectors, but not on the other observations, provided that their movement does not cause it to cross the boundary.\n\n\nIt can lead to overfitting when \\(p\\) is large. (No misclassification on training set)\nHope the classifier will also have a large margin on the test data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe use the e1071 package to fit the SVM. There is a cost parameter C , with default value 1. This parameter has a significant impact on non-separable problems. However, for our separable case, we will set this to be a very large value, meaning that the cost for having a wrong classification is very large. We also need to specify the linear kernel.\n\nindex gives the index of all support vectors\ncoefs provides the yiÎ±i for the support vectors\nSV are the xi values correspond to the support vectors\nrho is negative Î²0\n\nthey âsupportâ the maximal margin hyperplane in the sense vector that if these points were moved slightly then the maximal margin hyperplane would move as well.\nhttps://afit-r.github.io/svm\nkernlab::ksvm()\nwe can compute the (perpendicular) distance from each training observation to a given separating hyperplane; the smallest such distance is the minimal distance from the observations to the hyperplane, and is known as the margin. The maximal margin hyperplane is the separating hyperplane for which the margin is margin largestâthat is, it is the hyperplane that has the farthest minimum distance to the training observations.\n\nUse from sklearn import svm for Python implementation."
  },
  {
    "objectID": "slides/13-svm.html#linearly-separable-svm",
    "href": "slides/13-svm.html#linearly-separable-svm",
    "title": "Support Vector Machines \n",
    "section": "Linearly Separable SVM",
    "text": "Linearly Separable SVM\nIn linear SVM, \\(f(\\mathbf{x}) = \\beta_0 + \\mathbf{x}' \\boldsymbol \\beta\\). When \\(f(\\mathbf{x}) = 0\\), it corresponds to a hyperplane that separates the two classes:\n\\[\\{ \\mathbf{x}: \\beta_0 + \\mathbf{x}'\\boldsymbol \\beta = 0 \\}\\]\n\nFor this separable case, all observations with \\(y_i = 1\\) are on one side \\(f(\\mathbf{x}) &gt; 0\\), and observations with \\(y_i = -1\\) are on the other side.\nThe distance from any point \\(\\mathbf{x}_0\\) to the hyperplane is\n\n\\[\\frac{1}{\\lVert \\boldsymbol \\beta\\lVert} |f(\\mathbf{x}_0)|\\] For \\(p = 2\\), and the plane \\(\\beta_0 + \\beta_1 X_1 + \\beta_2X_2 = 0\\), the distance is \\[ \\frac{ |\\beta_0 + \\beta_1 x_{01} + \\beta_2x_{02}|}{\\sqrt{\\beta_1^2 + \\beta^2_2}}\\]"
  },
  {
    "objectID": "slides/13-svm.html#optimization-for-linearly-separable-svm",
    "href": "slides/13-svm.html#optimization-for-linearly-separable-svm",
    "title": "Support Vector Machines \n",
    "section": "Optimization for Linearly Separable SVM",
    "text": "Optimization for Linearly Separable SVM\n\\[\\begin{align}\n\\underset{\\boldsymbol \\beta, \\beta_0, M}{\\text{max}} \\quad & M \\\\\n\\text{s.t.} \\quad & \\frac{1}{\\lVert \\boldsymbol \\beta\\lVert} y_i(\\mathbf{x}' \\boldsymbol \\beta+ \\beta_0) \\geq M, \\,\\, i = 1, \\ldots, n.\n\\end{align}\\]\n\nThe constraint requires that each point be on the correct side of the hyperplane, with some cushion.\nThe scale of \\(\\boldsymbol \\beta\\) can be arbitrary, so just set it as \\(\\lVert \\boldsymbol \\beta\\rVert = 1\\):\n\n\\[\\begin{align}\n\\underset{\\boldsymbol \\beta, \\beta_0, M}{\\text{max}} \\quad & M \\\\\n\\text{s.t.}\n\\quad & \\lVert \\boldsymbol \\beta\\lVert = 1, \\\\\n\\quad &  y_i(\\mathbf{x}' \\boldsymbol \\beta+ \\beta_0) \\geq M, \\,\\, i = 1, \\ldots, n.\n\\end{align}\\]\n\n\nHow to solve it? Learn it in MSSC 5650."
  },
  {
    "objectID": "slides/13-svm.html#linearly-non-separable-svm-with-slack-variables",
    "href": "slides/13-svm.html#linearly-non-separable-svm-with-slack-variables",
    "title": "Support Vector Machines \n",
    "section": "Linearly Non-separable SVM with Slack Variables",
    "text": "Linearly Non-separable SVM with Slack Variables\n\nOften, no separating hyperplane exists, so there is no maximal margin classifier.\nThe previous optimization problem has no solution with \\(M &gt; 0\\).\nIdea: develop a hyperplane that almost separates the classes, using a so-called soft margin: soft margin classifier."
  },
  {
    "objectID": "slides/13-svm.html#why-linearly-non-separable-support-vector-classifier",
    "href": "slides/13-svm.html#why-linearly-non-separable-support-vector-classifier",
    "title": "Support Vector Machines \n",
    "section": "Why Linearly Non-separable Support Vector Classifier",
    "text": "Why Linearly Non-separable Support Vector Classifier\n\nEven if a separating hyperplane does exist, the maximum-margin classifier might not be desirable.\nThe maximal margin hyperplane is extremely sensitive to a change in a single observation: it may overfit the training data. (low-bias high-variance)\n\n\n\n\n\n\nSource: ISL Fig. 9.5"
  },
  {
    "objectID": "slides/13-svm.html#soft-margin-classifier",
    "href": "slides/13-svm.html#soft-margin-classifier",
    "title": "Support Vector Machines \n",
    "section": "Soft Margin Classifier",
    "text": "Soft Margin Classifier\n\n\nConsider a classifier based on a hyperplane that does NOT perfectly separate the two classes, but\n\nBetter classification of most of the training observations.\nGreater robustness to individual observations\n\n\nIt could be worthwhile to misclassify a few training points in order to do a better job in classifying the remaining observations.\nAllow some points to be on the incorrect side of the margin (8 and 1), or even the incorrect side of the hyperplane (12 and 11 training points misclassified by the classifier).\n\n\n\n\n\n\nSource: ISL Fig. 9.6"
  },
  {
    "objectID": "slides/13-svm.html#optimization-for-soft-margin-classifier",
    "href": "slides/13-svm.html#optimization-for-soft-margin-classifier",
    "title": "Support Vector Machines \n",
    "section": "Optimization for Soft Margin Classifier",
    "text": "Optimization for Soft Margin Classifier\n\\[\\begin{align}\n\\underset{\\boldsymbol \\beta, \\beta_0, \\epsilon_1, \\dots, \\epsilon_n, M}{\\text{max}} \\quad & M \\\\\n\\text{s.t.}\n\\quad & \\lVert \\boldsymbol \\beta\\lVert = 1, \\\\\n\\quad &  y_i(\\mathbf{x}' \\boldsymbol \\beta+ \\beta_0) \\geq M(1 - \\epsilon_i), \\\\\n  \\quad & \\epsilon_i \\ge 0, \\sum_{i=1}^n\\epsilon_i \\le B, \\,\\, i = 1, \\ldots, n,\n\\end{align}\\] where \\(B &gt; 0\\) is a tuning parameter.\n\n\\(\\epsilon_1, \\dots, \\epsilon_n\\) are slack variables that allow individual points to be on the wrong side of the margin or the hyperplane.\n\nThe \\(i\\)th point is on the\n\n\ncorrect side of the margin when \\(\\epsilon_i = 0\\)\n\n\nwrong side of the margin when \\(\\epsilon_i &gt; 0\\)\n\n\nwrong side of the hyperplane when \\(\\epsilon_i &gt; 1\\)\n\n\n\n\n\ntells us where the \\(i\\)th observation is located, relative to the hyperplane and relative to the margin."
  },
  {
    "objectID": "slides/13-svm.html#optimization-for-soft-margin-classifier-1",
    "href": "slides/13-svm.html#optimization-for-soft-margin-classifier-1",
    "title": "Support Vector Machines \n",
    "section": "Optimization for Soft Margin Classifier",
    "text": "Optimization for Soft Margin Classifier\n\\[\\begin{align}\n\\underset{\\boldsymbol \\beta, \\beta_0, \\epsilon_1, \\dots, \\epsilon_n, M}{\\text{max}} \\quad & M \\\\\n\\text{s.t.}\n\\quad & \\lVert \\boldsymbol \\beta\\lVert = 1, \\\\\n\\quad &  y_i(\\mathbf{x}' \\boldsymbol \\beta+ \\beta_0) \\geq M(1 - \\epsilon_i), \\\\\n  \\quad & \\epsilon_i \\ge 0, \\sum_{i=1}^n\\epsilon_i \\le B, \\,\\, i = 1, \\ldots, n,\n\\end{align}\\] where \\(B &gt; 0\\) is a tuning parameter.\n\n\n\\(B\\) determines the number and severity of the violations to the margin/hyperplane we tolerate.\n\n\n\\(B = 0\\): no budget for violations (\\(\\epsilon_1 = \\cdots = \\epsilon_n = 0\\))\n\n\\(B &gt; 0\\): no more than \\(B\\) points can be on the wrong side of the hyperplane. (\\(\\epsilon_i &gt; 1\\))\nAs \\(B\\) increases, more violations and wider margin. (more bias less variance)\nChoose \\(B\\) via cross-validation.\n\n\n\nB as a budget for the amount that the margin can be violated by the n observations"
  },
  {
    "objectID": "slides/13-svm.html#optimization-for-soft-margin-classifier-2",
    "href": "slides/13-svm.html#optimization-for-soft-margin-classifier-2",
    "title": "Support Vector Machines \n",
    "section": "Optimization for Soft Margin Classifier",
    "text": "Optimization for Soft Margin Classifier"
  },
  {
    "objectID": "slides/13-svm.html#optimization-for-soft-margin-classifier-3",
    "href": "slides/13-svm.html#optimization-for-soft-margin-classifier-3",
    "title": "Support Vector Machines \n",
    "section": "Optimization for Soft Margin Classifier",
    "text": "Optimization for Soft Margin Classifier\n\n\n\n\n\n\nWarning\n\n\n\nThe argument cost in e1071::svm() and C in sklearn.svm.SVC() is the \\(C\\) defined in the primal form \\[\\begin{align}\n\\underset{\\boldsymbol \\beta, \\beta_0}{\\text{min}} \\quad & \\frac{1}{2}\\lVert \\boldsymbol \\beta\\rVert^2 + C \\sum_{i=1}^n \\epsilon_i \\\\\n\\text{s.t} \\quad & y_i (\\mathbf{x}_i' \\boldsymbol \\beta+ \\beta_0) \\geq (1 - \\epsilon_i), \\\\\n\\text{} \\quad & \\epsilon_i \\geq 0, \\,\\, i = 1, \\ldots, n,\n\\end{align}\\]\n\nso small cost \\(C\\) means larger budget \\(B\\)."
  },
  {
    "objectID": "slides/13-svm.html#svm-lda-and-logistic-regression",
    "href": "slides/13-svm.html#svm-lda-and-logistic-regression",
    "title": "Support Vector Machines \n",
    "section": "SVM, LDA and Logistic Regression",
    "text": "SVM, LDA and Logistic Regression\n\n\n\n\n\n\nNote\n\n\n\n\nSVM decision rule is based only on a subset of the training data (robust to the behavior of data that are far away from the hyperplane.)\nLDA depends on the mean of all of the observations within each class, and within-class covariance matrix computed using all of the data.\nLogistic regression, unlike LDA, is insensitive to observations far from the decision boundary too."
  },
  {
    "objectID": "slides/13-svm.html#classification-with-non-linear-decision-boundaries",
    "href": "slides/13-svm.html#classification-with-non-linear-decision-boundaries",
    "title": "Support Vector Machines \n",
    "section": "Classification with Non-Linear Decision Boundaries",
    "text": "Classification with Non-Linear Decision Boundaries\n\nThe soft margin classifier is a natural approach for classification in the two-class setting, if the boundary between the two classes is linear.\nIn practice we are often faced with non-linear class boundaries."
  },
  {
    "objectID": "slides/13-svm.html#classification-with-non-linear-decision-boundaries-1",
    "href": "slides/13-svm.html#classification-with-non-linear-decision-boundaries-1",
    "title": "Support Vector Machines \n",
    "section": "Classification with Non-Linear Decision Boundaries",
    "text": "Classification with Non-Linear Decision Boundaries\n\nIn regression, we enlarge the feature space using functions of the predictors to address this non-linearity.\nIn SVM (logistic regression too!), we could address non-linear boundaries by enlarging the feature space.\nFor example, rather than fitting a support vector classifier using \\(p\\) features, \\(X_1, \\dots, X_p\\), we could instead fit a support vector classifier using \\(2p\\) features \\(X_1,X_1^2,X_2,X_2^2, \\dots , X_p, X_p^2\\).\n\n\n\n\\[\\begin{align}\n\\underset{\\beta_0, \\beta_{11}, \\beta_{12}, \\dots, \\beta_{p1}, \\beta_{p2}, \\epsilon_1, \\dots, \\epsilon_n, M}{\\text{max}} \\quad & M \\\\\n\\text{s.t.}\n\\quad &  y_i\\left(\\beta_0 + \\sum_{i = 1}^n \\beta_{j1}x_{ij} + \\sum_{i = 1}^n \\beta_{j2}x_{ij}^2\\right) \\geq M(1 - \\epsilon_i), \\\\\n  \\quad & \\epsilon_i \\ge 0, \\sum_{i=1}^n\\epsilon_i \\le B, \\,\\, i = 1, \\ldots, n,\\\\\n  \\quad & \\sum_{j=1}^p\\sum_{k=1}^2\\beta_{jk}^2 = 1.\n\\end{align}\\]\n\nIn the enlarged feature space, the decision boundary that results from (9.16) is in fact linear. But in the original feature space, the decision boundary is of the form q(x) = 0, where q is a quadratic polynomial, and its solutions are generally non-linear.\nThe support vector machine, which we present next, allows us to enlarge the feature space used by the support vector classifier in a way that leads to efficient computations"
  },
  {
    "objectID": "slides/13-svm.html#enlarge-feature-space",
    "href": "slides/13-svm.html#enlarge-feature-space",
    "title": "Support Vector Machines \n",
    "section": "Enlarge Feature Space",
    "text": "Enlarge Feature Space\n\n\n\n\n\nSource: https://teazrq.github.io/stat542/notes/SVM.pdf"
  },
  {
    "objectID": "slides/13-svm.html#nonlinear-boundaries",
    "href": "slides/13-svm.html#nonlinear-boundaries",
    "title": "Support Vector Machines \n",
    "section": "Nonlinear Boundaries",
    "text": "Nonlinear Boundaries\n\n\n\n\n\nSource: https://teazrq.github.io/stat542/notes/SVM.pdf"
  },
  {
    "objectID": "slides/13-svm.html#solution-to-support-vector-classifier",
    "href": "slides/13-svm.html#solution-to-support-vector-classifier",
    "title": "Support Vector Machines \n",
    "section": "Solution to Support Vector Classifier",
    "text": "Solution to Support Vector Classifier\n\nThe solution to the support vector classifier optimization involves only the inner products of the observations: \\(\\langle \\mathbf{x}_i, \\mathbf{x}_{i'} \\rangle = \\sum_{j=1}^px_{ij}x_{i'j}\\)\nThe linear support vector classifier can be represented as \\[f(\\mathbf{x}) = \\beta_0 + \\sum_{i\\in \\mathcal{S}}\\alpha_i\\langle \\mathbf{x}, \\mathbf{x}_{i} \\rangle\\] where \\(\\mathcal{S}\\) is the collection of indices of the support points.\n\\(\\alpha_i\\) is nonzero only for the support vectors in the solution.\n\n\n\n\nTo evaluate the function \\(f(\\mathbf{x}_0)\\), we compute \\(\\langle \\mathbf{x}_0, \\mathbf{x}_{i} \\rangle\\)."
  },
  {
    "objectID": "slides/13-svm.html#nonlinear-svm-via-kernel-trick",
    "href": "slides/13-svm.html#nonlinear-svm-via-kernel-trick",
    "title": "Support Vector Machines \n",
    "section": "Nonlinear SVM via Kernel Trick",
    "text": "Nonlinear SVM via Kernel Trick\n\n\n\n\n\nThe support vector machine (SVM) is an extension of the support vector classifier that results from enlarging the feature space in a specific way, using kernels.\nThe kernel approach is an efficient computational approach for enlarging our feature space and non-linear boundary.\n\n\n\nKernel Trick: \\[f(\\mathbf{x}) = \\beta_0 + \\sum_{i\\in \\mathcal{S}}\\alpha_i K\\left(\\mathbf{x}, \\mathbf{x}_{i}\\right) \\]\nLinear kernel: \\(K\\left(\\mathbf{x}_0, \\mathbf{x}_{i}\\right) = \\langle \\mathbf{x}_0, \\mathbf{x}_{i'} \\rangle = \\sum_{j=1}^px_{0j}x_{ij}\\)\nPolynomial kernel: \\(K\\left(\\mathbf{x}_0, \\mathbf{x}_{i}\\right) = \\left(1 + \\sum_{j=1}^px_{0j}x_{ij}\\right)^d\\)\nRadial kernel: \\(K\\left(\\mathbf{x}_0, \\mathbf{x}_{i'}\\right) = \\exp \\left(-\\gamma\\sum_{j=1}^p (x_{0j}-x_{ij})^2 \\right)\\)"
  },
  {
    "objectID": "slides/13-svm.html#radial-kernel-decision-doundary",
    "href": "slides/13-svm.html#radial-kernel-decision-doundary",
    "title": "Support Vector Machines \n",
    "section": "Radial Kernel Decision Doundary",
    "text": "Radial Kernel Decision Doundary"
  },
  {
    "objectID": "slides/13-svm.html#svm-as-a-penalized-model",
    "href": "slides/13-svm.html#svm-as-a-penalized-model",
    "title": "Support Vector Machines \n",
    "section": "SVM as a Penalized Model",
    "text": "SVM as a Penalized Model\n\\[\\begin{align}\n\\underset{\\boldsymbol \\beta, \\beta_0, \\epsilon_1, \\dots, \\epsilon_n}{\\text{max}} \\quad & M \\\\\n\\text{s.t.}\n\\quad & \\lVert \\boldsymbol \\beta\\lVert = 1, \\\\\n\\quad &  y_i(\\mathbf{x}_i' \\boldsymbol \\beta+ \\beta_0) \\geq M(1 - \\epsilon_i), \\\\\n  \\quad & \\epsilon_i \\ge 0, \\sum_{i=1}^n\\epsilon_i \\le B, \\,\\, i = 1, \\ldots, n,\n\\end{align}\\]\n\n\\[\\begin{align}\n\\underset{\\boldsymbol \\beta, \\beta_0}{\\text{min}} \\left\\{ \\sum_{i=1}^n  \\max \\left[ 0, 1 - y_i (\\mathbf{x}_i' \\boldsymbol \\beta+ \\beta_0) \\right] + \\lambda \\lVert \\boldsymbol \\beta\\lVert ^ 2 \\right\\}\n\\end{align}\\] where \\(\\sum_{i=1}^n  \\max \\left[ 0, 1 - y_i (\\mathbf{x}' \\boldsymbol \\beta+ \\beta_0) \\right]\\) is known as hinge loss.\n\nLarge \\(\\lambda\\) (large \\(B\\)): small \\(\\beta_j\\)s, high-bias and low-variance.\nSmall \\(\\lambda\\) (small \\(B\\)): low-bias and high-variance."
  },
  {
    "objectID": "slides/13-svm.html#loss-functions",
    "href": "slides/13-svm.html#loss-functions",
    "title": "Support Vector Machines \n",
    "section": "Loss Functions",
    "text": "Loss Functions\n\n\n\nThe hinge loss is zero for observations for which \\(y_i (\\mathbf{x}' \\boldsymbol \\beta+ \\beta_0) \\ge 1\\) (correct side of the margin).\nThe logistic loss is not zero anywhere.\nSVM is better when classes are well separated.\nLogistic regression is preferred in more overlapping regimes.\n\n\n\n\n\n\n\n\n\n\n\n\n, but small for points far from the boundary.\n\n\n\n\nmssc6250-s25.github.io/website"
  },
  {
    "objectID": "slides/11-knn.html#nonparametric-examplar-based-methods",
    "href": "slides/11-knn.html#nonparametric-examplar-based-methods",
    "title": "K-nearest Neighbors \n",
    "section": "Nonparametric Examplar-based Methods",
    "text": "Nonparametric Examplar-based Methods\n\nSo far we have mostly focused on parametric models, either unconditional \\(p({\\bf y} \\mid \\boldsymbol \\theta)\\) or conditional \\(p({\\bf y} \\mid \\mathbf{x}, \\boldsymbol \\theta)\\), where \\(\\boldsymbol \\theta\\) is a fixed-dimensional vector of parameters. 1\nThe parameters are estimated from the training set \\(\\mathcal{D} = \\{(\\mathbf{x}_i, \\mathbf{y}_i)\\}_{i=1}^n\\) but after model fitting, the data is not used anymore.\n\n\n\nThe nonparametric models that keep the training data around at the test time are called examplar-based models, instance-based learning or memory-based learning.\n\nK-nearest neighbors classification and regression\nKernel regression\nLocal regression, e.g., LOESS\nKernel density estimation\n\n\n\n\n\n\nThe examplar-based models usually perform a local averaging technique based on the similarity or distance between a test input \\(\\mathbf{x}_0\\) and each of the training inputs \\(\\mathbf{x}_i\\).\n\n\n\nFor example, \\(\\beta\\) coefficients in linear regression."
  },
  {
    "objectID": "slides/11-knn.html#k-nearest-neighbor-regression",
    "href": "slides/11-knn.html#k-nearest-neighbor-regression",
    "title": "K-nearest Neighbors \n",
    "section": "K-nearest Neighbor Regression",
    "text": "K-nearest Neighbor Regression\n\nK-nearest neighbor (KNN) is a nonparametric method that can be used for regression and classification.\nIn KNN, we donât have parameters \\(\\boldsymbol \\beta\\), and \\(f(\\mathbf{x}_0) = \\mathbf{x}_0'\\boldsymbol \\beta\\) in linear regression.1\nWe directly estimate \\(f(\\mathbf{x}_0)\\) using our examples or memory.\n\n\\[ \\widehat{y}_0 = \\frac{1}{k} \\sum_{x_i \\in N_k(x_0)} y_i,\\] where the neighborhood of \\(x_0\\), \\(N_k(x_0)\\), defines the \\(k\\) training data points that are closest to \\(x_0\\).\n\nCloseness (Similarity) is defined using a distance measure, such as the Euclidean distance.\n\\(\\widehat{y}_0 = \\widehat{f}(x_0) = \\mathbf{x}_0'\\hat{\\boldsymbol \\beta}\\)."
  },
  {
    "objectID": "slides/11-knn.html#nearest-neighbor-regression",
    "href": "slides/11-knn.html#nearest-neighbor-regression",
    "title": "K-nearest Neighbors \n",
    "section": "1-Nearest Neighbor Regression",
    "text": "1-Nearest Neighbor Regression"
  },
  {
    "objectID": "slides/11-knn.html#tuning-k",
    "href": "slides/11-knn.html#tuning-k",
    "title": "K-nearest Neighbors \n",
    "section": "Tuning \\(k\\)\n",
    "text": "Tuning \\(k\\)\n\n\n\\(y_i = 2\\sin(x_i) + \\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0, 1), \\quad i = 1, \\dots, 200\\)\n\n\n\nput all weights on one single training point. The predictive value at some test \\(x\\) relies solely on one single training point that is closest to \\(x\\).\nCarry lots of noises"
  },
  {
    "objectID": "slides/11-knn.html#the-bias-variance-trade-off",
    "href": "slides/11-knn.html#the-bias-variance-trade-off",
    "title": "K-nearest Neighbors \n",
    "section": "The Bias-variance Trade-off",
    "text": "The Bias-variance Trade-off\n\nIf we consider different values of \\(k\\), we can observe the trade-off between bias and variance. - Whatâs the training error when K=1? - What is the fitted curve like when K is all training data points?"
  },
  {
    "objectID": "slides/11-knn.html#bias-variance-trade-off",
    "href": "slides/11-knn.html#bias-variance-trade-off",
    "title": "K-nearest Neighbors \n",
    "section": "Bias-Variance Trade-Off",
    "text": "Bias-Variance Trade-Off\n\\[\\begin{aligned}\n\\text{E}\\Big[ \\big( Y - \\widehat f(x_0) \\big)^2 \\Big]\n&= \\underbrace{\\text{E}\\Big[ ( Y - f(x_0))^2 \\big]}_{\\text{Irreducible Error}} +\n\\underbrace{\\Big(f(x_0) - \\text{E}[\\widehat f(x_0)]\\Big)^2}_{\\text{Bias}^2} +\n\\underbrace{\\text{E}\\Big[ \\big(\\widehat f(x_0) - \\text{E}[\\widehat f(x_0)] \\big)^2 \\Big]}_{\\text{Variance}}\n\\end{aligned}\\]\n\nAs \\(k \\uparrow\\), bias \\(\\uparrow\\) and variance \\(\\downarrow\\) (smoother)\nAs \\(k \\downarrow\\), bias \\(\\downarrow\\) and variance \\(\\uparrow\\) (more wiggly)\n\nwhen k is small, the estimated model is unstable. Also, each time we observe a new training data, we may get a very different estimation. (due to the closest sample and ) When k is large, the estimated model eventually deviates (systematically) from the truth. If we use more âneighbouringâ points, say k, the variance would reduce to approximately Ï2/k. But the bias2 will increase as neighbours are far away from x0. k determines the model complexity"
  },
  {
    "objectID": "slides/11-knn.html#degrees-of-freedom",
    "href": "slides/11-knn.html#degrees-of-freedom",
    "title": "K-nearest Neighbors \n",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\n\n\\(k\\) determines the model complexity and degrees of freedom (df).\nIn general, the df can be defined as\n\n\\[\\text{df}(\\hat{f}) = \\frac{1}{\\sigma^2}\\text{Trace}\\left( \\mathrm{Cov}(\\hat{\\mathbf{y}}, \\mathbf{y})\\right)= \\frac{1}{\\sigma^2}\\sum_{i=1}^n \\mathrm{Cov}(\\hat{y}_i, y_i)\\]\n\n\\(k = 1\\): \\(\\hat{f}(x_i) = y_i\\) and \\(\\text{df}(\\hat{f}) = n\\)\n\\(k = n\\): \\(\\hat{f}(x_i) = \\bar{y}\\) and \\(\\text{df}(\\hat{f}) = 1\\)\nFor general \\(k\\), \\(\\text{df}(\\hat{f}) = n/k\\).\n\n\n\nLinear regression with \\(p\\) coefficients: \\(\\text{df}(\\hat{f}) = \\text{Trace}\\left( {\\bf H} \\right) = p\\)\nFor any linear smoother \\(\\hat{\\mathbf{y}} = {\\bf S} \\mathbf{y}\\), \\(\\text{df}(\\hat{f}) = \\text{Trace}({\\bf S})\\).\n\n\nHow y and y_hat move together, how true y and fitted y move together\nThe covariance term quantifies how much the predictions depend on the observed data.\n\n\\(\\sum_{i=1}^n \\mathrm{Cov}(\\hat{y}_i, y_i)\\) measures how much \\(\\hat{y}_i\\) depend on the observed data.\nIf a model is very flexible (e.g., an overfitted model), the predictions are highly dependent on the observed values, meaning higher covariance.\nas a model increases in complexity, it becomes more data-dependent, meaning the predicted values closely follow the observed ones.\nSimpler models have lower covariance, meaning they do not adapt too closely to the specific fluctuations in the data.\nDegrees of freedom measure the modelâs ability to fit data independently."
  },
  {
    "objectID": "slides/11-knn.html#k-nearest-neighbor-classification",
    "href": "slides/11-knn.html#k-nearest-neighbor-classification",
    "title": "K-nearest Neighbors \n",
    "section": "K-nearest Neighbor Classification",
    "text": "K-nearest Neighbor Classification\n\nInstead of taking average in regression, KNN classification uses majority voting:\n\n\nLook for the most popular class label among its neighbors.\n\n\n1NN decision boundary is a Voronoi diagram.\n\n\n\n\n\n\n\n\n\n\n\n\nVoronoi tessellation. https://en.wikipedia.org/wiki/Voronoi_diagram\nEvery boundary is the middle line of any two training points.\nIn each region, there is one training point, and all possible data points in the region will be labelled as the category the training point belongs to."
  },
  {
    "objectID": "slides/11-knn.html#example-esl.mixture.rda",
    "href": "slides/11-knn.html#example-esl.mixture.rda",
    "title": "K-nearest Neighbors \n",
    "section": "Example: ESL.mixture.rda\n",
    "text": "Example: ESL.mixture.rda\n\n\n\nThe KNN decision boundary is nonlinear.\nR: class::knn(), kknn::kknn(), FNN::knn(), parsnip::nearest_neighbor()\nPython: from sklearn.neighbors import KNeighborsClassifier"
  },
  {
    "objectID": "slides/11-knn.html#example-esl.mixture.rda-1",
    "href": "slides/11-knn.html#example-esl.mixture.rda-1",
    "title": "K-nearest Neighbors \n",
    "section": "Example: ESL.mixture.rda\n",
    "text": "Example: ESL.mixture.rda"
  },
  {
    "objectID": "slides/11-knn.html#confusion-matrix",
    "href": "slides/11-knn.html#confusion-matrix",
    "title": "K-nearest Neighbors \n",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\n\nknn_fit &lt;- class::knn(train = x, test = x, cl = y, k = 15)\ncaret::confusionMatrix(table(knn_fit, y))\n\nConfusion Matrix and Statistics\n\n       y\nknn_fit  0  1\n      0 82 13\n      1 18 87\n                                          \n               Accuracy : 0.845           \n                 95% CI : (0.7873, 0.8922)\n    No Information Rate : 0.5             \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.69            \n                                          \n Mcnemar's Test P-Value : 0.4725          \n                                          \n            Sensitivity : 0.8200          \n            Specificity : 0.8700          \n         Pos Pred Value : 0.8632          \n         Neg Pred Value : 0.8286          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4100          \n   Detection Prevalence : 0.4750          \n      Balanced Accuracy : 0.8450          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "slides/11-knn.html#choosing-k",
    "href": "slides/11-knn.html#choosing-k",
    "title": "K-nearest Neighbors \n",
    "section": "Choosing K",
    "text": "Choosing K\n\nCodeset.seed(2025)\nlibrary(caret)\ncontrol &lt;- trainControl(method = \"cv\", number = 10)\nknn_cvfit &lt;- train(y ~ ., method = \"knn\", \n                   data = data.frame(\"x\" = x, \"y\" = as.factor(y)),\n                   tuneGrid = data.frame(k = seq(1, 40, 1)),\n                   trControl = control)\npar(mar = c(4, 4, 0, 0))\nplot(knn_cvfit$results$k, 1 - knn_cvfit$results$Accuracy,\n     xlab = \"K\", ylab = \"Classification Error\", type = \"b\",\n     pch = 19, col = 2)"
  },
  {
    "objectID": "slides/11-knn.html#best-k",
    "href": "slides/11-knn.html#best-k",
    "title": "K-nearest Neighbors \n",
    "section": "Best K",
    "text": "Best K\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix and Statistics\n\n       y\nknn_fit  0  1\n      0 82 14\n      1 18 86\n                                          \n               Accuracy : 0.84            \n                 95% CI : (0.7817, 0.8879)\n    No Information Rate : 0.5             \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.68            \n                                          \n Mcnemar's Test P-Value : 0.5959          \n                                          \n            Sensitivity : 0.8200          \n            Specificity : 0.8600          \n         Pos Pred Value : 0.8542          \n         Neg Pred Value : 0.8269          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4100          \n   Detection Prevalence : 0.4800          \n      Balanced Accuracy : 0.8400          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "slides/11-knn.html#scaling-and-distance-measures",
    "href": "slides/11-knn.html#scaling-and-distance-measures",
    "title": "K-nearest Neighbors \n",
    "section": "Scaling and Distance Measures",
    "text": "Scaling and Distance Measures\n\nBy default, we use Euclidean distance (\\(\\ell_2\\) norm) \\(d^2(\\mathbf{u}, \\mathbf{v}) = \\lVert \\mathbf{u}- \\mathbf{v}\\rVert_2^2 = \\sum_{j=1}^p (u_j - v_j)^2\\)\nThis measure is not scale invariant: Multiplying the data with a factor changes the distance!\n\n\n\nOften consider a normalized version:\n\n\\[d^2(\\mathbf{u}, \\mathbf{v}) = \\sum_{j=1}^p \\frac{(u_j - v_j)^2}{\\sigma_j^2}\\]\n\n\n\n\nMahalanobis distance takes the covariance structure into account\n\n\\[d^2(\\mathbf{u}, \\mathbf{v}) = (\\mathbf{u}- \\mathbf{v})' \\Sigma^{-1} (\\mathbf{u}- \\mathbf{v}),\\]\n\nIf \\(\\Sigma = \\mathbf{I}\\), Mahalanobis = Euclidean\nIf \\(\\Sigma = diag(\\sigma^2_1, \\dots, \\sigma^2_p)\\), Mahalanobis = normalized version"
  },
  {
    "objectID": "slides/11-knn.html#mahalanobis-distance",
    "href": "slides/11-knn.html#mahalanobis-distance",
    "title": "K-nearest Neighbors \n",
    "section": "Mahalanobis distance",
    "text": "Mahalanobis distance\n\nRed and green points have the same Euclidean distance to the center.\nThe red point is farther away from the center in terms of Mahalanobis distance.\n\n\nIn the following plot, the red cross and orange cross have the same Euclidean distance to the center. However, the red cross is more of a âoutlierâ based on the joint distribution. The Mahalanobis distance would reflect this."
  },
  {
    "objectID": "slides/11-knn.html#example-image-data-elemstatlearnzip.train",
    "href": "slides/11-knn.html#example-image-data-elemstatlearnzip.train",
    "title": "K-nearest Neighbors \n",
    "section": "Example: Image Data ElemStatLearn::zip.train\n",
    "text": "Example: Image Data ElemStatLearn::zip.train\n\n\nDigits 0-9 scanned from envelopes by the U.S. Postal Service\n\\(16 \\times 16\\) pixel images, totally \\(p=256\\) variables\nAt each pixel, we have the gray scale as the numerical value\n1NN with Euclidean distance gives 5.6% error rate\n1NN with tangent distance (Simard et al., 1993) gives 2.6% error"
  },
  {
    "objectID": "slides/11-knn.html#example-3nn-on-image-data",
    "href": "slides/11-knn.html#example-3nn-on-image-data",
    "title": "K-nearest Neighbors \n",
    "section": "Example: 3NN on Image Data",
    "text": "Example: 3NN on Image Data\n\n# fit 3nn model and calculate the error\nknn.fit &lt;- class::knn(zip.train[, 2:257], zip.test[, 2:257], zip.train[, 1], k = 3)\n# overall prediction error\nmean(knn.fit != zip.test[, 1])\n\n[1] 0.05480817\n\n\n\n\n# the confusion matrix\ntable(knn.fit, zip.test[, 1])\n\n       \nknn.fit   0   1   2   3   4   5   6   7   8   9\n      0 355   0   8   1   0   2   3   0   4   1\n      1   0 258   0   0   2   0   0   1   0   0\n      2   2   0 182   2   0   2   1   1   0   0\n      3   0   0   1 153   0   3   0   1   4   0\n      4   0   3   1   0 182   0   2   4   0   3\n      5   0   0   0   8   2 147   1   0   2   1\n      6   0   2   1   0   2   1 163   0   1   0\n      7   1   1   2   1   2   0   0 138   1   4\n      8   0   0   3   0   1   1   0   1 151   0\n      9   1   0   0   1   9   4   0   1   3 168"
  },
  {
    "objectID": "slides/11-knn.html#example-3nn-on-image-data-1",
    "href": "slides/11-knn.html#example-3nn-on-image-data-1",
    "title": "K-nearest Neighbors \n",
    "section": "Example: 3NN on Image Data",
    "text": "Example: 3NN on Image Data"
  },
  {
    "objectID": "slides/11-knn.html#computational-issues",
    "href": "slides/11-knn.html#computational-issues",
    "title": "K-nearest Neighbors \n",
    "section": "Computational Issues",
    "text": "Computational Issues\n\nNeed to store the entire training data for prediction. (Lazy learner)\nNeeds to calculate the distance from \\(x_0\\) to all training sample and sort them.1\nDistance measures may affect accuracy.\n\n\\(K\\)NN can be quite computationally intense for large sample size because to find the nearest neighbors, we need to calculate and compare the distances to each of the data point. In addition, it is not memory friendly because we need to store the entire training dataset for future prediction. In contrast, for linear model, we only need to store the estimated \\(\\boldsymbol \\beta\\) parameters. Some algorithms have been developed to search for the neighbors more efficiently. You can try the FNN package for these faster computations when \\(n\\) is large.\nR FNN package for faster computations when \\(n\\) is large."
  },
  {
    "objectID": "slides/11-knn.html#curse-of-dimensionality",
    "href": "slides/11-knn.html#curse-of-dimensionality",
    "title": "K-nearest Neighbors \n",
    "section": "Curse of Dimensionality",
    "text": "Curse of Dimensionality\n\nKNN does does not work well in high-dimensional space (\\(p \\gg n\\)) due to curse of dimensionality.\n\n\nAs \\(p\\) increases, itâs getting harder to find \\(k\\) neighbors in the input space. KNN needs to explore a large range of values along each input dimension to grab the âneighborsâ.\n\n\nThe âneighborsâ of \\(x_0\\) are in fact far away from \\(x_0\\), and so they may not be good predictors about the behavior of the function at \\(x_0\\).\nThe method is not local anymore despite the name ânearest neighborâ!\nIn high dimensions KNN often performs worse than linear regression.\n\nIn high dimensions, all points tend to become equidistant from each other. This means that the difference between the nearest and farthest neighbor becomes very small, making it difficult for KNN to distinguish between close and distant points."
  },
  {
    "objectID": "slides/11-knn.html#curse-of-dimensionality-1",
    "href": "slides/11-knn.html#curse-of-dimensionality-1",
    "title": "K-nearest Neighbors \n",
    "section": "Curse of Dimensionality",
    "text": "Curse of Dimensionality\n\nData points are uniformly spread out on \\([0, 1]^p\\).\nIn 10 dimensions we need to cover 80% of the range of each coordinate to capture 10% of the data.\n\n\n\n\n\nSource: ESL Fig. 2.6\n\n\n\n\nThe problem comes from the fact that the volume of space grows exponentially fast with dimension, so we may have to look quite far away in space to find our nearest neighbor.\n\n\n\n\nmssc6250-s25.github.io/website"
  },
  {
    "objectID": "slides/06-lasso-vs.html#when-ols-doesnt-work-well",
    "href": "slides/06-lasso-vs.html#when-ols-doesnt-work-well",
    "title": "Feature Selection and LASSO \n",
    "section": "When OLS Doesnât Work Well",
    "text": "When OLS Doesnât Work Well\nWhen \\(p \\gg n\\), it is often that many of the features in the model are not associated with the response.\n\nModel Interpretability: By removing irrelevant features \\(X_j\\)s, i.e., setting the corresponding \\(\\beta_j\\)s to zero, we can obtain a model that is more easily interpreted. (Feature/Variable selection)\nLeast squares is unlikely to yield any coefficient estimates that are exactly zero."
  },
  {
    "objectID": "slides/06-lasso-vs.html#variable-selection",
    "href": "slides/06-lasso-vs.html#variable-selection",
    "title": "Feature Selection and LASSO \n",
    "section": "Variable Selection",
    "text": "Variable Selection\n\nWe have a large pool of candidate regressors, of which only a few are likely to be important.\n\nTwo âconflictingâ goals in model building:\n\nas many features as possible for better predictive performance on new data (smaller bias).\n\nas few regressors as possible because as the number of regressors increases,\n\n\n\\(\\mathrm{Var}(\\hat{y})\\) will increase (larger variance)\ncost more in data collecting and maintaining\nmore model complexity\n\n\n\nA compromise between the two hopefully leads to the âbestâ regression equation.\n\nWhat does best mean?\n\n\nThere is no unique definition of âbestâ, and different methods specify different subsets of the candidate regressors as best."
  },
  {
    "objectID": "slides/06-lasso-vs.html#three-classes-of-methods",
    "href": "slides/06-lasso-vs.html#three-classes-of-methods",
    "title": "Feature Selection and LASSO \n",
    "section": "Three Classes of Methods",
    "text": "Three Classes of Methods\n\n\nSubset Selection (ISL Sec. 6.1) Identify a subset of the \\(p\\) predictors that we believe to be related to the response.\n\nNeed a selection method and a selection criterion.\nCovered in MSSC 5780 slides\n\n\n\n\n\n\n\nShrinkage (ISL Sec. 6.2) Fit a model that forces some coefficients to be shrunk to zero.\n\n\nLasso (Discussed in MSSC 6250)  \n\n\n\n\n\n\n\n\nDimension Reduction (ISL Sec. 6.3) Find \\(m\\) representative features that are linear combinations of the \\(p\\) original predictors (\\(m \\ll p\\)), then fit least squares.\n\nPrincipal component regression (Unsupervised)  \nPartial least squares (Supervised)"
  },
  {
    "objectID": "slides/06-lasso-vs.html#why-lasso",
    "href": "slides/06-lasso-vs.html#why-lasso",
    "title": "Feature Selection and LASSO \n",
    "section": "Why Lasso?",
    "text": "Why Lasso?\n\n\nSubset selection methods\n\nmay be computationally infeasible (fit OLS over millions of times)\ndo not explore all possible subset models (no global solution)\n\n\n\n\nRidge regression does shrink coefficients, but still include all predictors.\nLasso regularizes coefficients so that some coefficients are shrunk to zero, doing feature selection.\nLike ridge regression, for a given \\(\\lambda\\), Lasso only fits a single model.\n\nRidge OK for prediction, but bad at interpretation https://stats.stackexchange.com/questions/76518/what-is-the-time-complexity-of-lasso-regression same computation complexity as OLS \\(O(p^2n)\\) when \\(p &lt; n\\) and \\(O(p^3)\\)"
  },
  {
    "objectID": "slides/06-lasso-vs.html#what-is-lasso",
    "href": "slides/06-lasso-vs.html#what-is-lasso",
    "title": "Feature Selection and LASSO \n",
    "section": "What is Lasso?",
    "text": "What is Lasso?\nDifferent from the Ridge regression that adds \\(\\ell_2\\) norm, Lasso adds \\(\\ell_1\\) penalty on the parameters:\n\\[\\begin{align}\n\\widehat{\\boldsymbol \\beta}^\\text{l} =& \\, \\, \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} \\lVert \\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta\\rVert_2^2 + n \\lambda \\lVert\\boldsymbol \\beta\\rVert_1\\\\\n=& \\, \\, \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i' \\boldsymbol \\beta)^2 + \\lambda \\sum_{j=1}^p |\\beta_j|,\n\\end{align}\\]\n\nThe \\(\\ell_1\\) penalty forces some of the coefficient estimates to be exactly equal to zero when \\(\\lambda\\) is sufficiently large, yielding sparse models."
  },
  {
    "objectID": "slides/06-lasso-vs.html#ell_2-vs.-ell_1",
    "href": "slides/06-lasso-vs.html#ell_2-vs.-ell_1",
    "title": "Feature Selection and LASSO \n",
    "section": "\n\\(\\ell_2\\) vs.Â \\(\\ell_1\\)\n",
    "text": "\\(\\ell_2\\) vs.Â \\(\\ell_1\\)\n\n\nRidge shrinks big coefficients much more than lasso.\nLasso has larger penalty on small coefficients.\n\n\n\nRidge shrinks big coefficients much more than lasso.\nLasso has larger penalty on small coefficients. This is one of the intuitions why Lasso has exact zero coefficients after shrinkage. Because when punishing small coefficients a lot more, it may be better to set them exactly equal to zero to get smaller loss or objective value."
  },
  {
    "objectID": "slides/06-lasso-vs.html#elemstatlearnprostate-data",
    "href": "slides/06-lasso-vs.html#elemstatlearnprostate-data",
    "title": "Feature Selection and LASSO \n",
    "section": "\nElemStatLearn::prostate Data",
    "text": "ElemStatLearn::prostate Data\n\n\n\n\n\n\n\n\nlcavol\nlweight\nage\nlbph\nsvi\nlcp\ngleason\npgg45\nlpsa\ntrain\n\n\n\n-0.580\n2.77\n50\n-1.386\n0\n-1.39\n6\n0\n-0.431\nTRUE\n\n\n-0.994\n3.32\n58\n-1.386\n0\n-1.39\n6\n0\n-0.163\nTRUE\n\n\n-0.511\n2.69\n74\n-1.386\n0\n-1.39\n7\n20\n-0.163\nTRUE\n\n\n-1.204\n3.28\n58\n-1.386\n0\n-1.39\n6\n0\n-0.163\nTRUE\n\n\n0.751\n3.43\n62\n-1.386\n0\n-1.39\n6\n0\n0.372\nTRUE\n\n\n-1.050\n3.23\n50\n-1.386\n0\n-1.39\n6\n0\n0.765\nTRUE\n\n\n0.737\n3.47\n64\n0.615\n0\n-1.39\n6\n0\n0.765\nFALSE\n\n\n0.693\n3.54\n58\n1.537\n0\n-1.39\n6\n0\n0.854\nTRUE"
  },
  {
    "objectID": "slides/06-lasso-vs.html#cv.glmnetalpha-1",
    "href": "slides/06-lasso-vs.html#cv.glmnetalpha-1",
    "title": "Feature Selection and LASSO \n",
    "section": "cv.glmnet(alpha = 1)",
    "text": "cv.glmnet(alpha = 1)\n\nlasso_fit &lt;- cv.glmnet(x = data.matrix(prostate[, 1:8]), y = prostate$lpsa, nfolds = 10, \n                       alpha = 1)\n\n\nCodeplot(lasso_fit)\nplot(lasso_fit$glmnet.fit, \"lambda\")\n\n\ngenridge Package"
  },
  {
    "objectID": "slides/06-lasso-vs.html#lasso-coefficients",
    "href": "slides/06-lasso-vs.html#lasso-coefficients",
    "title": "Feature Selection and LASSO \n",
    "section": "Lasso Coefficients",
    "text": "Lasso Coefficients\n\nlambda.min contains more nonzero coefficients.\nLarger penalty \\(\\lambda\\) forces more coefficients to be zero, and the model is more âsparseâ.\n\n\n\n\ncoef(lasso_fit, s = \"lambda.min\")\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                  s1\n(Intercept)  0.17999\nlcavol       0.56099\nlweight      0.61908\nage         -0.02069\nlbph         0.09531\nsvi          0.75180\nlcp         -0.09912\ngleason      0.04745\npgg45        0.00432\n\n\n\n\ncoef(lasso_fit, s = \"lambda.1se\")\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n               s1\n(Intercept) 0.644\nlcavol      0.455\nlweight     0.314\nage         .    \nlbph        .    \nsvi         0.367\nlcp         .    \ngleason     .    \npgg45       ."
  },
  {
    "objectID": "slides/06-lasso-vs.html#one-variable-lasso-and-shrinkage-concept",
    "href": "slides/06-lasso-vs.html#one-variable-lasso-and-shrinkage-concept",
    "title": "Feature Selection and LASSO \n",
    "section": "One-Variable Lasso and Shrinkage: Concept",
    "text": "One-Variable Lasso and Shrinkage: Concept\n\nLasso solution does not have an analytic or closed form in general.\nThe univariate regression model with no intercept\n\n\\[\\underset{\\beta}{\\mathop{\\mathrm{arg\\,min}}}  \\quad \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i \\beta)^2 + \\lambda |\\beta|\\]\nWith some derivation, and also utilize the OLS solution of the loss function, we have\n\\[\\begin{align}\n&\\frac{1}{n} \\sum_{i=1}^n (y_i - x_i \\beta)^2 \\\\\n=& \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i b + x_i b - x_i \\beta)^2 \\\\\n=& \\frac{1}{n} \\sum_{i=1}^n \\Big[ \\underbrace{(y_i - x_i b)^2}_{\\text{I}} + \\underbrace{2(y_i - x_i b)(x_i b - x_i \\beta)}_{\\text{II}} + \\underbrace{(x_i b - x_i \\beta)^2}_{\\text{III}} \\Big]\n\\end{align}\\]"
  },
  {
    "objectID": "slides/06-lasso-vs.html#one-variable-lasso-and-shrinkage-concept-1",
    "href": "slides/06-lasso-vs.html#one-variable-lasso-and-shrinkage-concept-1",
    "title": "Feature Selection and LASSO \n",
    "section": "One-Variable Lasso and Shrinkage: Concept",
    "text": "One-Variable Lasso and Shrinkage: Concept\n\\[\\begin{align}\n& \\sum_{i=1}^n 2(y_i - x_i b)(x_i b - x_i \\beta)\n= (b - \\beta) {\\color{OrangeRed}{\\sum_{i=1}^n 2(y_i - x_i b)x_i}}\n= (b - \\beta) {\\color{OrangeRed}{0}} = 0\n\\end{align}\\]\nOur original problem reduces to just the third term and the penalty\n\\[\\begin{align}\n&\\underset{\\beta}{\\mathop{\\mathrm{arg\\,min}}}  \\quad \\frac{1}{n} \\sum_{i=1}^n (x_ib - x_i \\beta)^2 + \\lambda |\\beta| \\\\\n=&\\underset{\\beta}{\\mathop{\\mathrm{arg\\,min}}}  \\quad \\frac{1}{n} \\left[ \\sum_{i=1}^n x_i^2 \\right] (b - \\beta)^2 + \\lambda |\\beta|\n\\end{align}\n\\] Without loss of generality, assume that \\(x\\) is standardized with mean 0 and variance \\(\\frac{1}{n}\\sum_{i=1}^n x_i^2 = 1\\).\nb is arbitrary We learned that to solve for an optimizer, we can set the gradient to be zero. However, the function is not everywhere differentiable"
  },
  {
    "objectID": "slides/06-lasso-vs.html#one-variable-lasso-and-shrinkage-concept-2",
    "href": "slides/06-lasso-vs.html#one-variable-lasso-and-shrinkage-concept-2",
    "title": "Feature Selection and LASSO \n",
    "section": "One-Variable Lasso and Shrinkage: Concept",
    "text": "One-Variable Lasso and Shrinkage: Concept\nThis leads to a general problem of\n\\[\\underset{\\beta}{\\mathop{\\mathrm{arg\\,min}}}  \\quad (\\beta - b)^2 + \\lambda |\\beta|,\\] For \\(\\beta &gt; 0\\),\n\\[\\begin{align}\n0 =& \\frac{\\partial}{\\partial \\beta} \\,\\, \\left[(\\beta - b)^2 + \\lambda |\\beta| \\right] = 2 (\\beta - b) + \\lambda \\\\\n\\Longrightarrow \\quad \\beta =&\\, b - \\lambda/2\n\\end{align}\\]\n\\[\\begin{align}\n\\widehat{\\beta}^l &=\n        \\begin{cases}\n        b - \\lambda/2 & \\text{if} \\quad b &gt; \\lambda/2 \\\\\n        0 & \\text{if} \\quad |b| \\le \\lambda/2 \\\\\n        b + \\lambda/2 & \\text{if} \\quad b &lt; -\\lambda/2 \\\\\n        \\end{cases}\n\\end{align}\\]\n\nLasso provides a soft-thresholding solution.\nWhen \\(\\lambda\\) is large enough, \\(\\widehat{\\beta}^l\\) will be shrunk to zero.\n\nb is arbitrary We learned that to solve for an optimizer, we can set the gradient to be zero. However, the function is not everywhere differentiable"
  },
  {
    "objectID": "slides/06-lasso-vs.html#objective-function",
    "href": "slides/06-lasso-vs.html#objective-function",
    "title": "Feature Selection and LASSO \n",
    "section": "Objective function",
    "text": "Objective function\nThe objective function is \\((\\beta - 1)^2 + \\lambda |\\beta|\\). Once the penalty is larger than 2, the optimizer would stay at 0.\n\n\n\n\n\n\nBased on our analysis, once the penalty is larger than 2, the optimizer would stay at 0."
  },
  {
    "objectID": "slides/06-lasso-vs.html#variable-selection-property-and-shrinkage",
    "href": "slides/06-lasso-vs.html#variable-selection-property-and-shrinkage",
    "title": "Feature Selection and LASSO \n",
    "section": "Variable Selection Property and Shrinkage",
    "text": "Variable Selection Property and Shrinkage\n\nThe proportion of times a variable has a zero parameter estimation.\n\\(\\mathbf{y}= \\mathbf{X}\\boldsymbol \\beta + \\epsilon = \\sum_{j = 1}^{20} X_j \\times 0.4^{\\sqrt{j}} + N(0, 1).\\)\n\\(\\beta_1 = 0.4, \\beta_2 = 0.4^{\\sqrt{2}}, \\dots, \\beta_{20} = 0.4^{\\sqrt{20}}\\)\n\n\n\nSince Lasso shrinks some parameter to exactly zero, it has the variable selection property â the ones that are nonzero are the ones being selected. This is a very nice properly in high-dimensional data analysis, where we cannot estimate the effects of all variables.\nWe can then look at the proportion of times a variable has a non-zero parameter estimation"
  },
  {
    "objectID": "slides/06-lasso-vs.html#bias-variance-trade-off",
    "href": "slides/06-lasso-vs.html#bias-variance-trade-off",
    "title": "Feature Selection and LASSO \n",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off"
  },
  {
    "objectID": "slides/06-lasso-vs.html#constrained-optimization",
    "href": "slides/06-lasso-vs.html#constrained-optimization",
    "title": "Feature Selection and LASSO \n",
    "section": "Constrained Optimization",
    "text": "Constrained Optimization\n\n\nLasso \\[\\begin{align}\n\\min_{\\boldsymbol \\beta} \\,\\,& \\lVert \\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta\\rVert_2^2 + n\\lambda\\lVert\\boldsymbol \\beta\\rVert_1\n\\end{align}\\]\n\\[\\begin{align}\n\\min_{\\boldsymbol \\beta} \\,\\,& \\lVert \\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta\\rVert_2^2\\\\\n\\text{s.t.} \\,\\, & \\sum_{j=1}^p|\\beta_j| \\leq s\n\\end{align}\\]\n\nRidge \\[\\begin{align}\n\\min_{\\boldsymbol \\beta} \\,\\,& \\lVert \\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta\\rVert_2^2 + n\\lambda\\lVert\\boldsymbol \\beta\\rVert_2^2\n\\end{align}\\]\n\\[\\begin{align}\n\\min_{\\boldsymbol \\beta} \\,\\,& \\lVert \\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta\\rVert_2^2\\\\\n\\text{s.t.} \\,\\, & \\sum_{j=1}^p \\beta_j^2 \\leq s\n\\end{align}\\]\n\n\n\nFor every value of \\(\\lambda\\), there is some \\(s\\) such that the two optimization problems are equivalent, giving the same coefficient estimates.\nThe \\(\\ell_1\\) and \\(\\ell_2\\) penalties form a constraint region that \\(\\beta_j\\) can move around or budget for how large \\(\\beta_j\\) can be.\nLarger \\(s\\) (smaller \\(\\lambda\\)) means a larger region \\(\\beta_j\\) can freely move."
  },
  {
    "objectID": "slides/06-lasso-vs.html#geometric-representation-of-optimization",
    "href": "slides/06-lasso-vs.html#geometric-representation-of-optimization",
    "title": "Feature Selection and LASSO \n",
    "section": "Geometric Representation of Optimization",
    "text": "Geometric Representation of Optimization\n\nWhat do the constraints look like geometrically?\n\n\nWhen \\(p = 2\\),\n\nthe \\(\\ell_1\\) constraint is \\(|\\beta_1| + |\\beta_2| \\leq s\\) (diamond)\nthe \\(\\ell_2\\) constraint is \\(\\beta_1^2 + \\beta_2^2 \\leq s\\) (circle)\n\n\n\n\n\n\n\n\nSource: https://stats.stackexchange.com/questions/350046/the-graphical-intuiton-of-the-lasso-in-case-p-2\n\n\n\n\nthe solution has to stay within the shaded area. The objective function is shown with the contour, and once the contained area is sufficiently small, some Î² parameter will be shrunk to exactly zero. On the other hand, the Ridge regression also has a similar interpretation. However, since the constrained areas is a circle, it will never for the estimated parameters to be zero."
  },
  {
    "objectID": "slides/06-lasso-vs.html#way-of-shrinking-p-1-and-standardized-x",
    "href": "slides/06-lasso-vs.html#way-of-shrinking-p-1-and-standardized-x",
    "title": "Feature Selection and LASSO \n",
    "section": "Way of Shrinking (\\(p = 1\\) and standardized \\(x\\))",
    "text": "Way of Shrinking (\\(p = 1\\) and standardized \\(x\\))\n\n\nLasso Soft-thresholding\n\n\\[\\begin{align}\n\\widehat{\\beta}^l &=\n        \\begin{cases}\n        b - \\lambda/2 & \\text{if} \\quad b &gt; \\lambda/2 \\\\\n        0 & \\text{if} \\quad |b| &lt; \\lambda/2 \\\\\n        b + \\lambda/2 & \\text{if} \\quad b &lt; -\\lambda/2 \\\\\n        \\end{cases}\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\nRidge Proportional shrinkage\n\\[\\begin{align}\n\\widehat{\\beta}^r = \\dfrac{b}{1+\\lambda}\\end{align}\\]"
  },
  {
    "objectID": "slides/06-lasso-vs.html#predictive-performance",
    "href": "slides/06-lasso-vs.html#predictive-performance",
    "title": "Feature Selection and LASSO \n",
    "section": "Predictive Performance",
    "text": "Predictive Performance\nPerform well (lower test MSE) when\n\n\nLasso (â)\n\nA relatively small number of \\(\\beta_j\\)s are substantially large, and the remaining \\(\\beta_k\\)s are small or equal to zero.\nReduce more bias\n\n\n\n\n\nISL Fig. 6.9\n\n\n\n\nRidge (\\(\\cdots\\cdots\\))\n\nThe response is a function of many predictors, all with coefficients of roughly equal size.\nReduce more variance\n\n\n\n\n\nISL Fig. 6.8\n\n\n\n\n\nRidge focus more on variance stablization Lasso focus more on model selection and interpretbility higher R^2 (larger lambda), more overfitting, so low bias high variance"
  },
  {
    "objectID": "slides/06-lasso-vs.html#bayesian-interpretation",
    "href": "slides/06-lasso-vs.html#bayesian-interpretation",
    "title": "Feature Selection and LASSO \n",
    "section": "Bayesian Interpretation",
    "text": "Bayesian Interpretation\n\n\nLasso\n\nRidge\n\n\n\n\nLetâs wait until we discuss Bayesian Regression!"
  },
  {
    "objectID": "slides/06-lasso-vs.html#notes-of-lasso",
    "href": "slides/06-lasso-vs.html#notes-of-lasso",
    "title": "Feature Selection and LASSO \n",
    "section": "Notes of Lasso",
    "text": "Notes of Lasso\n\n\n\n\n\n\nWarning\n\n\n\n\nEven Lasso does feature selection, do NOT add predictors that are known to be not associated with the response in any way.\nCurse of dimensionality. The test MSE tends to increase as the dimensionality \\(p\\) increases, unless the additional features are truly associated with the response.\nDo NOT conclude that the predictors with non-zero coefficients selected by Lasso and other selection methods predict the response more effectively than other predictors not included in the model.\n\n\n\n\n\nThis is just one of many possible models for predicting response, and that it must be further validated on independent data sets."
  },
  {
    "objectID": "slides/06-lasso-vs.html#other-topics",
    "href": "slides/06-lasso-vs.html#other-topics",
    "title": "Feature Selection and LASSO \n",
    "section": "Other Topics",
    "text": "Other Topics\n\n\nCombination \\(\\ell_1\\) and \\(\\ell_2\\): Elastic net penalty (Zou and Hastie, 2005)\n\n\n\\[\\lambda \\left[ (1 - \\alpha) \\lVert \\boldsymbol \\beta\\rVert_2^2 + \\alpha \\lVert\\boldsymbol \\beta\\lVert_1 \\right]\\]\n\n\n\nGeneral \\(\\ell_q\\) penalty: \\(\\lambda \\sum_{j=1}^p |\\beta_j|^q\\)\n\n\n\n\n\n\nESL Fig. 3.12"
  },
  {
    "objectID": "slides/06-lasso-vs.html#other-topics-1",
    "href": "slides/06-lasso-vs.html#other-topics-1",
    "title": "Feature Selection and LASSO \n",
    "section": "Other Topics",
    "text": "Other Topics\n\n\nAlgorithms\n\nShooting algorithm (Fu 1998)\nLeast angle regression (LAR) (Efron et al.Â 2004)\nCoordinate descent (Friedman et al 2010) (used in glmnet)\n\n\n\n\n\n\nVariants\n\nAdaptive Lasso\nGroup Lasso\nBayesian Lasso\nSpike-and-Slab Lasso, etc.\n\n\n\nLasso may suffer in the case where two variables are strongly correlated. The situation is similar to OLS, however, in Lasso, it would only select one out of the two, instead of letting both parameter estimates to be large. This is not preferred in some practical situations such as genetic studies because expressions of genes from the same pathway may have large correlation, but biologist want to identify all of them instead of just one. The Ridge penalty may help in this case because it naturally considers the correlation structure. The following simulation may show the effect. - The Lasso problem is convex, although it may not be strictly convex in Î² when p is large - The solution is a global minimum, but may not be the unique global one - The Lasso solution is unique under conditions of the covariance matrix\n\n\n\n\nmssc6250-s25.github.io/website"
  },
  {
    "objectID": "course-news.html",
    "href": "course-news.html",
    "title": "News/Announcements",
    "section": "",
    "text": "Any announcement will be posted in this page. The latest news will also be put on top of the main page.",
    "crumbs": [
      "Course Information",
      "News/Annoucements"
    ]
  },
  {
    "objectID": "course-news.html#jan-14-2025",
    "href": "course-news.html#jan-14-2025",
    "title": "News/Announcements",
    "section": "Jan 14, 2025",
    "text": "Jan 14, 2025\n\nNo office hours first week.",
    "crumbs": [
      "Course Information",
      "News/Annoucements"
    ]
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "MSSC 6250 - Statistical Machine Learning (Spring 2025)",
    "section": "",
    "text": "The course discuss machine learning from statistical and modeling points of view, covering supervised learning and unsupervised learning models and algorithms. Supervised learning methods include various regression and classification methods, and unsupervised learning methods involves dimension reduction and clustering techniques. Topics include Bayesian linear regression, shrinkage and regularization, regression splines, Gaussian processes, logistic regression, discriminant analysis, nearest neighbors, tree-based methods, principal components, K-means, Gaussian mixture clustering, neural networks, etc.",
    "crumbs": [
      "Course Information",
      "Overview"
    ]
  },
  {
    "objectID": "code/13-svm-code.html",
    "href": "code/13-svm-code.html",
    "title": "13 - Support Vector Machine Code Demo",
    "section": "",
    "text": "Codelibrary(e1071)\nlibrary(kernlab)\n\n\n\nCodeset.seed(2025)\nn &lt;- 6\np &lt;- 2\nxneg &lt;- matrix(rnorm(n * p), n, p)\nxpos &lt;- matrix(rnorm(n * p, mean = 3), n, p)\nx &lt;- rbind(xpos, xneg)\ny &lt;- matrix(as.factor(c(rep(1, n), rep(-1, n))))\n\n\n\nCodesvm_fit &lt;- e1071::svm(y ~ ., data = data.frame(x, y), type = 'C-classification', \n                      kernel = 'linear', scale = FALSE, cost = 10000)\n\n\n\nCodesvm_fit2 &lt;- kernlab::ksvm(x, y, type = \"C-svc\", kernel = 'vanilladot', C = 10000)\n\n Setting default kernel parameters  \n\n\n\n\nCodeload(\"../data/ESL.mixture.rda\", verbose = TRUE)\n\nLoading objects:\n  ESL.mixture\n\nCodex &lt;- ESL.mixture$x\ny &lt;- ESL.mixture$y\ndat &lt;- data.frame(y = factor(y), x)\nfit &lt;- svm(y ~ ., data = dat, scale = FALSE, kernel = \"radial\", cost = 5)\n\n\n\nCodepx1 &lt;- ESL.mixture$px1\npx2 &lt;- ESL.mixture$px2\nxgrid &lt;- expand.grid(X1 = px1, X2 = px2)\nfunc &lt;- predict(fit, xgrid, decision.values = TRUE)\nfunc &lt;- attributes(func)$decision\n\n\n\nCodeygrid &lt;- predict(fit, xgrid)\nplot(xgrid, col = ifelse(ygrid == 1, 2, 4), \n     pch = 20, cex = 0.3, main=\"SVM with RBF kernal\")\npoints(x, col = ifelse(y == 1, 2, 4), pch = 19)\ncontour(px1, px2, matrix(func, 69, 99), level = 0, add = TRUE, lwd = 5)"
  },
  {
    "objectID": "code/13-svm-code.html#r-implementation",
    "href": "code/13-svm-code.html#r-implementation",
    "title": "13 - Support Vector Machine Code Demo",
    "section": "",
    "text": "Codelibrary(e1071)\nlibrary(kernlab)\n\n\n\nCodeset.seed(2025)\nn &lt;- 6\np &lt;- 2\nxneg &lt;- matrix(rnorm(n * p), n, p)\nxpos &lt;- matrix(rnorm(n * p, mean = 3), n, p)\nx &lt;- rbind(xpos, xneg)\ny &lt;- matrix(as.factor(c(rep(1, n), rep(-1, n))))\n\n\n\nCodesvm_fit &lt;- e1071::svm(y ~ ., data = data.frame(x, y), type = 'C-classification', \n                      kernel = 'linear', scale = FALSE, cost = 10000)\n\n\n\nCodesvm_fit2 &lt;- kernlab::ksvm(x, y, type = \"C-svc\", kernel = 'vanilladot', C = 10000)\n\n Setting default kernel parameters  \n\n\n\n\nCodeload(\"../data/ESL.mixture.rda\", verbose = TRUE)\n\nLoading objects:\n  ESL.mixture\n\nCodex &lt;- ESL.mixture$x\ny &lt;- ESL.mixture$y\ndat &lt;- data.frame(y = factor(y), x)\nfit &lt;- svm(y ~ ., data = dat, scale = FALSE, kernel = \"radial\", cost = 5)\n\n\n\nCodepx1 &lt;- ESL.mixture$px1\npx2 &lt;- ESL.mixture$px2\nxgrid &lt;- expand.grid(X1 = px1, X2 = px2)\nfunc &lt;- predict(fit, xgrid, decision.values = TRUE)\nfunc &lt;- attributes(func)$decision\n\n\n\nCodeygrid &lt;- predict(fit, xgrid)\nplot(xgrid, col = ifelse(ygrid == 1, 2, 4), \n     pch = 20, cex = 0.3, main=\"SVM with RBF kernal\")\npoints(x, col = ifelse(y == 1, 2, 4), pch = 19)\ncontour(px1, px2, matrix(func, 69, 99), level = 0, add = TRUE, lwd = 5)"
  },
  {
    "objectID": "code/13-svm-code.html#python-implementation",
    "href": "code/13-svm-code.html#python-implementation",
    "title": "13 - Support Vector Machine Code Demo",
    "section": "\n2 Python implementation",
    "text": "2 Python implementation\n\nCodeimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\n\n\n\n2.1 Linear kernel\n\nCodenp.random.seed(1)\nn = 6\np = 2\nxneg = np.random.normal(size=(n, p))\nxpos = np.random.normal(loc=3, size=(n, p))\nx = np.vstack((xpos, xneg))\ny = np.array([1] * n + [-1] * n)\n\n\n\nCodesvm_fit = SVC(kernel=\"linear\", C=10000, probability=True)\nsvm_fit.fit(x, y)\n\n\n\n\nSVC(C=10000, kernel='linear', probability=True)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nÂ Â SVC?Documentation for SVCiFittedSVC(C=10000, kernel='linear', probability=True) \n\n\n\n\n2.2 Radial basis (Gaussian) kernel\n\nCodeimport rdata\nmixture_example = rdata.read_rda('../data/ESL.mixture.rda')\n\n/Users/chenghanyu/.virtualenvs/r-reticulate/lib/python3.12/site-packages/rdata/conversion/_conversion.py:856: UserWarning: Missing constructor for R class \"matrix\". The underlying R object is returned instead.\n  warnings.warn(\n\nCodex = mixture_example['ESL.mixture']['x']\ny = mixture_example['ESL.mixture']['y']\n\n\n\nCodesvm_rbf = SVC(kernel = 'rbf', C = 5, probability = True)\nsvm_rbf.fit(x, y)\n\n\n\n\nSVC(C=5, probability=True)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nÂ Â SVC?Documentation for SVCiFittedSVC(C=5, probability=True) \n\n\n\n\nCodepx1 = mixture_example['ESL.mixture']['px1']\npx2 = mixture_example['ESL.mixture']['px2']\nppx1, ppx2 = np.meshgrid(px1, px2)\nx_grid = np.c_[ppx1.ravel(), ppx2.ravel()]\ndecision_function = svm_rbf.decision_function(x_grid).reshape(ppx1.shape)\nprobability = svm_rbf.predict_proba(x_grid)[:, 1].reshape(ppx1.shape)\n\n\n\nCodeplt.figure()\nplt.contour(px1, px2, decision_function, levels=[0], colors=\"black\", linewidths=2)\nplt.scatter(x[:, 0], x[:, 1], c=y, cmap=\"bwr\", s=50, edgecolor=\"k\")\nplt.scatter(x_grid[:, 0], x_grid[:, 1], \n            c=np.where(probability.ravel() &gt;= 0.5, \"red\", \"blue\"), s=10, alpha=0.1)\nplt.title(\"SVM with RBF Kernel\")\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.show()"
  },
  {
    "objectID": "code/06-lasso-vs-code.html",
    "href": "code/06-lasso-vs-code.html",
    "title": "06-Lasso Code Demo",
    "section": "",
    "text": "Show/Hidelibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\n\n\nShow/Hideprostate &lt;- read.csv(\"../data/prostate.csv\")\nlasso_fit &lt;- cv.glmnet(x = data.matrix(prostate[, 1:8]), \n                       y = prostate$lpsa, nfolds = 10, \n                       alpha = 1)\n\n\n\nShow/Hidelasso_fit$lambda.min\n\n[1] 0.02961435\n\nShow/Hidelasso_fit$lambda.1se\n\n[1] 0.2089234\n\nShow/Hideplot(lasso_fit)\n\n\n\n\n\n\nShow/Hideplot(lasso_fit$glmnet.fit, \"lambda\")\n\n\n\n\n\n\n\n\nShow/Hidecoef(lasso_fit, s = \"lambda.min\")\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept)  0.169279155\nlcavol       0.509085165\nlweight      0.558643448\nage         -0.010256773\nlbph         0.067418867\nsvi          0.598979114\nlcp          .          \ngleason      0.008139093\npgg45        0.002387475\n\n\n\nShow/Hidecoef(lasso_fit, s = \"lambda.1se\")\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                   s1\n(Intercept) 0.7820637\nlcavol      0.4485189\nlweight     0.2804033\nage         .        \nlbph        .        \nsvi         0.3383490\nlcp         .        \ngleason     .        \npgg45       ."
  },
  {
    "objectID": "code/06-lasso-vs-code.html#r-implementation",
    "href": "code/06-lasso-vs-code.html#r-implementation",
    "title": "06-Lasso Code Demo",
    "section": "",
    "text": "Show/Hidelibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\n\n\nShow/Hideprostate &lt;- read.csv(\"../data/prostate.csv\")\nlasso_fit &lt;- cv.glmnet(x = data.matrix(prostate[, 1:8]), \n                       y = prostate$lpsa, nfolds = 10, \n                       alpha = 1)\n\n\n\nShow/Hidelasso_fit$lambda.min\n\n[1] 0.02961435\n\nShow/Hidelasso_fit$lambda.1se\n\n[1] 0.2089234\n\nShow/Hideplot(lasso_fit)\n\n\n\n\n\n\nShow/Hideplot(lasso_fit$glmnet.fit, \"lambda\")\n\n\n\n\n\n\n\n\nShow/Hidecoef(lasso_fit, s = \"lambda.min\")\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept)  0.169279155\nlcavol       0.509085165\nlweight      0.558643448\nage         -0.010256773\nlbph         0.067418867\nsvi          0.598979114\nlcp          .          \ngleason      0.008139093\npgg45        0.002387475\n\n\n\nShow/Hidecoef(lasso_fit, s = \"lambda.1se\")\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                   s1\n(Intercept) 0.7820637\nlcavol      0.4485189\nlweight     0.2804033\nage         .        \nlbph        .        \nsvi         0.3383490\nlcp         .        \ngleason     .        \npgg45       ."
  },
  {
    "objectID": "code/06-lasso-vs-code.html#python-implementation",
    "href": "code/06-lasso-vs-code.html#python-implementation",
    "title": "06-Lasso Code Demo",
    "section": "Python implementation",
    "text": "Python implementation\n\nShow/Hideimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Lasso, LassoCV\nfrom sklearn.preprocessing import StandardScaler\n\n\n\nShow/Hideprostate = pd.read_csv(\"../data/prostate.csv\")\nX = prostate.iloc[:, 0:8]\ny = prostate[\"lpsa\"]\n\n# Standardize predictors\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Perform Lasso regression with cross-validation\n# Automatically selects alphas\nlasso_cv = LassoCV(alphas=None, cv=10, random_state=2025)  \nlasso_cv.fit(X_scaled, y)\n\n\n\n\nLassoCV(cv=10, random_state=2025)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nÂ Â LassoCV?Documentation for LassoCViFittedLassoCV(cv=10, random_state=2025) \n\n\nShow/Hide# Plot MSE across alphas\nmse_path = np.mean(lasso_cv.mse_path_, axis=1)\n\nplt.plot(np.log(lasso_cv.alphas_), mse_path, marker='o', label=\"Mean MSE\")\nplt.axvline(np.log(lasso_cv.alpha_), color=\"red\", linestyle=\"--\", \n            label=f\"Optimal Lambda: {lasso_cv.alpha_:.4f}\")\nplt.xlabel(\"Log(lambda)\")\nplt.ylabel(\"Mean Squared Error (MSE)\")\nplt.title(\"Lasso Cross-Validation: MSE vs. Log(lambda)\", fontsize=14)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nShow/Hideoptimal_lambda = lasso_cv.alpha_\noptimal_lambda\n\n0.0008434274382607599\n\nShow/Hidelasso_cv.coef_\n\narray([ 0.65931859,  0.26435421, -0.15492328,  0.13834575,  0.31131745,\n       -0.14188085,  0.03451933,  0.12292875])\n\nShow/Hidelasso_cv.intercept_\n\n2.4783868783505154\n\n\n\nShow/Hide# Reverse standardization\nstd_devs = scaler.scale_  # Feature standard deviations\nmeans = scaler.mean_      # Feature means\ncoef_original = lasso_cv.coef_ / std_devs\ncoef_original\n\narray([ 0.56230244,  0.62026266, -0.02091681,  0.09585318,  0.75589027,\n       -0.10199745,  0.04805015,  0.00438119])\n\nShow/Hideintercept_original = lasso_cv.intercept_ - np.sum(lasso_cv.coef_ * means / std_devs)\nintercept_original\n\n0.18140520823512718\n\n\n\nShow/Hidelambdas = np.logspace(-6.5, 0.1, 100, base=np.exp(1))\n\ncoefficients = []\nfor lam in lambdas:\n    lasso = Lasso(alpha=lam, fit_intercept=True, max_iter=10000)\n    lasso.fit(X_scaled, y)\n    coefficients.append(lasso.coef_)\n\n\n\n\nLasso(alpha=1.1051709180756477, max_iter=10000)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nÂ Â Lasso?Documentation for LassoiFittedLasso(alpha=1.1051709180756477, max_iter=10000) \n\n\nShow/Hidecoefficients = np.array(coefficients)\n\n# Plot coefficient paths\nfor i in range(coefficients.shape[1]):\n    plt.plot(np.log(lambdas), coefficients[:, i], label=f\"Feature {i+1}\")\n\nplt.xlabel(\"Log(lambda)\")\nplt.ylabel(\"Coefficients\")\nplt.title(\"Lasso Coefficient Paths vs. Log(lambdas)\")\nplt.legend(loc=\"upper right\", frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\nShow/Hidelasso = Lasso(alpha=0.21, fit_intercept=True)\nlasso.fit(X_scaled, y)\n\n\n\n\nLasso(alpha=0.21)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nÂ Â Lasso?Documentation for LassoiFittedLasso(alpha=0.21) \n\n\nShow/Hidelasso.coef_\n\narray([0.52543798, 0.1186707 , 0.        , 0.        , 0.13865508,\n       0.        , 0.        , 0.        ])"
  },
  {
    "objectID": "code/05-ridge-cv-code.html",
    "href": "code/05-ridge-cv-code.html",
    "title": "05-Ridge Regression Code Demo",
    "section": "",
    "text": "Codelibrary(MASS)\nfit &lt;- MASS::lm.ridge(mpg ~ ., data = mtcars, lambda = 1) ## ridge fit\ncoef(fit) ## original scale\n\n                      cyl         disp           hp         drat           wt \n16.537660830 -0.162402755  0.002333078 -0.014934856  0.924631319 -2.461146015 \n        qsec           vs           am         gear         carb \n 0.492587517  0.374651744  2.308375781  0.685715851 -0.575791252 \n\nCodefit$coef ## standardized scale\n\n       cyl       disp         hp       drat         wt       qsec         vs \n-0.2854708  0.2846046 -1.0078499  0.4865947 -2.3702010  0.8663632  0.1858566 \n        am       gear       carb \n 1.1337179  0.4979561 -0.9153711 \n\n\n\nCodeX &lt;- scale(data.matrix(mtcars[, -1]), center = TRUE, scale = TRUE)\n\n\n\nCodedf &lt;- data.frame(cbind(mtcars[, 1, drop=FALSE], X))\nridge_fit &lt;- lm.ridge(mpg ~ ., data = df, lambda = 0:40)\nMASS::select(ridge_fit)\n\nmodified HKB estimator is 2.58585 \nmodified L-W estimator is 1.837435 \nsmallest value of GCV  at 15 \n\n\n\nCodepar(mar = c(4, 4, 0, 0))\nplot(ridge_fit$lambda, ridge_fit$GCV, type = \"l\", col = \"darkgreen\", \n     ylab = \"GCV\", xlab = \"Lambda\", lwd = 3)\n\n\n\n\n\n\n\n\nBy defualt, glmnet() standardizes the x variables with standardize = TRUE, and does not standardize the response (standardize.response = FALSE).\nglmnet() always return the coefficients at the original scale.\n\n\nCodelibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\nCoderidge_cv_fit &lt;- cv.glmnet(x = X, y = mtcars$mpg, alpha = 0,\n                          nfolds = 10, type.measure = \"mse\")\nplot(ridge_cv_fit$glmnet.fit, \"lambda\")\n\n\n\n\n\n\n\n\nCodeplot(ridge_cv_fit)\n\n\n\n\n\n\n\n\nCoderidge_cv_fit$lambda.min\n\n[1] 3.014598\n\nCode# largest lambda s.t. error is within 1 s.e of the min\nridge_cv_fit$lambda.1se \n\n[1] 11.08883\n\nCodecoef(ridge_cv_fit, s = \"lambda.min\")\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept) 20.0906250\ncyl         -0.6681360\ndisp        -0.6591218\nhp          -0.7889394\ndrat         0.5644227\nwt          -1.1786358\nqsec         0.2866108\nvs           0.3966957\nam           0.7941621\ngear         0.3997316\ncarb        -0.8635288\n\n\n\n\nCodelibrary(glmnet)\nridge_cv_fit_ori &lt;- cv.glmnet(x = data.matrix(mtcars[, -1]), y = mtcars$mpg, alpha = 0,\n                          nfolds = 10, type.measure = \"mse\")\nplot(ridge_cv_fit_ori$glmnet.fit, \"lambda\")\n\n\n\n\n\n\n\n\nCodeplot(ridge_cv_fit_ori)\n\n\n\n\n\n\n\n\nCoderidge_cv_fit_ori$lambda.min\n\n[1] 3.014598\n\nCode# largest lambda s.t. error is within 1 s.e of the min\nridge_cv_fit_ori$lambda.1se \n\n[1] 12.16998\n\nCodecoef(ridge_cv_fit_ori, s = \"lambda.min\")\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept) 21.051283516\ncyl         -0.374112703\ndisp        -0.005318127\nhp          -0.011506803\ndrat         1.055629523\nwt          -1.204585685\nqsec         0.160391657\nvs           0.787069385\nam           1.591536197\ngear         0.541785546\ncarb        -0.534626533"
  },
  {
    "objectID": "code/05-ridge-cv-code.html#r-implementation",
    "href": "code/05-ridge-cv-code.html#r-implementation",
    "title": "05-Ridge Regression Code Demo",
    "section": "",
    "text": "Codelibrary(MASS)\nfit &lt;- MASS::lm.ridge(mpg ~ ., data = mtcars, lambda = 1) ## ridge fit\ncoef(fit) ## original scale\n\n                      cyl         disp           hp         drat           wt \n16.537660830 -0.162402755  0.002333078 -0.014934856  0.924631319 -2.461146015 \n        qsec           vs           am         gear         carb \n 0.492587517  0.374651744  2.308375781  0.685715851 -0.575791252 \n\nCodefit$coef ## standardized scale\n\n       cyl       disp         hp       drat         wt       qsec         vs \n-0.2854708  0.2846046 -1.0078499  0.4865947 -2.3702010  0.8663632  0.1858566 \n        am       gear       carb \n 1.1337179  0.4979561 -0.9153711 \n\n\n\nCodeX &lt;- scale(data.matrix(mtcars[, -1]), center = TRUE, scale = TRUE)\n\n\n\nCodedf &lt;- data.frame(cbind(mtcars[, 1, drop=FALSE], X))\nridge_fit &lt;- lm.ridge(mpg ~ ., data = df, lambda = 0:40)\nMASS::select(ridge_fit)\n\nmodified HKB estimator is 2.58585 \nmodified L-W estimator is 1.837435 \nsmallest value of GCV  at 15 \n\n\n\nCodepar(mar = c(4, 4, 0, 0))\nplot(ridge_fit$lambda, ridge_fit$GCV, type = \"l\", col = \"darkgreen\", \n     ylab = \"GCV\", xlab = \"Lambda\", lwd = 3)\n\n\n\n\n\n\n\n\nBy defualt, glmnet() standardizes the x variables with standardize = TRUE, and does not standardize the response (standardize.response = FALSE).\nglmnet() always return the coefficients at the original scale.\n\n\nCodelibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\nCoderidge_cv_fit &lt;- cv.glmnet(x = X, y = mtcars$mpg, alpha = 0,\n                          nfolds = 10, type.measure = \"mse\")\nplot(ridge_cv_fit$glmnet.fit, \"lambda\")\n\n\n\n\n\n\n\n\nCodeplot(ridge_cv_fit)\n\n\n\n\n\n\n\n\nCoderidge_cv_fit$lambda.min\n\n[1] 3.014598\n\nCode# largest lambda s.t. error is within 1 s.e of the min\nridge_cv_fit$lambda.1se \n\n[1] 11.08883\n\nCodecoef(ridge_cv_fit, s = \"lambda.min\")\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept) 20.0906250\ncyl         -0.6681360\ndisp        -0.6591218\nhp          -0.7889394\ndrat         0.5644227\nwt          -1.1786358\nqsec         0.2866108\nvs           0.3966957\nam           0.7941621\ngear         0.3997316\ncarb        -0.8635288\n\n\n\n\nCodelibrary(glmnet)\nridge_cv_fit_ori &lt;- cv.glmnet(x = data.matrix(mtcars[, -1]), y = mtcars$mpg, alpha = 0,\n                          nfolds = 10, type.measure = \"mse\")\nplot(ridge_cv_fit_ori$glmnet.fit, \"lambda\")\n\n\n\n\n\n\n\n\nCodeplot(ridge_cv_fit_ori)\n\n\n\n\n\n\n\n\nCoderidge_cv_fit_ori$lambda.min\n\n[1] 3.014598\n\nCode# largest lambda s.t. error is within 1 s.e of the min\nridge_cv_fit_ori$lambda.1se \n\n[1] 12.16998\n\nCodecoef(ridge_cv_fit_ori, s = \"lambda.min\")\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept) 21.051283516\ncyl         -0.374112703\ndisp        -0.005318127\nhp          -0.011506803\ndrat         1.055629523\nwt          -1.204585685\nqsec         0.160391657\nvs           0.787069385\nam           1.591536197\ngear         0.541785546\ncarb        -0.534626533"
  },
  {
    "objectID": "code/05-ridge-cv-code.html#python-implementation",
    "href": "code/05-ridge-cv-code.html#python-implementation",
    "title": "05-Ridge Regression Code Demo",
    "section": "\n2 Python implementation",
    "text": "2 Python implementation\n\nCodeimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.preprocessing import StandardScaler\n\n\n\nCodemtcars = pd.read_csv(\"../data/mtcars.csv\")\nX = mtcars.drop(columns=[\"mpg\"])\ny = mtcars[\"mpg\"]\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nridge_model = Ridge(alpha=1) ## alpha is the n*lambda\nridge_model.fit(X_scaled, y)\n\n\n\n\nRidge(alpha=1)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nRidgeRidge(alpha=1)\n\n\n\nCoderidge_model.coef_\n\narray([-0.28547077,  0.28460463, -1.00784994,  0.4865947 , -2.37020101,\n        0.86636324,  0.18585663,  1.13371791,  0.49795614, -0.91537115])\n\n\n\nCodelambdas = np.arange(1, 41) ## alpha must be &gt; 0\n# Ridge regression with cross-validation to select the best lambda\n# Enable storing CV values (can only use LOOCV)\nridge_cv = RidgeCV(alphas=lambdas, store_cv_values=True)  \nridge_cv.fit(X_scaled, y)\n\n\n\n\nRidgeCV(alphas=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40]),\n        store_cv_values=True)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nRidgeCVRidgeCV(alphas=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40]),\n        store_cv_values=True)\n\n\n\nCode# Optimal lambda and corresponding coefficients\noptimal_lambda = ridge_cv.alpha_\noptimal_lambda\n\n13\n\nCodeoptimal_coefficients = ridge_cv.coef_\noptimal_coefficients\n\narray([-0.64745384, -0.62930084, -0.79208923,  0.55274563, -1.2273356 ,\n        0.2909736 ,  0.37094879,  0.81866969,  0.3981575 , -0.89842163])\n\nCoderidge_cv.intercept_\n\n20.090625000000003\n\nCode# Cross-validation mean squared error for each lambda\ncv_mse = np.mean(ridge_cv.cv_values_, axis=0)\n\n# Plot the CV MSE vs Lambda\nplt.plot(lambdas, cv_mse, marker=\"o\", linestyle=\"-\")\nplt.axvline(optimal_lambda, color=\"red\", linestyle=\"--\", \n            label=f\"Optimal Lambda = {optimal_lambda}\")\nplt.xlabel(\"Lambda (Alpha)\")\nplt.ylabel(\"Mean Squared Error (MSE)\")\nplt.title(\"Cross-Validation MSE vs Lambda\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nHere we transform the coefficients back to the original scale when non-standardized X is used.\n\nCode# Reverse standardization\nstd_devs = scaler.scale_  # Feature standard deviations\nmeans = scaler.mean_      # Feature means\n\ncoef_original = optimal_coefficients / std_devs\ncoef_original\n\narray([-0.36833293, -0.00515876, -0.0117376 ,  1.05033188, -1.27442867,\n        0.16543865,  0.74776248,  1.66690255,  0.54828706, -0.56512958])\n\nCodeintercept_original = ridge_cv.intercept_ - np.sum(optimal_coefficients * means / std_devs)\nintercept_original\n\n21.21467495074396\n\n\n\nCode## the lambda size is matching the size used in cv.glmnet(). Should it be multiplied by 32?\nlambdas = 5 * 10 ** np.linspace(-1, 3, 100)\n\n# RidgeCV with 10-fold cross-validation\nridge_cv = RidgeCV(alphas=lambdas, scoring=\"neg_mean_squared_error\", cv=10)\nridge_cv.fit(X_scaled, y)\n\n\n\n\nRidgeCV(alphas=array([5.00000000e-01, 5.48749383e-01, 6.02251770e-01, 6.60970574e-01,\n       7.25414389e-01, 7.96141397e-01, 8.73764200e-01, 9.58955131e-01,\n       1.05245207e+00, 1.15506485e+00, 1.26768225e+00, 1.39127970e+00,\n       1.52692775e+00, 1.67580133e+00, 1.83918989e+00, 2.01850863e+00,\n       2.21531073e+00, 2.43130079e+00, 2.66834962e+00, 2.92851041e+00,\n       3.21403656e+00, 3.52740116e+0...\n       5.88405976e+02, 6.45774833e+02, 7.08737081e+02, 7.77838072e+02,\n       8.53676324e+02, 9.36908711e+02, 1.02825615e+03, 1.12850986e+03,\n       1.23853818e+03, 1.35929412e+03, 1.49182362e+03, 1.63727458e+03,\n       1.79690683e+03, 1.97210303e+03, 2.16438064e+03, 2.37540508e+03,\n       2.60700414e+03, 2.86118383e+03, 3.14014572e+03, 3.44630605e+03,\n       3.78231664e+03, 4.15108784e+03, 4.55581378e+03, 5.00000000e+03]),\n        cv=10, scoring='neg_mean_squared_error')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nRidgeCVRidgeCV(alphas=array([5.00000000e-01, 5.48749383e-01, 6.02251770e-01, 6.60970574e-01,\n       7.25414389e-01, 7.96141397e-01, 8.73764200e-01, 9.58955131e-01,\n       1.05245207e+00, 1.15506485e+00, 1.26768225e+00, 1.39127970e+00,\n       1.52692775e+00, 1.67580133e+00, 1.83918989e+00, 2.01850863e+00,\n       2.21531073e+00, 2.43130079e+00, 2.66834962e+00, 2.92851041e+00,\n       3.21403656e+00, 3.52740116e+0...\n       5.88405976e+02, 6.45774833e+02, 7.08737081e+02, 7.77838072e+02,\n       8.53676324e+02, 9.36908711e+02, 1.02825615e+03, 1.12850986e+03,\n       1.23853818e+03, 1.35929412e+03, 1.49182362e+03, 1.63727458e+03,\n       1.79690683e+03, 1.97210303e+03, 2.16438064e+03, 2.37540508e+03,\n       2.60700414e+03, 2.86118383e+03, 3.14014572e+03, 3.44630605e+03,\n       3.78231664e+03, 4.15108784e+03, 4.55581378e+03, 5.00000000e+03]),\n        cv=10, scoring='neg_mean_squared_error')\n\n\n\nCodebest_lambda = ridge_cv.alpha_\n\ncoefficients = []\nfor lam in lambdas:\n    ridge = Ridge(alpha=lam)\n    ridge.fit(X_scaled, y)\n    coefficients.append(ridge.coef_)\n\n\n\n\nRidge(alpha=5000.0)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nRidgeRidge(alpha=5000.0)\n\n\n\nCodecoefficients = np.array(coefficients)\nfor i in range(coefficients.shape[1]):\n    plt.plot(np.log(lambdas), coefficients[:, i])\nplt.xlabel(\"Log(Lambda)\")\nplt.ylabel(\"Coefficients\")\nplt.show()"
  },
  {
    "objectID": "code/14-tree-code.html",
    "href": "code/14-tree-code.html",
    "title": "14 - Tree Methods Code Demo",
    "section": "",
    "text": "Code# generate some data \nset.seed(2025)\nn &lt;- 1000\nx1 &lt;- runif(n, -1, 1)\nx2 &lt;- runif(n, -1, 1)\ny &lt;- rbinom(n, size = 1, prob = ifelse((x1 + x2 &gt; -0.5) & (x1 + x2 &lt; 0.5) , 0.8, 0.2))\nxgrid &lt;- expand.grid(x1 = seq(-1, 1, 0.01), x2 = seq(-1, 1, 0.01))\n\n\n\n\n\nCodelibrary(rpart)\nrpart_fit &lt;- rpart(as.factor(y) ~ x1 + x2, data = data.frame(x1, x2, y))\npred &lt;- matrix(predict(rpart_fit, xgrid, type = \"class\") == 1, 201, 201)\ncontour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = \"\",\n        lwd = 3)\npoints(xgrid, pch = \".\", cex = 0.2, col = ifelse(pred, \"lightblue\", \"pink\"))\npoints(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt = \"n\", xaxt = \"n\")\nbox()\ntitle(main = list(\"CART\"))\n\n\n\n\n\n\n\n\nCodepar(mar = c(0.5, 0, 0.5, 0))\nplot(rpart_fit)\ntext(rpart_fit, cex = 0.8)\n\n\n\n\n\n\n\n\n\nrpart() uses the 10-fold CV (xval in rpart.control())\n\ncp is the complexity parameter\n\n\nCoderpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, \n              maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10,\n              surrogatestyle = 0, maxdepth = 30, ...)\n\n\n\nCoderpart_fit$cptable\n\n          CP nsplit rel error    xerror       xstd\n1 0.06485356      0 1.0000000 1.0000000 0.03304618\n2 0.05857741      3 0.7845188 0.9058577 0.03277990\n3 0.05439331      4 0.7259414 0.8284519 0.03235476\n4 0.03242678      5 0.6715481 0.7050209 0.03127115\n5 0.02719665      9 0.5418410 0.6401674 0.03048685\n6 0.01464435     10 0.5146444 0.6087866 0.03004981\n7 0.01359833     11 0.5000000 0.5732218 0.02950636\n8 0.01000000     13 0.4728033 0.5732218 0.02950636\n\n\n\nCodeplotcp(rpart_fit)\n\n\n\n\n\n\n\n\nCodeprunedtree &lt;- prune(rpart_fit, cp = 0.012)\nprunedtree\n\nn= 1000 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 1000 478 0 (0.5220000 0.4780000)  \n    2) x1&gt;=0.8238083 93  26 0 (0.7204301 0.2795699)  \n      4) x2&gt;=-0.4195459 74  10 0 (0.8648649 0.1351351) *\n      5) x2&lt; -0.4195459 19   3 1 (0.1578947 0.8421053) *\n    3) x1&lt; 0.8238083 907 452 0 (0.5016538 0.4983462)  \n      6) x2&lt; -0.546795 203  69 0 (0.6600985 0.3399015)  \n       12) x1&lt; 0.3121958 145  27 0 (0.8137931 0.1862069) *\n       13) x1&gt;=0.3121958 58  16 1 (0.2758621 0.7241379) *\n      7) x2&gt;=-0.546795 704 321 1 (0.4559659 0.5440341)  \n       14) x2&gt;=0.5828916 189  74 0 (0.6084656 0.3915344)  \n         28) x1&gt;=-0.1670433 115  23 0 (0.8000000 0.2000000) *\n         29) x1&lt; -0.1670433 74  23 1 (0.3108108 0.6891892) *\n       15) x2&lt; 0.5828916 515 206 1 (0.4000000 0.6000000)  \n         30) x1&lt; -0.9438582 17   2 0 (0.8823529 0.1176471) *\n         31) x1&gt;=-0.9438582 498 191 1 (0.3835341 0.6164659)  \n           62) x1&gt;=0.192903 169  79 1 (0.4674556 0.5325444)  \n            124) x2&gt;=-0.1156255 97  33 0 (0.6597938 0.3402062) *\n            125) x2&lt; -0.1156255 72  15 1 (0.2083333 0.7916667) *\n           63) x1&lt; 0.192903 329 112 1 (0.3404255 0.6595745)  \n            126) x2&lt; -0.2736896 80  31 0 (0.6125000 0.3875000)  \n              252) x1&lt; -0.05781444 57  16 0 (0.7192982 0.2807018) *\n              253) x1&gt;=-0.05781444 23   8 1 (0.3478261 0.6521739) *\n            127) x2&gt;=-0.2736896 249  63 1 (0.2530120 0.7469880)  \n              254) x1&lt; -0.568942 82  34 1 (0.4146341 0.5853659)  \n                508) x2&lt; 0.2270988 39  13 0 (0.6666667 0.3333333) *\n                509) x2&gt;=0.2270988 43   8 1 (0.1860465 0.8139535) *\n              255) x1&gt;=-0.568942 167  29 1 (0.1736527 0.8263473) *\n\n\n\nCoderpart.plot::rpart.plot(prunedtree)\n\n\n\n\n\n\n\n\nRead ISLR Sec 8.3 for tree() demo.\n\n\nCodelibrary(ipred)\nbag_fit &lt;- bagging(as.factor(y) ~ x1 + x2, data = data.frame(x1, x2, y), \n                   nbagg = 200, ns = 400)\npred &lt;- matrix(predict(prune(bag_fit), xgrid) == 1, 201, 201)\ncontour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = \"\", \n        lwd = 3)\npoints(xgrid, pch = \".\", cex = 0.2, col = ifelse(pred, \"lightblue\", \"pink\"))\npoints(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt = \"n\", xaxt = \"n\")\nbox()\ntitle(main = list(\"Bagging\"))\n\n\n\n\n\n\n\n\n\nCodelibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\nCoderf_fit &lt;- randomForest(cbind(x1, x2), as.factor(y), ntree = 200, mtry = 1, \n                      nodesize = 20, sampsize = 400)\npred &lt;- matrix(predict(rf_fit, xgrid) == 1, 201, 201)\ncontour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = \"\",\n        lwd = 3)\npoints(xgrid, pch = \".\", cex = 0.2, col = ifelse(pred, \"lightblue\", \"pink\"))\npoints(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt=\"n\", xaxt = \"n\")\nbox()\ntitle(main = list(\"Random Forests\", cex = 2))\n\n\n\n\n\n\n\n\n\nCodelibrary(gbm)\n\nLoaded gbm 2.2.2\n\n\nThis version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n\nCodegbm_fit &lt;- gbm(y ~ ., data = data.frame(x1, x2, y), distribution = \"bernoulli\", \n               n.trees = 10000, shrinkage = 0.01, bag.fraction = 0.6, \n               interaction.depth = 2, cv.folds = 10)\nusetree &lt;- gbm.perf(gbm_fit, method = \"cv\", plot.it = FALSE)\nFx &lt;- predict(gbm_fit, xgrid, n.trees=usetree)\npred &lt;- matrix(1 / (1 + exp(-2 * Fx)) &gt; 0.5, 201, 201)\ncontour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = \"\",\n        lwd = 3)\npoints(xgrid, pch = \".\", cex = 0.2, col = ifelse(pred, \"lightblue\", \"pink\"))\npoints(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt = \"n\", xaxt = \"n\")\nbox()\ntitle(main = list(\"Boosting\"))"
  },
  {
    "objectID": "code/14-tree-code.html#cart",
    "href": "code/14-tree-code.html#cart",
    "title": "14 - Tree Methods Code Demo",
    "section": "",
    "text": "Codelibrary(rpart)\nrpart_fit &lt;- rpart(as.factor(y) ~ x1 + x2, data = data.frame(x1, x2, y))\npred &lt;- matrix(predict(rpart_fit, xgrid, type = \"class\") == 1, 201, 201)\ncontour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = \"\",\n        lwd = 3)\npoints(xgrid, pch = \".\", cex = 0.2, col = ifelse(pred, \"lightblue\", \"pink\"))\npoints(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt = \"n\", xaxt = \"n\")\nbox()\ntitle(main = list(\"CART\"))\n\n\n\n\n\n\n\n\nCodepar(mar = c(0.5, 0, 0.5, 0))\nplot(rpart_fit)\ntext(rpart_fit, cex = 0.8)\n\n\n\n\n\n\n\n\n\nrpart() uses the 10-fold CV (xval in rpart.control())\n\ncp is the complexity parameter\n\n\nCoderpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, \n              maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10,\n              surrogatestyle = 0, maxdepth = 30, ...)\n\n\n\nCoderpart_fit$cptable\n\n          CP nsplit rel error    xerror       xstd\n1 0.06485356      0 1.0000000 1.0000000 0.03304618\n2 0.05857741      3 0.7845188 0.9058577 0.03277990\n3 0.05439331      4 0.7259414 0.8284519 0.03235476\n4 0.03242678      5 0.6715481 0.7050209 0.03127115\n5 0.02719665      9 0.5418410 0.6401674 0.03048685\n6 0.01464435     10 0.5146444 0.6087866 0.03004981\n7 0.01359833     11 0.5000000 0.5732218 0.02950636\n8 0.01000000     13 0.4728033 0.5732218 0.02950636\n\n\n\nCodeplotcp(rpart_fit)\n\n\n\n\n\n\n\n\nCodeprunedtree &lt;- prune(rpart_fit, cp = 0.012)\nprunedtree\n\nn= 1000 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 1000 478 0 (0.5220000 0.4780000)  \n    2) x1&gt;=0.8238083 93  26 0 (0.7204301 0.2795699)  \n      4) x2&gt;=-0.4195459 74  10 0 (0.8648649 0.1351351) *\n      5) x2&lt; -0.4195459 19   3 1 (0.1578947 0.8421053) *\n    3) x1&lt; 0.8238083 907 452 0 (0.5016538 0.4983462)  \n      6) x2&lt; -0.546795 203  69 0 (0.6600985 0.3399015)  \n       12) x1&lt; 0.3121958 145  27 0 (0.8137931 0.1862069) *\n       13) x1&gt;=0.3121958 58  16 1 (0.2758621 0.7241379) *\n      7) x2&gt;=-0.546795 704 321 1 (0.4559659 0.5440341)  \n       14) x2&gt;=0.5828916 189  74 0 (0.6084656 0.3915344)  \n         28) x1&gt;=-0.1670433 115  23 0 (0.8000000 0.2000000) *\n         29) x1&lt; -0.1670433 74  23 1 (0.3108108 0.6891892) *\n       15) x2&lt; 0.5828916 515 206 1 (0.4000000 0.6000000)  \n         30) x1&lt; -0.9438582 17   2 0 (0.8823529 0.1176471) *\n         31) x1&gt;=-0.9438582 498 191 1 (0.3835341 0.6164659)  \n           62) x1&gt;=0.192903 169  79 1 (0.4674556 0.5325444)  \n            124) x2&gt;=-0.1156255 97  33 0 (0.6597938 0.3402062) *\n            125) x2&lt; -0.1156255 72  15 1 (0.2083333 0.7916667) *\n           63) x1&lt; 0.192903 329 112 1 (0.3404255 0.6595745)  \n            126) x2&lt; -0.2736896 80  31 0 (0.6125000 0.3875000)  \n              252) x1&lt; -0.05781444 57  16 0 (0.7192982 0.2807018) *\n              253) x1&gt;=-0.05781444 23   8 1 (0.3478261 0.6521739) *\n            127) x2&gt;=-0.2736896 249  63 1 (0.2530120 0.7469880)  \n              254) x1&lt; -0.568942 82  34 1 (0.4146341 0.5853659)  \n                508) x2&lt; 0.2270988 39  13 0 (0.6666667 0.3333333) *\n                509) x2&gt;=0.2270988 43   8 1 (0.1860465 0.8139535) *\n              255) x1&gt;=-0.568942 167  29 1 (0.1736527 0.8263473) *\n\n\n\nCoderpart.plot::rpart.plot(prunedtree)\n\n\n\n\n\n\n\n\nRead ISLR Sec 8.3 for tree() demo."
  },
  {
    "objectID": "code/14-tree-code.html#bagging",
    "href": "code/14-tree-code.html#bagging",
    "title": "14 - Tree Methods Code Demo",
    "section": "",
    "text": "Codelibrary(ipred)\nbag_fit &lt;- bagging(as.factor(y) ~ x1 + x2, data = data.frame(x1, x2, y), \n                   nbagg = 200, ns = 400)\npred &lt;- matrix(predict(prune(bag_fit), xgrid) == 1, 201, 201)\ncontour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = \"\", \n        lwd = 3)\npoints(xgrid, pch = \".\", cex = 0.2, col = ifelse(pred, \"lightblue\", \"pink\"))\npoints(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt = \"n\", xaxt = \"n\")\nbox()\ntitle(main = list(\"Bagging\"))"
  },
  {
    "objectID": "code/14-tree-code.html#random-forests",
    "href": "code/14-tree-code.html#random-forests",
    "title": "14 - Tree Methods Code Demo",
    "section": "",
    "text": "Codelibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\nCoderf_fit &lt;- randomForest(cbind(x1, x2), as.factor(y), ntree = 200, mtry = 1, \n                      nodesize = 20, sampsize = 400)\npred &lt;- matrix(predict(rf_fit, xgrid) == 1, 201, 201)\ncontour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = \"\",\n        lwd = 3)\npoints(xgrid, pch = \".\", cex = 0.2, col = ifelse(pred, \"lightblue\", \"pink\"))\npoints(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt=\"n\", xaxt = \"n\")\nbox()\ntitle(main = list(\"Random Forests\", cex = 2))"
  },
  {
    "objectID": "code/14-tree-code.html#boosting",
    "href": "code/14-tree-code.html#boosting",
    "title": "14 - Tree Methods Code Demo",
    "section": "",
    "text": "Codelibrary(gbm)\n\nLoaded gbm 2.2.2\n\n\nThis version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n\nCodegbm_fit &lt;- gbm(y ~ ., data = data.frame(x1, x2, y), distribution = \"bernoulli\", \n               n.trees = 10000, shrinkage = 0.01, bag.fraction = 0.6, \n               interaction.depth = 2, cv.folds = 10)\nusetree &lt;- gbm.perf(gbm_fit, method = \"cv\", plot.it = FALSE)\nFx &lt;- predict(gbm_fit, xgrid, n.trees=usetree)\npred &lt;- matrix(1 / (1 + exp(-2 * Fx)) &gt; 0.5, 201, 201)\ncontour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = \"\",\n        lwd = 3)\npoints(xgrid, pch = \".\", cex = 0.2, col = ifelse(pred, \"lightblue\", \"pink\"))\npoints(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt = \"n\", xaxt = \"n\")\nbox()\ntitle(main = list(\"Boosting\"))"
  },
  {
    "objectID": "code/14-tree-code.html#cart-1",
    "href": "code/14-tree-code.html#cart-1",
    "title": "14 - Tree Methods Code Demo",
    "section": "\n2.1 CART",
    "text": "2.1 CART\n\nCodedt = DecisionTreeClassifier(random_state=2025)\nparam_grid = {\n    \"max_depth\": [3, 5, 7, 10],  # Control tree depth\n    \"min_samples_split\": [2, 5, 10],  # Minimum samples to split a node\n    \"min_samples_leaf\": [1, 5, 10]  # Minimum samples in a leaf node\n}\ngrid_search = GridSearchCV(dt, param_grid, cv=10, scoring=\"accuracy\")\ngrid_search.fit(np.c_[x1, x2], y)\n\n\n\n\nGridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=2025),\n             param_grid={'max_depth': [3, 5, 7, 10],\n                         'min_samples_leaf': [1, 5, 10],\n                         'min_samples_split': [2, 5, 10]},\n             scoring='accuracy')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\nÂ Â GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=2025),\n             param_grid={'max_depth': [3, 5, 7, 10],\n                         'min_samples_leaf': [1, 5, 10],\n                         'min_samples_split': [2, 5, 10]},\n             scoring='accuracy') \n\n\nestimator: DecisionTreeClassifierDecisionTreeClassifier(random_state=2025) \n\nÂ DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier(random_state=2025) \n\n\n\n\nCodebest_dt = grid_search.best_estimator_\nbest_pred = best_dt.predict(xgrid).reshape(201, 201)\n\nplt.figure()\nplt.contourf(x1_grid, x2_grid, best_pred, cmap=\"coolwarm\", alpha=0.3)\nplt.scatter(x1, x2, c=y, cmap=\"bwr\", s=15, alpha=0.7)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"CART\")\nplt.show()"
  },
  {
    "objectID": "code/14-tree-code.html#bagging-1",
    "href": "code/14-tree-code.html#bagging-1",
    "title": "14 - Tree Methods Code Demo",
    "section": "\n2.2 Bagging",
    "text": "2.2 Bagging\n\nCodefrom sklearn.ensemble import BaggingClassifier\n\n\n\nCodebagging_clf = BaggingClassifier(\n    estimator=DecisionTreeClassifier(),\n    n_estimators=200,  # Same as nbagg = 200 in R\n    max_samples=0.4,   # Equivalent to ns = 400 in R (approx)\n    random_state=2025\n)\nbagging_clf.fit(np.c_[x1, x2], y)\n\n\n\n\nBaggingClassifier(estimator=DecisionTreeClassifier(), max_samples=0.4,\n                  n_estimators=200, random_state=2025)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\nÂ Â BaggingClassifier?Documentation for BaggingClassifieriFittedBaggingClassifier(estimator=DecisionTreeClassifier(), max_samples=0.4,\n                  n_estimators=200, random_state=2025) \n\n\nestimator: DecisionTreeClassifierDecisionTreeClassifier() \n\nÂ DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier() \n\n\n\n\nCodebagging_pred = bagging_clf.predict(xgrid).reshape(201, 201)\nplt.figure()\nplt.contourf(x1_grid, x2_grid, bagging_pred, cmap=\"coolwarm\", alpha=0.3)\nplt.scatter(x1, x2, c=y, cmap=\"bwr\", s=15, alpha=0.7)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Bagging\")\nplt.show()"
  },
  {
    "objectID": "code/14-tree-code.html#random-forests-1",
    "href": "code/14-tree-code.html#random-forests-1",
    "title": "14 - Tree Methods Code Demo",
    "section": "\n2.3 Random Forests",
    "text": "2.3 Random Forests\n\nCodefrom sklearn.ensemble import RandomForestClassifier\n\n\n\nCoderf_clf = RandomForestClassifier(\n    n_estimators=200,  # Equivalent to ntree = 200 in R\n    max_features=1,    # Equivalent to mtry = 1 in R\n    min_samples_leaf=20,  # Equivalent to nodesize = 20 in R\n    max_samples=400,  # Equivalent to sampsize = 400 in R\n    random_state=2025\n)\nrf_clf.fit(np.c_[x1, x2], y)\n\n\n\n\nRandomForestClassifier(max_features=1, max_samples=400, min_samples_leaf=20,\n                       n_estimators=200, random_state=2025)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nÂ Â RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(max_features=1, max_samples=400, min_samples_leaf=20,\n                       n_estimators=200, random_state=2025) \n\n\nCoderf_pred = rf_clf.predict(xgrid).reshape(201, 201)\nplt.figure()\nplt.contourf(x1_grid, x2_grid, rf_pred, cmap=\"coolwarm\", alpha=0.3)\nplt.scatter(x1, x2, c=y, cmap=\"bwr\", s=15, alpha=0.7)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Random Forests\")\nplt.show()"
  },
  {
    "objectID": "code/14-tree-code.html#boosting-1",
    "href": "code/14-tree-code.html#boosting-1",
    "title": "14 - Tree Methods Code Demo",
    "section": "\n2.4 Boosting",
    "text": "2.4 Boosting\n\nCodefrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_score\n\n\n\nCodegbm_clf = GradientBoostingClassifier(\n    n_estimators=10000,  # Equivalent to n.trees = 10000\n    learning_rate=0.01,  # Equivalent to shrinkage = 0.01\n    subsample=0.6,       # Equivalent to bag.fraction = 0.6\n    max_depth=2,         # Equivalent to interaction.depth = 2\n    random_state=2025\n)\ncv_scores = []\nfor n_trees in range(100, 2000, 100):  # Testing tree counts\n    gbm_clf.set_params(n_estimators=n_trees)\n    score = np.mean(cross_val_score(gbm_clf, np.c_[x1, x2], y, cv=10, scoring='accuracy'))\n    cv_scores.append((n_trees, score))\n\n\n\n\nGradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=1900,\n                           random_state=2025, subsample=0.6)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nÂ Â GradientBoostingClassifier?Documentation for GradientBoostingClassifieriNot fittedGradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=1900,\n                           random_state=2025, subsample=0.6) \n\n\nCodebest_n_trees = max(cv_scores, key=lambda x: x[1])[0]\ngbm_clf.set_params(n_estimators=best_n_trees)\n\n\n\n\nGradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=500,\n                           random_state=2025, subsample=0.6)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nÂ Â GradientBoostingClassifier?Documentation for GradientBoostingClassifieriNot fittedGradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=500,\n                           random_state=2025, subsample=0.6) \n\n\nCodegbm_clf.fit(np.c_[x1, x2], y)\n\n\n\n\nGradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=500,\n                           random_state=2025, subsample=0.6)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nÂ Â GradientBoostingClassifier?Documentation for GradientBoostingClassifieriFittedGradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=500,\n                           random_state=2025, subsample=0.6) \n\n\nCodeFx = gbm_clf.decision_function(xgrid)  # Raw output before sigmoid transformation\npred = (1 / (1 + np.exp(-2 * Fx)) &gt; 0.5).reshape(201, 201)\nplt.figure()\nplt.contourf(x1_grid, x2_grid, pred, cmap=\"coolwarm\", alpha=0.3)\nplt.scatter(x1, x2, c=y, cmap=\"bwr\", s=15, alpha=0.7)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Gradient Boosting\")\nplt.show()"
  },
  {
    "objectID": "code/09-class-glm-code.html",
    "href": "code/09-class-glm-code.html",
    "title": "09- Logistic Regression Code Demo",
    "section": "",
    "text": "Codelibrary(ISLR2)\n\n\n\n\nCodedata(\"Default\")\nlogit_fit &lt;- glm(default ~ balance, data = Default, family = binomial)\ncoef(summary(logit_fit))\n\n                 Estimate   Std. Error   z value      Pr(&gt;|z|)\n(Intercept) -10.651330614 0.3611573721 -29.49221 3.623124e-191\nbalance       0.005498917 0.0002203702  24.95309 1.976602e-137\n\n\n\nCodepi_hat &lt;- predict(logit_fit, type = \"response\")\neta_hat &lt;- predict(logit_fit, type = \"link\")  ## default gives us b0 + b1*x\npredict(logit_fit, newdata = data.frame(balance = 2000), type = \"response\")\n\n        1 \n0.5857694 \n\n\n\nCodepred_prob &lt;- predict(logit_fit, type = \"response\")\ntable(pred_prob &gt; 0.5, Default$default)\n\n       \n          No  Yes\n  FALSE 9625  233\n  TRUE    42  100\n\n\n\nCodelibrary(ROCR)\n# create an object of class prediction \npred &lt;- ROCR::prediction(predictions = pred_prob, labels = Default$default)\n\n# calculates the ROC curve\nroc &lt;- ROCR::performance(prediction.obj = pred, measure = \"tpr\", x.measure = \"fpr\")\nplot(roc, colorize = TRUE, lwd = 3)\n\n\n\n\n\n\n\n\nCodeauc &lt;- ROCR::performance(prediction.obj = pred, measure = \"auc\")\nauc@y.values\n\n[[1]]\n[1] 0.9479785\n\n\n\n\nCodelibrary(foreign)\nmultino_data &lt;- foreign::read.dta(\"../data/hsbdemo.dta\")\n\n\n\nCodemultino_data$prog2 &lt;- relevel(multino_data$prog, ref = \"academic\")\nlevels(multino_data$prog2)\n\n[1] \"academic\" \"general\"  \"vocation\"\n\nCodelevels(multino_data$ses)\n\n[1] \"low\"    \"middle\" \"high\"  \n\n\n\nCodelibrary(nnet)\nmultino_fit &lt;- nnet::multinom(prog2 ~ ses + write, data = multino_data, trace = FALSE)\nsumm &lt;- summary(multino_fit)\nsumm$coefficients\n\n         (Intercept)  sesmiddle    seshigh      write\ngeneral     2.852198 -0.5332810 -1.1628226 -0.0579287\nvocation    5.218260  0.2913859 -0.9826649 -0.1136037\n\n\n\nCodehead(fitted(multino_fit))\n\n   academic   general  vocation\n1 0.1482764 0.3382454 0.5134781\n2 0.1202017 0.1806283 0.6991700\n3 0.4186747 0.2368082 0.3445171\n4 0.1726885 0.3508384 0.4764731\n5 0.1001231 0.1689374 0.7309395\n6 0.3533566 0.2377976 0.4088458\n\n\n\nCodedses &lt;- data.frame(ses = c(\"low\", \"middle\", \"high\"), \n                   write = mean(multino_data$write))\npredict(multino_fit, newdata = dses, type = \"probs\")\n\n   academic   general  vocation\n1 0.4396845 0.3581917 0.2021238\n2 0.4777488 0.2283353 0.2939159\n3 0.7009007 0.1784939 0.1206054\n\n\nWe can also use glmnet package.\n\nCodelibrary(fastDummies) # https://jacobkap.github.io/fastDummies/\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\nCodeprog &lt;- multino_data$prog2\ndummy_dat &lt;- dummy_cols(multino_data, select_columns = c(\"ses\"))\nx &lt;- dummy_dat |&gt; dplyr::select(ses_middle, ses_high, write)\nfit &lt;- glmnet(x = x, y = prog, family = \"multinomial\", lambda = 0)\ncoef(fit)\n\n$academic\n4 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s0\n           -2.69052260\nses_middle  .         \nses_high    0.98278066\nwrite       0.05793834\n\n$general\n4 x 1 sparse Matrix of class \"dgCMatrix\"\n                   s0\n            0.1625919\nses_middle -0.5337675\nses_high   -0.1805596\nwrite       .        \n\n$vocation\n4 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s0\n            2.52793069\nses_middle  0.29119238\nses_high    .         \nwrite      -0.05566551\n\nCodenewx &lt;- x[1:3, ]\nnewx[, 3] &lt;- mean(multino_data$write)\npredict(fit, as.matrix(newx), type=\"response\")\n\n, , s0\n\n   academic   general  vocation\n1 0.4396037 0.3582719 0.2021245\n2 0.4777583 0.2283218 0.2939199\n3 0.7009084 0.1784762 0.1206154\n\nCode# model_mat &lt;- model.matrix(prog2~ses+write, data=multino_data)"
  },
  {
    "objectID": "code/09-class-glm-code.html#r-implementation",
    "href": "code/09-class-glm-code.html#r-implementation",
    "title": "09- Logistic Regression Code Demo",
    "section": "",
    "text": "Codelibrary(ISLR2)\n\n\n\n\nCodedata(\"Default\")\nlogit_fit &lt;- glm(default ~ balance, data = Default, family = binomial)\ncoef(summary(logit_fit))\n\n                 Estimate   Std. Error   z value      Pr(&gt;|z|)\n(Intercept) -10.651330614 0.3611573721 -29.49221 3.623124e-191\nbalance       0.005498917 0.0002203702  24.95309 1.976602e-137\n\n\n\nCodepi_hat &lt;- predict(logit_fit, type = \"response\")\neta_hat &lt;- predict(logit_fit, type = \"link\")  ## default gives us b0 + b1*x\npredict(logit_fit, newdata = data.frame(balance = 2000), type = \"response\")\n\n        1 \n0.5857694 \n\n\n\nCodepred_prob &lt;- predict(logit_fit, type = \"response\")\ntable(pred_prob &gt; 0.5, Default$default)\n\n       \n          No  Yes\n  FALSE 9625  233\n  TRUE    42  100\n\n\n\nCodelibrary(ROCR)\n# create an object of class prediction \npred &lt;- ROCR::prediction(predictions = pred_prob, labels = Default$default)\n\n# calculates the ROC curve\nroc &lt;- ROCR::performance(prediction.obj = pred, measure = \"tpr\", x.measure = \"fpr\")\nplot(roc, colorize = TRUE, lwd = 3)\n\n\n\n\n\n\n\n\nCodeauc &lt;- ROCR::performance(prediction.obj = pred, measure = \"auc\")\nauc@y.values\n\n[[1]]\n[1] 0.9479785\n\n\n\n\nCodelibrary(foreign)\nmultino_data &lt;- foreign::read.dta(\"../data/hsbdemo.dta\")\n\n\n\nCodemultino_data$prog2 &lt;- relevel(multino_data$prog, ref = \"academic\")\nlevels(multino_data$prog2)\n\n[1] \"academic\" \"general\"  \"vocation\"\n\nCodelevels(multino_data$ses)\n\n[1] \"low\"    \"middle\" \"high\"  \n\n\n\nCodelibrary(nnet)\nmultino_fit &lt;- nnet::multinom(prog2 ~ ses + write, data = multino_data, trace = FALSE)\nsumm &lt;- summary(multino_fit)\nsumm$coefficients\n\n         (Intercept)  sesmiddle    seshigh      write\ngeneral     2.852198 -0.5332810 -1.1628226 -0.0579287\nvocation    5.218260  0.2913859 -0.9826649 -0.1136037\n\n\n\nCodehead(fitted(multino_fit))\n\n   academic   general  vocation\n1 0.1482764 0.3382454 0.5134781\n2 0.1202017 0.1806283 0.6991700\n3 0.4186747 0.2368082 0.3445171\n4 0.1726885 0.3508384 0.4764731\n5 0.1001231 0.1689374 0.7309395\n6 0.3533566 0.2377976 0.4088458\n\n\n\nCodedses &lt;- data.frame(ses = c(\"low\", \"middle\", \"high\"), \n                   write = mean(multino_data$write))\npredict(multino_fit, newdata = dses, type = \"probs\")\n\n   academic   general  vocation\n1 0.4396845 0.3581917 0.2021238\n2 0.4777488 0.2283353 0.2939159\n3 0.7009007 0.1784939 0.1206054\n\n\nWe can also use glmnet package.\n\nCodelibrary(fastDummies) # https://jacobkap.github.io/fastDummies/\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\nCodeprog &lt;- multino_data$prog2\ndummy_dat &lt;- dummy_cols(multino_data, select_columns = c(\"ses\"))\nx &lt;- dummy_dat |&gt; dplyr::select(ses_middle, ses_high, write)\nfit &lt;- glmnet(x = x, y = prog, family = \"multinomial\", lambda = 0)\ncoef(fit)\n\n$academic\n4 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s0\n           -2.69052260\nses_middle  .         \nses_high    0.98278066\nwrite       0.05793834\n\n$general\n4 x 1 sparse Matrix of class \"dgCMatrix\"\n                   s0\n            0.1625919\nses_middle -0.5337675\nses_high   -0.1805596\nwrite       .        \n\n$vocation\n4 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s0\n            2.52793069\nses_middle  0.29119238\nses_high    .         \nwrite      -0.05566551\n\nCodenewx &lt;- x[1:3, ]\nnewx[, 3] &lt;- mean(multino_data$write)\npredict(fit, as.matrix(newx), type=\"response\")\n\n, , s0\n\n   academic   general  vocation\n1 0.4396037 0.3582719 0.2021245\n2 0.4777583 0.2283218 0.2939199\n3 0.7009084 0.1784762 0.1206154\n\nCode# model_mat &lt;- model.matrix(prog2~ses+write, data=multino_data)"
  },
  {
    "objectID": "code/09-class-glm-code.html#python-implementation",
    "href": "code/09-class-glm-code.html#python-implementation",
    "title": "09- Logistic Regression Code Demo",
    "section": "\n2 Python implementation",
    "text": "2 Python implementation\n\n2.1 Binary (Binomial) Logistic Regression\n\nCodeimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\n\n\nCode# Load your dataset\nDefault = pd.read_csv(\"../data/Default.csv\")\n\n\n\nCodeDefault['default'] = Default['default'].map({'Yes': 1, 'No': 0})\nfrom statsmodels.formula.api import logit\nlogit_fit = logit(formula='default ~ balance', data=Default).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.079823\n         Iterations 10\n\nCodelogit_fit.summary2().tables[1]\n\n               Coef.  Std.Err.          z          P&gt;|z|     [0.025    0.975]\nIntercept -10.651331  0.361169 -29.491287  3.723665e-191 -11.359208 -9.943453\nbalance     0.005499  0.000220  24.952404  2.010855e-137   0.005067  0.005931\n\n\n\nCodepi_hat = logit_fit.predict(Default[['balance']])  # Type = \"response\" in R\neta_hat = logit_fit.predict(Default[['balance']], which=\"linear\")  # Type = \"link\" in R\nnew_data = pd.DataFrame({'balance': [2000]})\nlogit_fit.predict(new_data)\n\n0    0.585769\ndtype: float64\n\n\n\nCodefrom sklearn.metrics import confusion_matrix\npred_prob = logit_fit.predict(Default[['balance']])  # Type = \"response\" in R\n# Create predictions based on a 0.5 threshold\npred_class = (pred_prob &gt; 0.5).astype(int)  # Convert to binary class (0 or 1)\n## C00: true negatives; C10: false negatives; C01: false postives; C11: true positives\nconfusion_matrix(y_true=Default['default'], y_pred=pred_class)\n\narray([[9625,   42],\n       [ 233,  100]])\n\n\n\nCodefrom sklearn.metrics import roc_curve, roc_auc_score\n\n# Calculate the ROC curve\nfpr, tpr, thresholds = roc_curve(Default['default'], pred_prob)\n\n# Calculate the AUC (Area Under the Curve)\nauc = roc_auc_score(Default['default'], pred_prob)\nauc\n\n0.9479784946837808\n\n\n\nCodeplt.figure()\nplt.plot(fpr, tpr, color='blue')\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.show()\n\n\n\n\n\n\n\n\n2.2 Multinomial Logistic Regression\n\nCodemultino_data = pd.read_stata(\"../data/hsbdemo.dta\")\n\n\n\nCodemultino_data['prog2'] = multino_data['prog'].cat.reorder_categories(\n    ['academic'] + [cat for cat in multino_data['prog'].cat.categories if cat != 'academic'],\n    ordered=True\n)\nmultino_data['prog2'].unique()\n\n['vocation', 'general', 'academic']\nCategories (3, object): ['academic' &lt; 'general' &lt; 'vocation']\n\nCodemultino_data['ses'].unique()\n\n['low', 'middle', 'high']\nCategories (3, object): ['low' &lt; 'middle' &lt; 'high']\n\n\n\nCodemultino_data['prog_int'] = multino_data['prog2'].map({\n    'academic': 0,\n    'general': 1,\n    'vocation': 2\n})\nmultino_data['prog_int'] = multino_data['prog_int'].cat.codes\n\n\n\nCodefrom statsmodels.formula.api import mnlogit\nmultino_fit = mnlogit(\"prog_int ~ ses + write\", data=multino_data).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.899909\n         Iterations 6\n\nCodemultino_fit.params\n\n                      0         1\nIntercept      2.852186  5.218200\nses[T.middle] -0.533291  0.291393\nses[T.high]   -1.162832 -0.982670\nwrite         -0.057928 -0.113603\n\n\n\nCodefitted_df = pd.DataFrame(multino_fit.predict(), \n                         columns=['academic', 'general', 'vocation'])\nfitted_df.head()\n\n   academic   general  vocation\n0  0.148278  0.338249  0.513473\n1  0.120203  0.180629  0.699168\n2  0.418679  0.236808  0.344513\n3  0.172690  0.350841  0.476468\n4  0.100125  0.168938  0.730937\n\n\n\nCodedses = pd.DataFrame({\n    'ses': ['low', 'middle', 'high'],\n    'write': [multino_data['write'].mean()] * 3\n})\nmultino_fit.predict(dses)\n\n          0         1         2\n0  0.439684  0.358193  0.202123\n1  0.477749  0.228334  0.293917\n2  0.700902  0.178493  0.120605\n\n\nWe can also use sklearn package.\n\nCodefrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Separate features (X) and target (y)\nX = multino_data[['ses', 'write']]  # Independent variables\ny = multino_data['prog_int']          # Dependent variable (categorical target)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(drop='first'), ['ses']),  # One-hot encode 'ses'\n        ('num', 'passthrough', ['write'])              # Leave 'write' as is\n    ]\n)\n# Create a multinomial logistic regression model\nmodel = Pipeline([\n    ('preprocessor', preprocessor),  # Preprocessing step\n    ('classifier', LogisticRegression(multi_class='multinomial', penalty=None,\n                                      solver='lbfgs', max_iter=500))\n])\n\nmodel.fit(X, y)\n\n\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(drop='first'),\n                                                  ['ses']),\n                                                 ('num', 'passthrough',\n                                                  ['write'])])),\n                ('classifier',\n                 LogisticRegression(max_iter=500, multi_class='multinomial',\n                                    penalty=None))])\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\nÂ Â Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(drop='first'),\n                                                  ['ses']),\n                                                 ('num', 'passthrough',\n                                                  ['write'])])),\n                ('classifier',\n                 LogisticRegression(max_iter=500, multi_class='multinomial',\n                                    penalty=None))]) \n\n\n\nÂ preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('cat', OneHotEncoder(drop='first'), ['ses']),\n                                ('num', 'passthrough', ['write'])]) \n\n\n\ncat['ses'] \n\nÂ OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(drop='first') \n\n\n\nnum['write'] \n\npassthroughpassthrough \n\n\n\n\nÂ LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=500, multi_class='multinomial', penalty=None) \n\n\n\n\nCodemodel.named_steps.classifier.coef_\n\narray([[-0.71516627, -0.63453974,  0.05717724],\n       [ 0.4476491 , -0.0049987 , -0.00075151],\n       [ 0.26751717,  0.63953844, -0.05642573]])\n\nCodemodel.named_steps.classifier.intercept_\n\narray([-1.97496981, -0.28559419,  2.260564  ])\n\n\n\nCodedses = pd.DataFrame({\n    'ses': ['low', 'middle', 'high'],\n    'write': [multino_data['write'].mean()] * 3\n})\n\npd.DataFrame(model.predict_proba(dses),\n             columns=model.named_steps.classifier.classes_)\n\n          0         1         2\n0  0.439686  0.358190  0.202124\n1  0.477748  0.228334  0.293917\n2  0.700903  0.178494  0.120603"
  },
  {
    "objectID": "present-description-2.html",
    "href": "present-description-2.html",
    "title": "Midterm Presentation II - Classification",
    "section": "",
    "text": "Proposal. Please send me a one-page PDF describing what you are going to do for your project (no word limit) with your project title by Tuesday, 4/8 11:59 PM.\nPresentation. You will be presenting your project on Tuesday, 4/15 in class.\nMaterials. Please share your entire work (slides, code, data, etc) by Friday, 4/18 11:59 PM.",
    "crumbs": [
      "Midterm Project II",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description-2.html#proposal",
    "href": "present-description-2.html#proposal",
    "title": "Midterm Presentation II - Classification",
    "section": "Proposal",
    "text": "Proposal\n\nEach one of you loses 5 points of your project grade if you donât meet the requirement or miss the deadline.\nYour proposal (in PDF) should include three parts:\n\nProject title\nThe goal of your project. For example, what is the research question youâd like to answer? What machine learning method/model/algorithm youâd like to introduce? What data youâd like to use for analysis or demonstration? etc.\n\nAlthough it is risky, you can change your project topic after you submit your proposal if you decide to do something else.",
    "crumbs": [
      "Midterm Project II",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description-2.html#presentation",
    "href": "present-description-2.html#presentation",
    "title": "Midterm Presentation II - Classification",
    "section": "Presentation",
    "text": "Presentation\n\nEach group presentation should be between 10 and 11 minute long, followed by 1 to 2 minute Q&A. If your presentation is too short or too long, every one of you loses 5 points of your project grade.\nEvery group member has to present some part of the group work. The one who does not present receives no point.\nQuestions are REQUIRED during Q&A. Each group is required to ask as least one question. More questions are welcome. If you, as a group, donât ask a question when you should, every one of you loses 5 points of your project grade.\n\n\n\n\n\nTeam Presenting\nTeam Asking Questions\n\n\n\n\nTeam 1\nTeam 6\n\n\nTeam 2\nTeam 1\n\n\nTeam 3\nTeam 2\n\n\nTeam 4\nTeam 3\n\n\nTeam 5\nTeam 4\n\n\nTeam 6\nTeam 5",
    "crumbs": [
      "Midterm Project II",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description-2.html#materials",
    "href": "present-description-2.html#materials",
    "title": "Midterm Presentation II - Classification",
    "section": "Materials",
    "text": "Materials\n\nEach one of you loses 5 points of your project grade if you donât meet the requirement or miss the deadline.\nYou need to share your entire work, including slides, code, and data if applicable.\nYour code should be able to reproduce all the numerical results, outputs, tables, and figures shown in the slides, including the source of the raw data (where you find and load the data) if the project is about data analysis.",
    "crumbs": [
      "Midterm Project II",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description-2.html#data-analytics",
    "href": "present-description-2.html#data-analytics",
    "title": "Midterm Presentation II - Classification",
    "section": "Data Analytics",
    "text": "Data Analytics\nFor your DA project, you need to\n\nDescribe the selected data set.\nExplain and show why the chosen model(s) is appropriate for answering your research questions and better than others.\nInterpret your analysis result.\n\nBelow are some data repositories you can start with, but you are encouraged to explore more and find your favorite one.\n\nTidyTuesday\nKaggle\nAwesome Public Datasets\nHarvard Dataverse\nUCI Machine Learning Repository\nFiveThirtyEight",
    "crumbs": [
      "Midterm Project II",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description-2.html#modelalgorithm",
    "href": "present-description-2.html#modelalgorithm",
    "title": "Midterm Presentation II - Classification",
    "section": "Model/Algorithm",
    "text": "Model/Algorithm\nFor your MA project, you need to\n\nDescribe the intuition and idea of the method. What are the pros and cons of the method?\nProvide the mathematical expression of the model/algorithm. Explain the model and its properties, and how we do supervised learning with the model/algorithm.\nCompare the chosen method with other methods learned in class. Determine which method performs better under what conditions.\nDemo how to implement the method using a programming language for supervised learning.",
    "crumbs": [
      "Midterm Project II",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description-2.html#group-performance-evaluation",
    "href": "present-description-2.html#group-performance-evaluation",
    "title": "Midterm Presentation II - Classification",
    "section": "Group Performance Evaluation",
    "text": "Group Performance Evaluation\n\nYou will need to evaluate all group projects except the one you work on.\nYou evaluate group performance based on the rubric attached. Four evaluation criteria are considered:\n\nProject Content and Organization (8 pts)\nPresentation Material (Slides) Quality (4 pts)\nOral Presentation Skill and Delivery (4 pts)\nInteractions and Q&A (4 pts)\n\nThe total points of a project presentation is 20 points.\nEvaluation sheets will be provided on the presentation day.\n\nHow do you get the full points for each category? Check the rubric below. \n\nContent and Organization (DA)\n\nBeautiful visualization helps find out relationship of variables and specification of models\nAll questions are answered accurately by the models\nDiscuss how and why the models are chosen\nApply sophisticated models and detailed analysis\nAll ideas are presented in logical order\n\nContent and Organization (MA)\n\nExplain the method clearly and accuratly\nShow the pros and cons of the method, and compare with the methods learned in class.\nShow how the method can be implemednted for supervised learning.\nAll ideas are presented in logical order\n\nPresentation Material Quality\n\nPresentation material show code and output beautifully\nPresentation material clearly aid the speaker in telling a coherent story\nAll tables and graphics are informative and related to the topic and make it easier to understand\nAttractive design, layout, and neatness.\n\nOral Presentation Skill\n\nGood volume and energy\nProper pace and diction\nAvoidance of distracting gestures\n\nInteractions and Q&A\n\nGood eye contact with audience\nExcellent listening skills\nAnswers audience questions with authority and accuracy\n\nAfter you evaluate all group project presentations, you rank them from 1st to last based on their earned points.\nNo two groups receive the same ranking. If you give two or more groups some points, you still need to give them a different ranking, deciding which team deserves a higher ranking according to your preference.",
    "crumbs": [
      "Midterm Project II",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "hw/hw-1.html#homework-problems",
    "href": "hw/hw-1.html#homework-problems",
    "title": "Homework 1 - Bias-Variance Tradeoff and Linear Regression",
    "section": "Homework Problems",
    "text": "Homework Problems\n\nISL Sec. 2.4: 1\nISL Sec. 2.4: 3\nISL Sec. 2.4: 5\nISL Sec. 3.7: 4\nISL Sec. 3.7: 6\nISL Sec. 3.7: 14\nSimulation of simple linear regression. Consider the model \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i, ~~ \\epsilon_i \\stackrel{iid} \\sim N(0, \\sigma^2), ~~ i = 1, \\dots, n,\\) or equivalently, \\(p(y_i \\mid x, \\beta_0, \\beta_1) = N\\left(\\beta_0 + \\beta_1x_i, \\sigma^2 \\right).\\) Set \\(\\beta_0 = 2, \\beta_1 = -1.5, \\sigma = 1, n = 100, x_i = i.\\) Generate 1000 simulated data sets and fit the model to each data set.  The sampling distribution of the least-squares estimator \\(b_1\\) is \\[b_1 \\sim N\\left(\\beta_1, \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\right).\\]\n\nFind the average of the 1000 estimates \\(b_1\\). Is it close to its true expected value?\nFind the variance of 1000 estimates \\(b_1\\). Is it close to its true value?\nDraw the histogram of \\(b_1\\). Comment on your graph.\nObtain 1000 95% confidence interval for \\(\\beta_1\\). What is your pertentage of coverage for \\(\\beta_1\\)? Comment your result.\n\nSimulation of bias-variance tradeoff. Let \\(f(x) = x ^ 2\\) be the true regression function. Simulate the data using \\(y_i = f(x_i) + \\epsilon_i, i = 1, \\dots, 100\\), where \\(x_i = 0.01i\\) and \\(\\epsilon_i \\stackrel{iid} \\sim N(0, 0.3^2).\\)\n\nGenerate 250 training data sets.\nFor each data set, fit the following three models:\n\n\nModel 1: \\(y = \\beta_0+\\beta_1x+\\epsilon\\)\nModel 2: \\(y = \\beta_0+\\beta_1x+ \\beta_2x^2 + \\epsilon\\)\nModel 3: \\(y = \\beta_0+\\beta_1x+ \\beta_2x^2 + \\cdots + \\beta_9x^9 + \\epsilon\\)\n\n\nCalculate empirical MSE of \\(\\hat{f}\\), bias of \\(\\hat{f}\\) and variance of \\(\\hat{f}\\). Then show that \\[\\text{MSE}_{\\hat{f}} \\approx \\text{Bias}^2(\\hat{f}) + \\mathrm{Var}(\\hat{f}).\\] Specifically, for each value of \\(x_i\\), \\(i = 1, \\dots, 100\\),\n\n\\[\\text{MSE}_{\\hat{f}} =  \\frac{1}{250}\\sum_{k=1}^{250} \\left(\\hat{f}_k(x_i) - f(x_i) \\right) ^2,\\] \\[\\text{Bias}(\\hat{f}) = \\overline{\\hat{f}}(x_i) - f(x_i),\\] where \\(\\overline{\\hat{f}}(x_i) = \\frac{1}{250}\\sum_{k = 1}^{250}\\hat{f}_k(x_i)\\) is the sample mean of \\(\\hat{f}(x_i)\\) that approximates \\(\\mathrm{E}_{\\hat{f}}\\left(\\hat{f}(x_i)\\right).\\) \\[ \\mathrm{Var}(\\hat{f}) =  \\frac{1}{250}\\sum_{k=1}^{250} \\left(\\hat{f}_k(x_i) - \\overline{\\hat{f}}(x_i) \\right) ^2.\\] [Note:] If you calculate the variance using the built-in function such as var() in R, the identity holds only approximately because of the \\(250 - 1\\) term in the denominator in the sample variance formula. If instead \\(250\\) is used in the denominator, the identity holds exactly.\n\nFor each model, plot first ten estimated \\(f\\), \\(\\hat{f}_{1}(x), \\dots, \\hat{f}_{10}(x)\\), and the average of \\(\\hat{f}\\), \\(\\frac{1}{250}\\sum_{k=1}^{250}\\hat{f}_{k}(x)\\) in one figure, as FigureÂ 1 below. Whatâs your finding?\nGenerate one more data set and use it as the test data. Calculate the overall training MSE (for training \\(y\\)) and overall test MSE (for test \\(y\\)) for each model. \\[MSE_\\texttt{Tr} =  \\frac{1}{250} \\sum_{k = 1}^{250} \\frac{1}{100} \\sum_{i=1}^{100} \\left(\\hat{f}_{k}(x_i) - y_{i}^k\\right)^2\\] \\[MSE_\\texttt{Te} =  \\frac{1}{250} \\sum_{k = 1}^{250} \\frac{1}{100} \\sum_{i=1}^{100} \\left(\\hat{f}_{k}(x_i) - y_{i}^{\\texttt{Test}}\\right)^2\\]\n\n\n\n\n\n\n\n\n\n\nFigureÂ 1: Trained \\(f\\)\n\n\n\n\n\n\n\n(MSSC PhD) Stochastic gradient descent. Write your own stochastic gradient descent algorithm for linear regression. Implement it and compare with the built-in function such as optim() of R or scipy.optimize() of python to make sure that it converges correctly."
  },
  {
    "objectID": "hw/hw-3.html#homework-questions",
    "href": "hw/hw-3.html#homework-questions",
    "title": "Homework 3 - Bayesian Statistics, Logistic Regression, Generative Models, and K-Nearest Neighbors",
    "section": "Homework Questions",
    "text": "Homework Questions\n\nRequired\n\nISL Sec. 4.8: 2\nISL Sec. 4.8: 3\nISL Sec. 4.8: 5\nISL Sec. 4.8: 13\nMNIST Handwritten Digits Image\n\nLoad the prepared MNIST data mnist.csv. Print some images.\nUse the first 1000 observations as the training data and the second half as the test data.\nTraining with KNN and predicting on the test data with the best \\(K\\) selected from the training.\n\nCalculate the test error rate.\nGenerate the confusion matrix.\n\nTraining with multinomial logistic regression and predicting on the test data.\n\nCalculate the test error rate.\nGenerate the confusion matrix.\n\n\n(MSSC PhD) KNN Curse of Dimensionality\n\nGenerate Generate the covariates \\(x_1, x_2, \\dots, x_5\\) of \\(n = 1000\\) training data from independent standard normal distribution. Then, generate \\(Y\\) from \\[Y = X_1 + 0.5 X_2 - X_3 + \\epsilon,\\] where \\(\\epsilon \\sim N(0, 1).\\)\nUse the first 500 observations as the training data and the rest as the test data. Fit KNN regression, and report the test MSE of \\(y\\) with the optimal \\(K\\).\nAdd additional 95 noisy predictors as follows.\n\nCase 1: \\(x_6, x_7, \\dots, x_{100} \\overset{\\mathrm{iid}}{\\sim} N(0, 1)\\)\nCase 2: \\(XA\\) where \\(X_{1000 \\times 5} = [x_1 \\cdots x_5]\\) and \\(A_{5 \\times 95}\\) having entries from iid uniform(0, 1).\n\nFit KNN regression in both cases (with the total of 100 covariates) and select the best \\(K\\) value.\nFor both cases, what is the best K and the best mean squared error for prediction? Discuss the effect of adding 95 (unnecessary) covariates.\n\n\n\n\nDo one of the followings\n\n\nWatch the talk All About that Bayes: Probability, Statistics, and the Quest to Quantify Uncertainty by Dr.Â Kristin Lennox. In 250 words, summarize your thoughts and what you learned from the talk.\n\n\n\nIn 250 words, summarize your thoughts and what you learned from the deep learning workshop."
  },
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8",
    "section": "",
    "text": "Important",
    "crumbs": [
      "Weekly Materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week-8.html#participate",
    "href": "weeks/week-8.html#participate",
    "title": "Week 8",
    "section": "Participate",
    "text": "Participate\nð¥ï¸ Slides - Generative Models",
    "crumbs": [
      "Weekly Materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week-8.html#reading-and-resources",
    "href": "weeks/week-8.html#reading-and-resources",
    "title": "Week 8",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nð ISL Ch 3.5, 4.5",
    "crumbs": [
      "Weekly Materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week-8.html#exercise",
    "href": "weeks/week-8.html#exercise",
    "title": "Week 8",
    "section": "Exercise",
    "text": "Exercise\n\n\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly Materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week-13.html",
    "href": "weeks/week-13.html",
    "title": "Week 13",
    "section": "",
    "text": "Important",
    "crumbs": [
      "Weekly Materials",
      "Week 13"
    ]
  },
  {
    "objectID": "weeks/week-13.html#participate",
    "href": "weeks/week-13.html#participate",
    "title": "Week 13",
    "section": "Participate",
    "text": "Participate\nð¥ï¸ Slides - Tree-based Methods",
    "crumbs": [
      "Weekly Materials",
      "Week 13"
    ]
  },
  {
    "objectID": "weeks/week-13.html#reading",
    "href": "weeks/week-13.html#reading",
    "title": "Week 13",
    "section": "Reading",
    "text": "Reading\nð ISL Ch 8",
    "crumbs": [
      "Weekly Materials",
      "Week 13"
    ]
  },
  {
    "objectID": "weeks/week-13.html#exercise",
    "href": "weeks/week-13.html#exercise",
    "title": "Week 13",
    "section": "Exercise",
    "text": "Exercise\n\nr fontawesome::fa(\"table\") iris\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly Materials",
      "Week 13"
    ]
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "Important",
    "crumbs": [
      "Weekly Materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-10.html#participate",
    "href": "weeks/week-10.html#participate",
    "title": "Week 10",
    "section": "Participate",
    "text": "Participate\nð¥ï¸ Slides - K-Nearest Neighbors\nr fontawesome::fa(\"table\") Mixture Simulation Data\nr fontawesome::fa(\"table\") ZIP code Training and Test data",
    "crumbs": [
      "Weekly Materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-10.html#reading-and-resources",
    "href": "weeks/week-10.html#reading-and-resources",
    "title": "Week 10",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nð PMLI Ch 16.1",
    "crumbs": [
      "Weekly Materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-10.html#exercise",
    "href": "weeks/week-10.html#exercise",
    "title": "Week 10",
    "section": "Exercise",
    "text": "Exercise\n\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly Materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\nHomework 1 due Friday, Feb 14, 11:59 PM",
    "crumbs": [
      "Weekly Materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#participate",
    "href": "weeks/week-3.html#participate",
    "title": "Week 3",
    "section": "Participate",
    "text": "Participate\nð¥ï¸ Slides - Ridge Regression and Cross Validation\nr fontawesome::fa(\"table\") Data Set - mtcars.csv",
    "crumbs": [
      "Weekly Materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#reading-and-resources",
    "href": "weeks/week-3.html#reading-and-resources",
    "title": "Week 3",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nð ISL Ch 5.1, 6.2.1\nð MML Ch 8.2.3 - 8.2.4",
    "crumbs": [
      "Weekly Materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#exercise",
    "href": "weeks/week-3.html#exercise",
    "title": "Week 3",
    "section": "Exercise",
    "text": "Exercise\n\n\n\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly Materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Important\n\n\n\nHomework 1 due Friday, Feb 9, 11:59 PM",
    "crumbs": [
      "Weekly Materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#participate",
    "href": "weeks/week-4.html#participate",
    "title": "Week 4",
    "section": "Participate",
    "text": "Participate\nð¥ï¸ Slides - Lasso\n\nr fontawesome::fa(\"table\") Data Set - Prostate.csv",
    "crumbs": [
      "Weekly Materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#reading-and-resources",
    "href": "weeks/week-4.html#reading-and-resources",
    "title": "Week 4",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nð ISL Ch 6.1, 6.2",
    "crumbs": [
      "Weekly Materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#perform",
    "href": "weeks/week-4.html#perform",
    "title": "Week 4",
    "section": "Perform",
    "text": "Perform\n\n\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly Materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Week 7",
    "section": "",
    "text": "Important\n\n\n\nHomework 2 due Friday, Feb 28, 11:59 PM.",
    "crumbs": [
      "Weekly Materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week-7.html#participate",
    "href": "weeks/week-7.html#participate",
    "title": "Week 7",
    "section": "Participate",
    "text": "Participate\nð¥ï¸ Slides - Logistic Regression\nr fontawesome::fa(\"table\") ISLR2::Default\nr fontawesome::fa(\"table\") multinomial data hsbdemo.csv",
    "crumbs": [
      "Weekly Materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week-7.html#reading-and-resources",
    "href": "weeks/week-7.html#reading-and-resources",
    "title": "Week 7",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nð ISL Ch 4.1 - 4.3, 4.6",
    "crumbs": [
      "Weekly Materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week-7.html#exercise",
    "href": "weeks/week-7.html#exercise",
    "title": "Week 7",
    "section": "Exercise",
    "text": "Exercise\n\n\n\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly Materials",
      "Week 7"
    ]
  },
  {
    "objectID": "present-work.html",
    "href": "present-work.html",
    "title": "Midterm Project I Proposal and Presentation",
    "section": "",
    "text": "The presentation order is determined by the time you sent me your member info (c(\"John\", \"Jeremy\", \"Praful\") is the first to let me know, followed by c(\"Sajjad\", \"Tanjina\", \"Dewan\"), etc) and random sampling as follows.\n\nteam_lst &lt;- list(c(\"John\", \"Jeremy\", \"Praful\"), \n                 c(\"Sajjad\", \"Tanjina\", \"Dewan\"),\n                 c(\"Ethan\", \"Navid\", \"Sylvester\"), \n                 c(\"Violet\", \"Vanessa\", \"Michele\"), \n                 c(\"Rakesh\", \"Daniel\", \"Jeremy\"), \n                 c(\"Sai\", \"Rohith\", \"Shristi\"))\nset.seed(6250)\nteam_lst[sample(1:6, 6)]\n\n[[1]]\n[1] \"Sajjad\"  \"Tanjina\" \"Dewan\"  \n\n[[2]]\n[1] \"John\"   \"Jeremy\" \"Praful\"\n\n[[3]]\n[1] \"Violet\"  \"Vanessa\" \"Michele\"\n\n[[4]]\n[1] \"Rakesh\" \"Daniel\" \"Jeremy\"\n\n[[5]]\n[1] \"Sai\"     \"Rohith\"  \"Shristi\"\n\n[[6]]\n[1] \"Ethan\"     \"Navid\"     \"Sylvester\"",
    "crumbs": [
      "Midterm Project I",
      "Topics and Works"
    ]
  },
  {
    "objectID": "present-work.html#presentation-order",
    "href": "present-work.html#presentation-order",
    "title": "Midterm Project I Proposal and Presentation",
    "section": "",
    "text": "The presentation order is determined by the time you sent me your member info (c(\"John\", \"Jeremy\", \"Praful\") is the first to let me know, followed by c(\"Sajjad\", \"Tanjina\", \"Dewan\"), etc) and random sampling as follows.\n\nteam_lst &lt;- list(c(\"John\", \"Jeremy\", \"Praful\"), \n                 c(\"Sajjad\", \"Tanjina\", \"Dewan\"),\n                 c(\"Ethan\", \"Navid\", \"Sylvester\"), \n                 c(\"Violet\", \"Vanessa\", \"Michele\"), \n                 c(\"Rakesh\", \"Daniel\", \"Jeremy\"), \n                 c(\"Sai\", \"Rohith\", \"Shristi\"))\nset.seed(6250)\nteam_lst[sample(1:6, 6)]\n\n[[1]]\n[1] \"Sajjad\"  \"Tanjina\" \"Dewan\"  \n\n[[2]]\n[1] \"John\"   \"Jeremy\" \"Praful\"\n\n[[3]]\n[1] \"Violet\"  \"Vanessa\" \"Michele\"\n\n[[4]]\n[1] \"Rakesh\" \"Daniel\" \"Jeremy\"\n\n[[5]]\n[1] \"Sai\"     \"Rohith\"  \"Shristi\"\n\n[[6]]\n[1] \"Ethan\"     \"Navid\"     \"Sylvester\"",
    "crumbs": [
      "Midterm Project I",
      "Topics and Works"
    ]
  },
  {
    "objectID": "present-work.html#project-materials",
    "href": "present-work.html#project-materials",
    "title": "Midterm Project I Proposal and Presentation",
    "section": "Project Materials",
    "text": "Project Materials\n\nGroup 1 (Sajjad, Tanjina, Dewan) Predicting Life Expectancy: proposal\nGroup 2 (John, Jeremy, Praful) Poisson Regression: proposal\nGroup 3 (Violet, Vanessa, Michele) Is Ridge Regression or LASSO a better model to predict the price of houses in Ames, Iowa: proposal\nGroup 4 (Rakesh, Daniel, Jeremy) Predicting Abalone Age Using Regression Models: proposal\nGroup 5 (Sai, Rohith, Shristi) Analyzing the Impact of Housing Features on Sale Prices: A Regression-Based Study on the Ames Housing Dataset : proposal\nGroup 6 (Ethan, Navid, Sylvester) Predicting Car Sticker Price Regression: proposal",
    "crumbs": [
      "Midterm Project I",
      "Topics and Works"
    ]
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (âPublic Licenseâ). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 â Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapterâs License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 â Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor â Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor â Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapterâs License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 â License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapterâs License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapterâs License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapterâs License You apply.\n\n\n\nSection 4 â Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 â Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 â Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 â Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 â Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the âLicensor.â The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark âCreative Commonsâ or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "present-description.html",
    "href": "present-description.html",
    "title": "Midterm Presentation I - Regression",
    "section": "",
    "text": "Team up! You will be working as a group of 3. One of you, please email me your team member list by Friday, 2/21 11:59 PM\nProposal. Please send me a one-page PDF describing what you are going to do for your project (no word limit) with your project title by Friday, 2/28 11:59 PM.\nPresentation. You will be presenting your project on Thursday, 3/6 in class.\nMaterials. Please share your entire work (slides, code, data, etc) by Friday, 3/7 11:59 PM.",
    "crumbs": [
      "Midterm Project I",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description.html#team-up",
    "href": "present-description.html#team-up",
    "title": "Midterm Presentation I - Regression",
    "section": "Team up!",
    "text": "Team up!\n\nEach one of you loses 5 points of your project grade if you donât meet the requirement or miss the deadline.\nYou will be randomly assigned to a group if you do not belong to any group before the deadline.",
    "crumbs": [
      "Midterm Project I",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description.html#proposal",
    "href": "present-description.html#proposal",
    "title": "Midterm Presentation I - Regression",
    "section": "Proposal",
    "text": "Proposal\n\nEach one of you loses 5 points of your project grade if you donât meet the requirement or miss the deadline.\nYour proposal (in PDF) should include three parts:\n\nProject title\nThe goal of your project. For example, what is the research question youâd like to answer? What machine learning method/model/algorithm youâd like to introduce? What data youâd like to use for analysis or demonstration? etc.\n\nAlthough it is risky, you can change your project topic after you submit your proposal if you decide to do something else.",
    "crumbs": [
      "Midterm Project I",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description.html#presentation",
    "href": "present-description.html#presentation",
    "title": "Midterm Presentation I - Regression",
    "section": "Presentation",
    "text": "Presentation\n\nEach group presentation should be between 10 and 11 minute long, followed by 1 to 2 minute Q&A. If your presentation is too short or too long, every one of you loses 5 points of your project grade.\nEvery group member has to present some part of the group work. The one who does not present receives no point.\nQuestions are encouraged during Q&A. Everyone is welcome to ask any questions about the projects.",
    "crumbs": [
      "Midterm Project I",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description.html#materials",
    "href": "present-description.html#materials",
    "title": "Midterm Presentation I - Regression",
    "section": "Materials",
    "text": "Materials\n\nEach one of you loses 5 points of your project grade if you donât meet the requirement or miss the deadline.\nYou need to share your entire work, including slides, code, and data if applicable.\nYour code should be able to reproduce all the numerical results, outputs, tables, and figures shown in the slides, including the source of the raw data (where you find and load the data) if the project is about data analysis.",
    "crumbs": [
      "Midterm Project I",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description.html#data-analytics",
    "href": "present-description.html#data-analytics",
    "title": "Midterm Presentation I - Regression",
    "section": "Data Analytics",
    "text": "Data Analytics\nFor your DA project, you need to\n\nDescribe the selected data set.\nExplain and show why the chosen model(s) is appropriate for answering your research questions and better than others.\nInterpret your analysis result.\n\nBelow are some data repositories you can start with, but you are encouraged to explore more and find your favorite one.\n\nTidyTuesday\nKaggle\nAwesome Public Datasets\nHarvard Dataverse\nUCI Machine Learning Repository\nFiveThirtyEight",
    "crumbs": [
      "Midterm Project I",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description.html#modelalgorithm",
    "href": "present-description.html#modelalgorithm",
    "title": "Midterm Presentation I - Regression",
    "section": "Model/Algorithm",
    "text": "Model/Algorithm\nFor your MA project, you need to\n\nDescribe the intuition and idea of the method. What are the pros and cons of the method?\nProvide the mathematical expression of the model/algorithm. Explain the model and its properties, and how we do supervised learning with the model/algorithm.\nCompare the chosen method with other methods learned in class. Determine which method performs better under what conditions.\nDemo how to implement the method using a programming language for supervised learning.",
    "crumbs": [
      "Midterm Project I",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "present-description.html#group-performance-evaluation",
    "href": "present-description.html#group-performance-evaluation",
    "title": "Midterm Presentation I - Regression",
    "section": "Group Performance Evaluation",
    "text": "Group Performance Evaluation\n\nYou will need to evaluate all group projects except the one you work on.\nYou evaluate group performance based on the rubric attached. Four evaluation criteria are considered:\n\nProject Content and Organization (8 pts)\nPresentation Material (Slides) Quality (4 pts)\nOral Presentation Skill and Delivery (4 pts)\nInteractions and Q&A (4 pts)\n\nThe total points of a project presentation is 20 points.\nEvaluation sheets will be provided on the presentation day.\n\nHow do you get the full points for each category? Check the rubric below. \n\nContent and Organization (DA)\n\nBeautiful visualization helps find out relationship of variables and specification of models\nAll questions are answered accurately by the models\nDiscuss how and why the models are chosen\nApply sophisticated models and detailed analysis\nAll ideas are presented in logical order\n\nContent and Organization (MA)\n\nExplain the method clearly and accuratly\nShow the pros and cons of the method, and compare with the methods learned in class.\nShow how the method can be implemednted for supervised learning.\nAll ideas are presented in logical order\n\nPresentation Material Quality\n\nPresentation material show code and output beautifully\nPresentation material clearly aid the speaker in telling a coherent story\nAll tables and graphics are informative and related to the topic and make it easier to understand\nAttractive design, layout, and neatness.\n\nOral Presentation Skill\n\nGood volume and energy\nProper pace and diction\nAvoidance of distracting gestures\n\nInteractions and Q&A\n\nGood eye contact with audience\nExcellent listening skills\nAnswers audience questions with authority and accuracy\n\nAfter you evaluate all group project presentations, you rank them from 1st to last based on their earned points.\nNo two groups receive the same ranking. If you give two or more groups some points, you still need to give them a different ranking, deciding which team deserves a higher ranking according to your preference.",
    "crumbs": [
      "Midterm Project I",
      "Guideline and Policy"
    ]
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Week 6",
    "section": "",
    "text": "Important",
    "crumbs": [
      "Weekly Materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#participate",
    "href": "weeks/week-6.html#participate",
    "title": "Week 6",
    "section": "Participate",
    "text": "Participate\nð¥ï¸ Slides - Bayesian Linear Regression\nr fontawesome::fa(\"table\") bayesrules::bikes",
    "crumbs": [
      "Weekly Materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#reading-and-resources",
    "href": "weeks/week-6.html#reading-and-resources",
    "title": "Week 6",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nð Bayes Rules! An Introduction to Applied Bayesian Modeling\nð Statistical Rethinking\nð A Studentâs Guide to Bayesian Statistics\nð Bayesian Data Analysis",
    "crumbs": [
      "Weekly Materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#exercise",
    "href": "weeks/week-6.html#exercise",
    "title": "Week 6",
    "section": "Exercise",
    "text": "Exercise\n\n\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly Materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-5.html",
    "href": "weeks/week-5.html",
    "title": "Week 5",
    "section": "",
    "text": "Important",
    "crumbs": [
      "Weekly Materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#participate",
    "href": "weeks/week-5.html#participate",
    "title": "Week 5",
    "section": "Participate",
    "text": "Participate\nð¥ï¸ Slides - Splines and Generalized Additive Models\nr fontawesome::fa(\"table\") Data set - birthrates",
    "crumbs": [
      "Weekly Materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#reading-and-resources",
    "href": "weeks/week-5.html#reading-and-resources",
    "title": "Week 5",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nð ISL Ch 7",
    "crumbs": [
      "Weekly Materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#exercise",
    "href": "weeks/week-5.html#exercise",
    "title": "Week 5",
    "section": "Exercise",
    "text": "Exercise\n\n\n\n\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly Materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "ð Read the syllabus.\nð Get your laptop and computing environment ready!\nð Refresh your probability/statistics and linear algebra.",
    "crumbs": [
      "Weekly Materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#prepare",
    "href": "weeks/week-1.html#prepare",
    "title": "Week 1",
    "section": "",
    "text": "ð Read the syllabus.\nð Get your laptop and computing environment ready!\nð Refresh your probability/statistics and linear algebra.",
    "crumbs": [
      "Weekly Materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#participate",
    "href": "weeks/week-1.html#participate",
    "title": "Week 1",
    "section": "Participate",
    "text": "Participate\nð¥ï¸ Slides - Welcome to MSSC 6250\nð¥ï¸ Slides - Overview of Machine Learning\nð¥ï¸ Slides - Bias-variance Tradeoff",
    "crumbs": [
      "Weekly Materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#reading-and-resources",
    "href": "weeks/week-1.html#reading-and-resources",
    "title": "Week 1",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nð ISL Ch 1, 2\nð Dr.Â Yu MSSC 5780 Probability and Statistics Review\nð Dr.Â Yu MSSC 5780 Linear Algebra Review\nð Check any math you need from MML Ch 2 - 6",
    "crumbs": [
      "Weekly Materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#perform",
    "href": "weeks/week-1.html#perform",
    "title": "Week 1",
    "section": "Perform",
    "text": "Perform\nð Decide whether this is the right course for you by the drop deadline 1/21.\n\n\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly Materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Important\n\n\n\nHomework 1 released.",
    "crumbs": [
      "Weekly Materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#participate",
    "href": "weeks/week-2.html#participate",
    "title": "Week 2",
    "section": "Participate",
    "text": "Participate\nð¥ï¸ Slides - Linear Regression\nr fontawesome::fa(\"table\") Data Set - Advertising.csv",
    "crumbs": [
      "Weekly Materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#reading-and-resources",
    "href": "weeks/week-2.html#reading-and-resources",
    "title": "Week 2",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nð ISL Ch 3.1 - 3.4\nð Dr.Â Yu MSSC 5780 Regression Analysis Week 1 to 6\nð MML Ch 7.1, 8.1, 8.2.1, 8.2.2",
    "crumbs": [
      "Weekly Materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#perform",
    "href": "weeks/week-2.html#perform",
    "title": "Week 2",
    "section": "Perform",
    "text": "Perform\nð Homework 1\n\n\n\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly Materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-11.html",
    "href": "weeks/week-11.html",
    "title": "Week 11",
    "section": "",
    "text": "Important",
    "crumbs": [
      "Weekly Materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week-11.html#participate",
    "href": "weeks/week-11.html#participate",
    "title": "Week 11",
    "section": "Participate",
    "text": "Participate\nð¥ï¸ Slides - Gaussian Processes",
    "crumbs": [
      "Weekly Materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week-11.html#reading",
    "href": "weeks/week-11.html#reading",
    "title": "Week 11",
    "section": "Reading",
    "text": "Reading\nð Gaussian Processes for Machine Learning (GPML) Ch 2\nð Surrogates Ch 5\nð PMLI Ch 17.2",
    "crumbs": [
      "Weekly Materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week-11.html#exercise",
    "href": "weeks/week-11.html#exercise",
    "title": "Week 11",
    "section": "Exercise",
    "text": "Exercise\n\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly Materials",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 12",
    "section": "",
    "text": "Important",
    "crumbs": [
      "Weekly Materials",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week-12.html#participate",
    "href": "weeks/week-12.html#participate",
    "title": "Week 12",
    "section": "Participate",
    "text": "Participate\nð¥ï¸ Slides - Support Vector Machines",
    "crumbs": [
      "Weekly Materials",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week-12.html#reading",
    "href": "weeks/week-12.html#reading",
    "title": "Week 12",
    "section": "Reading",
    "text": "Reading\nð ISL Ch 9",
    "crumbs": [
      "Weekly Materials",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week-12.html#exercise",
    "href": "weeks/week-12.html#exercise",
    "title": "Week 12",
    "section": "Exercise",
    "text": "Exercise\n\n\n\nBack to course schedule â",
    "crumbs": [
      "Weekly Materials",
      "Week 12"
    ]
  },
  {
    "objectID": "hw/hw-4.html#homework-questions",
    "href": "hw/hw-4.html#homework-questions",
    "title": "Homework 4 - Support Vector Machines, Tree Methods, Unsupervised Learning",
    "section": "Homework Questions",
    "text": "Homework Questions\n\nISL Sec. 8.4: 12 (Donât do BART)\nISL Sec. 9.7: 3\nISL Sec. 9.7: 5\nISL Sec. 12.6: 10"
  },
  {
    "objectID": "hw/hw-2.html#homework-questions",
    "href": "hw/hw-2.html#homework-questions",
    "title": "Homework 2 - Ridge, Lasso, and Splines",
    "section": "Homework Questions",
    "text": "Homework Questions\n\n\nISL Sec. 6.6: 4\n\n\n\nISL Sec. 6.6: 9 (a)-(d)\nISL Sec. 7.9: 9\n(MSSC PhD) ISL Sec. 7.9: 11\n[Special Case for Ridge and Lasso] Suppose for a multiple linear regression problem with \\(n = p\\), \\({\\bf X} = {\\bf I}\\) and no intercept, i.e., \\(y_i = \\beta_i + \\epsilon_i, ~~ \\epsilon_i \\stackrel{iid} N(0, \\sigma^2)\\). Show that\n\nShow that the least squares problem can be simplified to finding \\(\\beta_1,     \\dots, \\beta_p\\) that minimize \\(\\sum_{j=1}^p\\left(y_j - \\beta_j\\right)^2\\). What is least squares estimator \\(b_j\\)?\nShow that the ridge estimator is \\(\\hat{\\beta}_j^r = \\frac{y_j}{1+\\lambda} = \\underset{\\beta_j}{\\arg\\min} \\sum_{j=1}^p\\left(y_j - \\beta_j\\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\\).\n\n\n\nShow that the lasso solution of \\(\\sum_{j=1}^p\\left(y_j - \\beta_j\\right)^2 + \\lambda \\sum_{j=1}^p | \\beta_j |\\) is \\[\\hat{\\beta}_j^l =\n  \\begin{cases}\ny_j - \\lambda/2       & \\quad \\text{if } y_j &gt; \\lambda/2\\\\\ny_j + \\lambda/2       & \\quad \\text{if } y_j &lt; -\\lambda/2 \\\\\n0  & \\quad \\text{if } |y_j| \\le \\lambda/2\n  \\end{cases}\\]\nDescribe the ridge and lasso shrinkage behavior.\n\n[Lasso with Correlated Variables]\nConsider the linear regression\n\\[ \\mathbf{y}= \\mathbf{X}\\boldsymbol \\beta+ \\boldsymbol \\epsilon\\]\nwhere \\(\\boldsymbol \\beta= (\\beta_1, \\beta_2, \\ldots, \\beta_{20})'\\) with \\(\\beta_1 = \\beta_{2} = \\beta_{3} = 0.5\\) and all other \\(\\beta_j = 0, j = 4, \\dots, 20\\). No intercept \\(\\beta_0\\). The input vector \\(\\mathbf{x}= (x_1, x_2, \\dots, x_{20})'\\) follows a multivariate Gaussian distribution\n\\[\\mathbf{x}\\sim N\\left(\\mathbf{0}, \\Sigma_{20 \\times 20}\\right)\\]\nIn \\(\\Sigma\\), all diagonal elements are 1, and all off-diagonal elements are \\(\\rho\\) which measures the correlation between any two predictors.\n\nGenerate training data of size 400 and test data of size 100 independently from the above model with \\(\\rho = 0.1\\) and \\(\\epsilon_i \\stackrel{iid} \\sim N(0, 1)\\).\nFit a Lasso model on the training data with 10-fold cross-validation (CV).\nCompute test MSE with the optimal \\(\\lambda\\) selected by CV in (b). Does Lasso select the correct variables?\nRepeat (a)-(c) 100 times. That is, generate 100 different training and test data sets. For each run, record the test MSE and whether or not the true model is correctly selected. Then compute the average test MSE and the proportion of runs where the correct model was selected.\nRedo (a)-(d) with \\(\\rho = 0.6\\). Compare the two average test MSEs and the proportions. Comment the result."
  },
  {
    "objectID": "code/10-generative-code.html",
    "href": "code/10-generative-code.html",
    "title": "10 - Generative Models for Classification Code Demo",
    "section": "",
    "text": "Codelibrary(ISLR2)\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:ISLR2':\n\n    Boston\n\nCodelibrary(e1071)\ndata(\"Default\")\n\n\n\nCodelda_fit &lt;- MASS::lda(default ~ balance, data = Default)\nlda_pred &lt;- predict(lda_fit, data = Default)\ntable(lda_pred$class, Default$default, dnn = c(\"Predicted\", \"Actual\"))\n\n         Actual\nPredicted   No  Yes\n      No  9643  257\n      Yes   24   76\n\n\n\nCodeqda_fit &lt;- MASS::qda(default ~ balance, data = Default)\nqda_pred &lt;- predict(qda_fit, data = Default)\ntable(qda_pred$class, Default$default, dnn = c(\"Predicted\", \"Actual\"))\n\n         Actual\nPredicted   No  Yes\n      No  9639  246\n      Yes   28   87\n\n\n\nCodenb_fit &lt;- e1071::naiveBayes(default ~ balance, data = Default)\nnb_pred &lt;- predict(nb_fit, Default)\n(nb_conf &lt;- table(nb_pred, Default$default))\n\n       \nnb_pred   No  Yes\n    No  9639  246\n    Yes   28   87"
  },
  {
    "objectID": "code/10-generative-code.html#r-implementation",
    "href": "code/10-generative-code.html#r-implementation",
    "title": "10 - Generative Models for Classification Code Demo",
    "section": "",
    "text": "Codelibrary(ISLR2)\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:ISLR2':\n\n    Boston\n\nCodelibrary(e1071)\ndata(\"Default\")\n\n\n\nCodelda_fit &lt;- MASS::lda(default ~ balance, data = Default)\nlda_pred &lt;- predict(lda_fit, data = Default)\ntable(lda_pred$class, Default$default, dnn = c(\"Predicted\", \"Actual\"))\n\n         Actual\nPredicted   No  Yes\n      No  9643  257\n      Yes   24   76\n\n\n\nCodeqda_fit &lt;- MASS::qda(default ~ balance, data = Default)\nqda_pred &lt;- predict(qda_fit, data = Default)\ntable(qda_pred$class, Default$default, dnn = c(\"Predicted\", \"Actual\"))\n\n         Actual\nPredicted   No  Yes\n      No  9639  246\n      Yes   28   87\n\n\n\nCodenb_fit &lt;- e1071::naiveBayes(default ~ balance, data = Default)\nnb_pred &lt;- predict(nb_fit, Default)\n(nb_conf &lt;- table(nb_pred, Default$default))\n\n       \nnb_pred   No  Yes\n    No  9639  246\n    Yes   28   87"
  },
  {
    "objectID": "code/10-generative-code.html#python-implementation",
    "href": "code/10-generative-code.html#python-implementation",
    "title": "10 - Generative Models for Classification Code Demo",
    "section": "\n2 Python implementation",
    "text": "2 Python implementation\n\nCodeimport pandas as pd\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix\n\n\n\nCodeDefault = pd.read_csv(\"../data/Default.csv\")\nX = Default[['balance']]\ny = Default['default']\n\n\n\nCodelda = LinearDiscriminantAnalysis()\nlda.fit(X, y)\n\n\n\n\nLinearDiscriminantAnalysis()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nÂ Â LinearDiscriminantAnalysis?Documentation for LinearDiscriminantAnalysisiFittedLinearDiscriminantAnalysis() \n\n\nCodelda_pred = lda.predict(X)\nlda_conf_df = pd.DataFrame(\n    confusion_matrix(y_true=y, y_pred=lda_pred), \n    index=[f\"Actual: {cls}\" for cls in lda.classes_],\n    columns=[f\"Predicted: {cls}\" for cls in lda.classes_]\n)\nlda_conf_df\n\n             Predicted: No  Predicted: Yes\nActual: No            9643              24\nActual: Yes            257              76\n\n\n\nCodeqda = QuadraticDiscriminantAnalysis()\nqda.fit(X, y)\n\n\n\n\nQuadraticDiscriminantAnalysis()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nÂ Â QuadraticDiscriminantAnalysis?Documentation for QuadraticDiscriminantAnalysisiFittedQuadraticDiscriminantAnalysis() \n\n\nCodeqda_pred = qda.predict(X)\nqda_conf_df = pd.DataFrame(\n    confusion_matrix(y, qda_pred), \n    index=[f\"Actual: {cls}\" for cls in qda.classes_],\n    columns=[f\"Predicted: {cls}\" for cls in qda.classes_]\n)\nqda_conf_df\n\n             Predicted: No  Predicted: Yes\nActual: No            9639              28\nActual: Yes            246              87\n\n\n\nCodenb = GaussianNB()\nnb.fit(X, y)\n\n\n\n\nGaussianNB()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nÂ Â GaussianNB?Documentation for GaussianNBiFittedGaussianNB() \n\n\nCodenb_pred = nb.predict(X)\nnb_conf_df = pd.DataFrame(\n    confusion_matrix(y, nb_pred),\n    index=[f\"Actual: {cls}\" for cls in nb.classes_],\n    columns=[f\"Predicted: {cls}\" for cls in nb.classes_]\n)\nnb_conf_df\n\n             Predicted: No  Predicted: Yes\nActual: No            9639              28\nActual: Yes            246              87"
  },
  {
    "objectID": "code/08-bayes-code.html",
    "href": "code/08-bayes-code.html",
    "title": "08- Bayesian Linear Regression Code Demo",
    "section": "",
    "text": "Show/Hidelibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(rstan)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidybayes)\nlibrary(janitor)\nlibrary(broom.mixed)\n\n\n\nShow/Hidedata(bikes)\n\n\n\nShow/Hidebike_model &lt;- rstanarm::stan_glm(rides ~ temp_feel, data = bikes,\n                                 family = gaussian,\n                                 prior_intercept = normal(5000, 1000),\n                                 prior = normal(100, 40), \n                                 prior_aux = exponential(0.0008),\n                                 chains = 4, iter = 5000*2, seed = 2025)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000667 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 6.67 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.187 seconds (Warm-up)\nChain 1:                0.201 seconds (Sampling)\nChain 1:                0.388 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 9e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.182 seconds (Warm-up)\nChain 2:                0.206 seconds (Sampling)\nChain 2:                0.388 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 8e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.109 seconds (Warm-up)\nChain 3:                0.213 seconds (Sampling)\nChain 3:                0.322 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.5e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.136 seconds (Warm-up)\nChain 4:                0.211 seconds (Sampling)\nChain 4:                0.347 seconds (Total)\nChain 4: \n\n\n\nShow/Hidebayesplot::mcmc_trace(bike_model, size = 0.1)\n\n\n\n\n\n\n\n\nShow/Hidebayesplot::mcmc_dens_overlay(bike_model)\n\n\n\n\n\n\n\n\nShow/Hidebayesplot::neff_ratio(bike_model)\n\n(Intercept)   temp_feel       sigma \n    0.99465     0.99180     0.95030 \n\n\n\nShow/Hidebayesplot::rhat(bike_model)\n\n(Intercept)   temp_feel       sigma \n  1.0000752   1.0000909   0.9999646 \n\n\n\nShow/Hidetidy(bike_model, effects = c(\"fixed\", \"aux\"),\n     conf.int = TRUE, conf.level = 0.80)\n\n# A tibble: 4 Ã 5\n  term        estimate std.error conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  -2191.     355.    -2653.    -1735. \n2 temp_feel       82.1      5.07     75.7      88.7\n3 sigma         1282.      41.1    1231.     1336. \n4 mean_PPD      3487.      80.3    3385.     3590."
  },
  {
    "objectID": "code/08-bayes-code.html#r-implementation",
    "href": "code/08-bayes-code.html#r-implementation",
    "title": "08- Bayesian Linear Regression Code Demo",
    "section": "",
    "text": "Show/Hidelibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(rstan)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidybayes)\nlibrary(janitor)\nlibrary(broom.mixed)\n\n\n\nShow/Hidedata(bikes)\n\n\n\nShow/Hidebike_model &lt;- rstanarm::stan_glm(rides ~ temp_feel, data = bikes,\n                                 family = gaussian,\n                                 prior_intercept = normal(5000, 1000),\n                                 prior = normal(100, 40), \n                                 prior_aux = exponential(0.0008),\n                                 chains = 4, iter = 5000*2, seed = 2025)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000667 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 6.67 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.187 seconds (Warm-up)\nChain 1:                0.201 seconds (Sampling)\nChain 1:                0.388 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 9e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.182 seconds (Warm-up)\nChain 2:                0.206 seconds (Sampling)\nChain 2:                0.388 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 8e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.109 seconds (Warm-up)\nChain 3:                0.213 seconds (Sampling)\nChain 3:                0.322 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.5e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.136 seconds (Warm-up)\nChain 4:                0.211 seconds (Sampling)\nChain 4:                0.347 seconds (Total)\nChain 4: \n\n\n\nShow/Hidebayesplot::mcmc_trace(bike_model, size = 0.1)\n\n\n\n\n\n\n\n\nShow/Hidebayesplot::mcmc_dens_overlay(bike_model)\n\n\n\n\n\n\n\n\nShow/Hidebayesplot::neff_ratio(bike_model)\n\n(Intercept)   temp_feel       sigma \n    0.99465     0.99180     0.95030 \n\n\n\nShow/Hidebayesplot::rhat(bike_model)\n\n(Intercept)   temp_feel       sigma \n  1.0000752   1.0000909   0.9999646 \n\n\n\nShow/Hidetidy(bike_model, effects = c(\"fixed\", \"aux\"),\n     conf.int = TRUE, conf.level = 0.80)\n\n# A tibble: 4 Ã 5\n  term        estimate std.error conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  -2191.     355.    -2653.    -1735. \n2 temp_feel       82.1      5.07     75.7      88.7\n3 sigma         1282.      41.1    1231.     1336. \n4 mean_PPD      3487.      80.3    3385.     3590."
  },
  {
    "objectID": "code/08-bayes-code.html#python-implementation",
    "href": "code/08-bayes-code.html#python-implementation",
    "title": "08- Bayesian Linear Regression Code Demo",
    "section": "Python implementation",
    "text": "Python implementation\n\nCheck PyMC demo\nError occurs when rendering the quarto file. But it can be run in a Python script.\n\n\nShow/Hideimport pymc as pm\nimport numpy as np\nimport pandas as pd\nimport arviz as az\nimport xarray as xr\n\n\n\nShow/Hide# Load your dataset\nbikes = pd.read_csv(\"../data/bikes.csv\")\n# Assuming `rides` is the target variable and `temp_feel` is the predictor\nrides = bikes[\"rides\"].values\ntemp_feel = bikes[\"temp_feel\"].values\n\n\n\n\n\n\n\n\n\n\nShow/Hideimport pymc as pm\n# Define the Bayesian model\nwith pm.Model() as bike_model:\n    # Priors\n    intercept = pm.Normal(\"intercept\", mu=5000, sigma=1000)\n    slope = pm.Normal(\"slope\", mu=100, sigma=40)\n    # sigma = pm.Exponential(\"sigma\", lam=0.0008)\n    sigma = pm.HalfCauchy(\"sigma\", beta=10)\n    # Linear model\n    mu = intercept + slope * temp_feel\n\n    # Likelihood\n    likelihood = pm.Normal(\"rides\", mu=mu, sigma=sigma, observed=rides)\n\n    # Sampling\n    draws = pm.sample(5000, tune=5000, chains=4, random_seed=2025)\n\n\n# pm.summary(draws)\n\n\n\nShow/Hidepm.summary(draws)\n\n\n\nShow/Hideaz.plot_trace(draws)\n\n\n\nShow/Hidetrace.posterior[\"y_model\"] = trace.posterior[\"Intercept\"] + trace.posterior[\"x\"] * xr.DataArray(x)\n\n\n\nShow/Hide_, ax = plt.subplots(figsize=(7, 7))\naz.plot_lm(trace=trace, y=\"y\", num_samples=100, axes=ax, y_model=\"y_model\")\nax.set_title(\"Posterior predictive regression lines\")\nax.set_xlabel(\"x\");"
  },
  {
    "objectID": "code/11-knn-code.html",
    "href": "code/11-knn-code.html",
    "title": "11 - K Nearest Neighbors Code Demo",
    "section": "",
    "text": "Codeload(\"../data/ESL.mixture.rda\", verbose = TRUE)\n\nLoading objects:\n  ESL.mixture\n\nCodex &lt;- ESL.mixture$x\ny &lt;- ESL.mixture$y\nlibrary(class)\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nCodeknn_fit &lt;- class::knn(train = x, test = x, cl = y, k = 15)\ncaret::confusionMatrix(table(knn_fit, y))\n\nConfusion Matrix and Statistics\n\n       y\nknn_fit  0  1\n      0 82 13\n      1 18 87\n                                          \n               Accuracy : 0.845           \n                 95% CI : (0.7873, 0.8922)\n    No Information Rate : 0.5             \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.69            \n                                          \n Mcnemar's Test P-Value : 0.4725          \n                                          \n            Sensitivity : 0.8200          \n            Specificity : 0.8700          \n         Pos Pred Value : 0.8632          \n         Neg Pred Value : 0.8286          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4100          \n   Detection Prevalence : 0.4750          \n      Balanced Accuracy : 0.8450          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nCodeset.seed(2025)\ncontrol &lt;- trainControl(method = \"cv\", number = 10)\nknn_cvfit &lt;- train(y ~ ., method = \"knn\", \n                   data = data.frame(\"x\" = x, \"y\" = as.factor(y)),\n                   tuneGrid = data.frame(k = seq(1, 40, 1)),\n                   trControl = control)\npar(mar = c(4, 4, 0, 0))\nplot(knn_cvfit$results$k, 1 - knn_cvfit$results$Accuracy,\n     xlab = \"K\", ylab = \"Classification Error\", type = \"b\",\n     pch = 19, col = 2)\n\n\n\n\n\n\n\n\nCodezip.train &lt;- read.csv(\"../data/zip.train.csv\")\nzip.test &lt;- read.csv(\"../data/zip.test.csv\")\n# fit 3nn model and calculate the error\nknn.fit &lt;- class::knn(zip.train[, 2:257], zip.test[, 2:257], zip.train[, 1], k = 3)\n# overall prediction error\nmean(knn.fit != zip.test[, 1])\n\n[1] 0.05530643\n\nCode# the confusion matrix\ntable(knn.fit, zip.test[, 1], dnn = c(\"pred\", \"true\"))\n\n    true\npred   0   1   2   3   4   5   6   7   8   9\n   0 355   0   6   2   0   4   3   0   4   1\n   1   0 257   0   0   2   0   0   1   0   0\n   2   2   0 184   2   0   3   1   1   2   0\n   3   0   0   2 153   0   3   0   1   5   0\n   4   0   3   1   0 182   0   2   4   0   3\n   5   0   0   0   6   2 145   1   0   1   1\n   6   0   2   0   0   2   0 163   0   0   0\n   7   1   2   2   1   2   0   0 138   1   4\n   8   0   0   3   0   1   1   0   1 151   0\n   9   1   0   0   2   9   4   0   1   2 168"
  },
  {
    "objectID": "code/11-knn-code.html#r-implementation",
    "href": "code/11-knn-code.html#r-implementation",
    "title": "11 - K Nearest Neighbors Code Demo",
    "section": "",
    "text": "Codeload(\"../data/ESL.mixture.rda\", verbose = TRUE)\n\nLoading objects:\n  ESL.mixture\n\nCodex &lt;- ESL.mixture$x\ny &lt;- ESL.mixture$y\nlibrary(class)\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nCodeknn_fit &lt;- class::knn(train = x, test = x, cl = y, k = 15)\ncaret::confusionMatrix(table(knn_fit, y))\n\nConfusion Matrix and Statistics\n\n       y\nknn_fit  0  1\n      0 82 13\n      1 18 87\n                                          \n               Accuracy : 0.845           \n                 95% CI : (0.7873, 0.8922)\n    No Information Rate : 0.5             \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.69            \n                                          \n Mcnemar's Test P-Value : 0.4725          \n                                          \n            Sensitivity : 0.8200          \n            Specificity : 0.8700          \n         Pos Pred Value : 0.8632          \n         Neg Pred Value : 0.8286          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4100          \n   Detection Prevalence : 0.4750          \n      Balanced Accuracy : 0.8450          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nCodeset.seed(2025)\ncontrol &lt;- trainControl(method = \"cv\", number = 10)\nknn_cvfit &lt;- train(y ~ ., method = \"knn\", \n                   data = data.frame(\"x\" = x, \"y\" = as.factor(y)),\n                   tuneGrid = data.frame(k = seq(1, 40, 1)),\n                   trControl = control)\npar(mar = c(4, 4, 0, 0))\nplot(knn_cvfit$results$k, 1 - knn_cvfit$results$Accuracy,\n     xlab = \"K\", ylab = \"Classification Error\", type = \"b\",\n     pch = 19, col = 2)\n\n\n\n\n\n\n\n\nCodezip.train &lt;- read.csv(\"../data/zip.train.csv\")\nzip.test &lt;- read.csv(\"../data/zip.test.csv\")\n# fit 3nn model and calculate the error\nknn.fit &lt;- class::knn(zip.train[, 2:257], zip.test[, 2:257], zip.train[, 1], k = 3)\n# overall prediction error\nmean(knn.fit != zip.test[, 1])\n\n[1] 0.05530643\n\nCode# the confusion matrix\ntable(knn.fit, zip.test[, 1], dnn = c(\"pred\", \"true\"))\n\n    true\npred   0   1   2   3   4   5   6   7   8   9\n   0 355   0   6   2   0   4   3   0   4   1\n   1   0 257   0   0   2   0   0   1   0   0\n   2   2   0 184   2   0   3   1   1   2   0\n   3   0   0   2 153   0   3   0   1   5   0\n   4   0   3   1   0 182   0   2   4   0   3\n   5   0   0   0   6   2 145   1   0   1   1\n   6   0   2   0   0   2   0 163   0   0   0\n   7   1   2   2   1   2   0   0 138   1   4\n   8   0   0   3   0   1   1   0   1 151   0\n   9   1   0   0   2   9   4   0   1   2 168"
  },
  {
    "objectID": "code/11-knn-code.html#python-implementation",
    "href": "code/11-knn-code.html#python-implementation",
    "title": "11 - K Nearest Neighbors Code Demo",
    "section": "\n2 Python implementation",
    "text": "2 Python implementation\n\nCodeimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport rdata\n\n\n\nCode# Load the .rda file into a dictionary\nmixture_example = rdata.read_rda('../data/ESL.mixture.rda')\n\n/Users/chenghanyu/.virtualenvs/r-reticulate/lib/python3.12/site-packages/rdata/conversion/_conversion.py:856: UserWarning: Missing constructor for R class \"matrix\". The underlying R object is returned instead.\n  warnings.warn(\n\nCodex = mixture_example['ESL.mixture']['x']\ny = mixture_example['ESL.mixture']['y']\n\n\n\nCodeknn = KNeighborsClassifier(n_neighbors=15)\nknn.fit(x, y)\n\n\n\n\nKNeighborsClassifier(n_neighbors=15)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nÂ Â KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(n_neighbors=15) \n\n\nCodepred = knn.predict(x)\npd.DataFrame(confusion_matrix(y, pred), \n             index=[f\"Actual {int(i)}\" for i in np.unique(y)], \n             columns=[f\"Pred {int(i)}\" for i in np.unique(y)])\n\n          Pred 0  Pred 1\nActual 0      82      18\nActual 1      13      87\n\n\n\nCode# Prepare train/test split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,\n                                                    random_state=2025)\n\n# Perform 10-fold cross-validation for different k values\nk_values = range(1, 41)\ncv_errors = []\n\nfor k in k_values:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Use negative accuracy for error rate\n    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n    cv_errors.append(1 - np.mean(scores))  # Classification error = 1 - accuracy\n\n# Plot classification error vs. k\nplt.figure()\nplt.plot(k_values, cv_errors, marker='o', linestyle='-', color='red')\nplt.xlabel('K')\nplt.ylabel('Classification Error')\nplt.title('K vs. Classification Error')\nplt.show()\n\n\n\n\n\n\n\n\nCodezip_train = pd.read_csv(\"../data/zip.train.csv\",).to_numpy()\nzip_test = pd.read_csv(\"../data/zip.test.csv\",).to_numpy()\nx_train = zip_train[:, 1:257]\ny_train = zip_train[:, 0]\nx_test = zip_test[:, 1:257]\ny_test = zip_test[:, 0]\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train, y_train)\n\n\n\n\nKNeighborsClassifier(n_neighbors=3)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nÂ Â KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(n_neighbors=3) \n\n\nCodeknn_pred = knn.predict(x_test)\n\nnp.mean(knn_pred != y_test)\n\n0.05530642750373692\n\nCodepd.DataFrame(confusion_matrix(y_test, knn_pred), \n             index=[f\"Actual {int(i)}\" for i in np.unique(y_test)], \n             columns=[f\"Pred {int(i)}\" for i in np.unique(y_test)])\n\n          Pred 0  Pred 1  Pred 2  Pred 3  ...  Pred 6  Pred 7  Pred 8  Pred 9\nActual 0     355       0       3       0  ...       0       0       0       1\nActual 1       0     258       0       0  ...       2       1       0       0\nActual 2       8       0     183       1  ...       0       2       3       0\nActual 3       3       0       2     153  ...       0       1       0       1\nActual 4       0       2       0       0  ...       2       2       1       8\nActual 5       5       0       3       3  ...       0       0       1       4\nActual 6       3       1       1       0  ...     163       0       0       0\nActual 7       0       1       1       1  ...       0     138       1       1\nActual 8       4       0       3       4  ...       0       1     151       2\nActual 9       2       0       0       0  ...       0       4       0     168\n\n[10 rows x 10 columns]"
  },
  {
    "objectID": "code/07-splines-code.html",
    "href": "code/07-splines-code.html",
    "title": "07- Splines and Generalized Addive Models Code Demo",
    "section": "",
    "text": "Show/Hidebirthrates &lt;- read.csv(\"../data/birthrates.csv\")\n\n\n\n\nShow/Hidelmfit3 &lt;- lm(Birthrate ~ poly(Year-mean(Year), degree = 3), data = birthrates)\nplot(birthrates, pch = 19, col = 4, main = \"degree = 3\")\nlines(birthrates$Year, lmfit3$fitted.values, lty = 1, col = 2, lwd = 2)\n\n\n\n\n\n\n\n\n\n\nShow/Hidelibrary(splines)\n\n\nFor linear splines, change degree = 3 to degree = 1.\n\nShow/Hidecub_sp &lt;- lm(Birthrate ~ splines::bs(Year, degree = 3, knots = c(1936, 1960, 1978)), \n             data = birthrates)\nplot(birthrates, pch = 19, col = 4, main = \"Cubic spline (k = 3) with 3 knots\")\nlines(birthrates$Year, cub_sp$fitted.values, lty = 1, col = 2, lwd = 3)\n\n\n\n\n\n\n\n\n\nShow/Hidefit &lt;- smooth.spline(birthrates$Year, birthrates$Birthrate, df = 15)\nplot(birthrates$Year, birthrates$Birthrate, pch = 19, \n     xlab = \"Year\", ylab = \"BirthRates\", col = 4)\nlines(seq(1917, 2003), predict(fit, seq(1917, 2003))$y, col = 2, lty = 1, lwd = 3)\n\n\n\n\n\n\n\n\n\nShow/Hidelibrary(gam)\n\nLoading required package: foreach\n\n\nLoaded gam 1.22-4\n\n\n\nShow/HideWage &lt;- read.csv(\"../data/Wage.csv\")\nWage$education &lt;- as.factor(Wage$education)\ngam.m3 &lt;- gam(wage ~ s(year, df = 4) + s(age , df = 5) + education, data = Wage)\npar(mfrow = c(1, 3), mar = c(4, 4, 2, 0))\nplot.Gam(gam.m3, se = TRUE, col = 4, lwd = 2, las = 1)"
  },
  {
    "objectID": "code/07-splines-code.html#r-implementation",
    "href": "code/07-splines-code.html#r-implementation",
    "title": "07- Splines and Generalized Addive Models Code Demo",
    "section": "",
    "text": "Show/Hidebirthrates &lt;- read.csv(\"../data/birthrates.csv\")\n\n\n\n\nShow/Hidelmfit3 &lt;- lm(Birthrate ~ poly(Year-mean(Year), degree = 3), data = birthrates)\nplot(birthrates, pch = 19, col = 4, main = \"degree = 3\")\nlines(birthrates$Year, lmfit3$fitted.values, lty = 1, col = 2, lwd = 2)\n\n\n\n\n\n\n\n\n\n\nShow/Hidelibrary(splines)\n\n\nFor linear splines, change degree = 3 to degree = 1.\n\nShow/Hidecub_sp &lt;- lm(Birthrate ~ splines::bs(Year, degree = 3, knots = c(1936, 1960, 1978)), \n             data = birthrates)\nplot(birthrates, pch = 19, col = 4, main = \"Cubic spline (k = 3) with 3 knots\")\nlines(birthrates$Year, cub_sp$fitted.values, lty = 1, col = 2, lwd = 3)\n\n\n\n\n\n\n\n\n\nShow/Hidefit &lt;- smooth.spline(birthrates$Year, birthrates$Birthrate, df = 15)\nplot(birthrates$Year, birthrates$Birthrate, pch = 19, \n     xlab = \"Year\", ylab = \"BirthRates\", col = 4)\nlines(seq(1917, 2003), predict(fit, seq(1917, 2003))$y, col = 2, lty = 1, lwd = 3)\n\n\n\n\n\n\n\n\n\nShow/Hidelibrary(gam)\n\nLoading required package: foreach\n\n\nLoaded gam 1.22-4\n\n\n\nShow/HideWage &lt;- read.csv(\"../data/Wage.csv\")\nWage$education &lt;- as.factor(Wage$education)\ngam.m3 &lt;- gam(wage ~ s(year, df = 4) + s(age , df = 5) + education, data = Wage)\npar(mfrow = c(1, 3), mar = c(4, 4, 2, 0))\nplot.Gam(gam.m3, se = TRUE, col = 4, lwd = 2, las = 1)"
  },
  {
    "objectID": "code/07-splines-code.html#python-implementation",
    "href": "code/07-splines-code.html#python-implementation",
    "title": "07- Splines and Generalized Addive Models Code Demo",
    "section": "Python implementation",
    "text": "Python implementation\n\nShow/Hideimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nShow/Hidebirthrates = pd.read_csv(\"../data/birthrates.csv\")\n\n\nPolynomial regression\n\nShow/Hidefrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\n\nShow/Hidebirthrates['Year_centered'] = birthrates['Year'] - birthrates['Year'].mean()\n\n# Polynomial regression with degree = 3\npoly = PolynomialFeatures(degree=3, include_bias=False)\nX_poly = poly.fit_transform(birthrates[['Year_centered']])\n\npolyfit3 = LinearRegression().fit(X_poly, birthrates['Birthrate'])\n\nplt.scatter(birthrates['Year'], birthrates['Birthrate'], color='blue')\nplt.plot(birthrates['Year'], polyfit3.predict(X_poly), color='red',\n         linewidth=2)\nplt.title(\"Cubic Polynomial Regression (Degree = 3)\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Birthrate\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCubic Splines\n\nShow/Hidefrom patsy import dmatrix\nfrom sklearn.linear_model import LinearRegression\n\n\nFor linear splines, change degree=3 to degree=1.\nNOTE:\n\nCanât find prediction or extrapolation for cubic splines.\nUse patsy.cr() to fit natural cubic splines.\n\n\nShow/Hideknots = [1936, 1960, 1978]\n# https://patsy.readthedocs.io/en/latest/API-reference.html\n# Generate cubic spline basis functions with specified knots\nspline_basis = dmatrix(\n    \"bs(Year, degree=3, knots=knots, include_intercept=True)\", \n    {\"Year\": birthrates[\"Year\"]}, \n    return_type=\"dataframe\"\n)\n# Fit the cubic spline model\n# import statsmodels.api as sm\n# model = sm.OLS(birthrates[\"Birthrate\"], spline_basis).fit()\n# birthrates[\"Fitted\"] = model.fittedvalues\ncub_sp = LinearRegression().fit(spline_basis, birthrates[\"Birthrate\"])\n\n# Plot the data and the fitted spline\nplt.scatter(birthrates[\"Year\"], birthrates[\"Birthrate\"], color=\"blue\")\nplt.plot(birthrates[\"Year\"], cub_sp.predict(spline_basis), color=\"red\",\n         linewidth=2)\nplt.title(\"Cubic Spline (d=3) with 3 Knots\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Birthrate\")\nplt.show()\n\n\n\n\n\n\n\n\nShow/Hide# https://patsy.readthedocs.io/en/latest/API-reference.html\n# Generate cubic spline basis functions with specified knots\nspline_basis = dmatrix(\n    \"bs(Year, degree=3, df=7, include_intercept=True)\", \n    {\"Year\": birthrates[\"Year\"]}, \n    return_type=\"dataframe\"\n)\n# Fit the cubic spline model\n# import statsmodels.api as sm\n# model = sm.OLS(birthrates[\"Birthrate\"], spline_basis).fit()\n# birthrates[\"Fitted\"] = model.fittedvalues\ncub_sp = LinearRegression().fit(spline_basis, birthrates[\"Birthrate\"])\n\n# Plot the data and the fitted spline\nplt.scatter(birthrates[\"Year\"], birthrates[\"Birthrate\"], color=\"blue\")\nplt.plot(birthrates[\"Year\"], cub_sp.predict(spline_basis), color=\"red\",\n         linewidth=2)\nplt.title(\"Cubic Spline (df=6)\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Birthrate\")\nplt.show()\n\n\n\n\n\n\n\nSmoothing Splines\nWe use scipy.interpolate.make_smoothing_spline.\n\nPython has no functions for smoothing splines that can directly specify the degrees of freedom. Please let me know if you find one.\nTo have similar smoothing results, R and Python would use quite a different size of penalty term \\(\\lambda\\), as well as the degrees of freedom and smoothing factor.\n\n\nShow/Hidefrom scipy.interpolate import make_smoothing_spline\n\n\n\nShow/Hidex = birthrates[\"Year\"].values\ny = birthrates[\"Birthrate\"].values\nspline = make_smoothing_spline(x, y, lam=20)\n# Predict for the range of years\nx_pred = np.linspace(1917, 2003, 500)\ny_pred = spline(x_pred)\n# Plot the original data\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, color='blue', label='Data', s=50)\nplt.plot(x_pred, y_pred, color='red', linewidth=3, label='Smoothing Spline')\nplt.title(\"Smoothing Spline\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Birthrate\")\nplt.show()\n\n\n\n\n\n\n\nTo use a smoothing factor, use splrep (make_splrep) and BSpline\nThe smoothing factor is set unresonably high to 4500. Please let me know if you figure out why.\n\nShow/Hidefrom scipy.interpolate import splrep, BSpline\n\n\n\nShow/Hidex = birthrates[\"Year\"].values\ny = birthrates[\"Birthrate\"].values\n\n# Fit the smoothing spline with a smoothing factor\nsmoothing_factor = 4500 # Adjust this for the desired smoothness\ntck = splrep(x, y, s=smoothing_factor)\n\n# Predict for a range of years\nx_pred = np.linspace(1917, 2003, 500)\ny_pred = BSpline(*tck)(x_pred)\n\n# Plot the original data\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, color='blue', label='Data', s=50)\n\n# Plot the fitted smoothing spline\nplt.plot(x_pred, y_pred, color='red', linewidth=3, label='Smoothing Spline')\n\n# Add labels and title\nplt.title(\"Smoothing Spline with splrep\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Birthrate\")\nplt.show()\n\n\n\n\n\n\n\nGAM\nhttps://kirenz.github.io/regression/docs/gam.html\nhttps://gist.github.com/josef-pkt/453de603b019143e831fbdd4dfb6aa30\n\nShow/Hidefrom statsmodels.gam.api import BSplines\nfrom statsmodels.gam.api import GLMGam\nfrom statsmodels.tools.eval_measures import mse, rmse\nimport statsmodels.api as sm\nimport patsy\n\n\n\nShow/HideWage = pd.read_csv(\"../data/Wage.csv\")\nWage['education'] = pd.Categorical(Wage['education'], categories=['1. &lt; HS Grad', '2. HS Grad', '3. Some College', '4. College Grad', '5. Advanced Degree'], ordered=True)\n\n# penalization weights are taken from mgcv to match up its results\n# sp = np.array([0.830689464223685, 425.361212061649])\n# s_scale = np.array([2.443955e-06, 0.007945455])\nx_spline = Wage[['year', 'age']].values\nexog = patsy.dmatrix('education', data=Wage)\n\n# TODO: set `include_intercept=True` automatically if constraints='center'\nbs = BSplines(x_spline, df=[4, 5], degree=[3, 3], variable_names=['year', 'age'], \n              constraints='center', include_intercept=True)\n# alpha = 1 / s_scale * sp / 2\ngam_bs = GLMGam(Wage['wage'], exog=exog, smoother=bs)\nres = gam_bs.fit()\n\n\n\nShow/Hidefig, axes = plt.subplots(1, 2, figsize=(12, 6))\nres.plot_partial(0, cpr=False, include_constant=False, ax=axes[0])\naxes[0].set_title(\"Year\")\nres.plot_partial(1, cpr=False, include_constant=False, ax=axes[1])\naxes[1].set_title(\"Age\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nShow/Hideprint(res.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                   wage   No. Observations:                 3000\nModel:                         GLMGam   Df Residuals:                     2988\nModel Family:                Gaussian   Df Model:                        11.00\nLink Function:               Identity   Scale:                          1238.8\nMethod:                         PIRLS   Log-Likelihood:                -14934.\nDate:                Mon, 10 Feb 2025   Deviance:                   3.7014e+06\nTime:                        21:43:15   Pearson chi2:                 3.70e+06\nNo. Iterations:                     3   Pseudo R-squ. (CS):             0.3358\nCovariance Type:            nonrobust                                         \n===================================================================================================\n                                      coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------------\nIntercept                          85.6860      2.156     39.745      0.000      81.461      89.911\neducation[T.2. HS Grad]            10.7413      2.431      4.418      0.000       5.977      15.506\neducation[T.3. Some College]       23.2067      2.563      9.056      0.000      18.184      28.229\neducation[T.4. College Grad]       37.8704      2.547     14.871      0.000      32.879      42.862\neducation[T.5. Advanced Degree]    62.4355      2.764     22.591      0.000      57.019      67.852\nyear_s0                             3.3874      4.257      0.796      0.426      -4.957      11.732\nyear_s1                             1.8170      4.220      0.431      0.667      -6.454      10.088\nyear_s2                             4.4943      1.754      2.563      0.010       1.057       7.931\nage_s0                             10.1360      5.932      1.709      0.087      -1.490      21.762\nage_s1                             47.6380      5.326      8.945      0.000      37.200      58.076\nage_s2                              6.7739      7.296      0.928      0.353      -7.526      21.074\nage_s3                            -10.0472     10.672     -0.941      0.346     -30.963      10.869\n===================================================================================================\n\n\nOne option is to use pygam package.\n\nShow/Hidefrom pygam import LinearGAM, s, f\nfrom pygam.datasets import wage\n\n\n\nShow/HideX, y = wage(return_X_y=True)\n\n## model\ngam = LinearGAM(s(0) + s(1) + f(2)).fit(X, y)\n\nfor i, term in enumerate(gam.terms):\n    if term.isintercept:\n        continue\n\n    XX = gam.generate_X_grid(term=i)\n    pdep, confi = gam.partial_dependence(term=i, X=XX, width=0.95)\n\n    plt.figure()\n    plt.plot(XX[:, term.feature], pdep)\n    plt.plot(XX[:, term.feature], confi, c='r', ls='--')\n    plt.title(repr(term))\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow/Hidegam.summary()\n\nLinearGAM                                                                                                 \n=============================================== ==========================================================\nDistribution:                        NormalDist Effective DoF:                                     25.1911\nLink Function:                     IdentityLink Log Likelihood:                                -24118.6847\nNumber of Samples:                         3000 AIC:                                            48289.7516\n                                                AICc:                                           48290.2307\n                                                GCV:                                             1255.6902\n                                                Scale:                                           1236.7251\n                                                Pseudo R-Squared:                                   0.2955\n==========================================================================================================\nFeature Function                  Lambda               Rank         EDoF         P &gt; x        Sig. Code   \n================================= ==================== ============ ============ ============ ============\ns(0)                              [0.6]                20           7.1          5.95e-03     **          \ns(1)                              [0.6]                20           14.1         1.11e-16     ***         \nf(2)                              [0.6]                5            4.0          1.11e-16     ***         \nintercept                                              1            0.0          1.11e-16     ***         \n==========================================================================================================\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n         which can cause p-values to appear significant when they are not.\n\nWARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n         are typically lower than they should be, meaning that the tests reject the null too readily.\n&lt;string&gt;:3: UserWarning: KNOWN BUG: p-values computed in this summary are likely much smaller than they should be. \n \nPlease do not make inferences based on these values! \n\nCollaborate on a solution, and stay up to date at: \ngithub.com/dswah/pyGAM/issues/163"
  },
  {
    "objectID": "code/04-linear-reg-code.html",
    "href": "code/04-linear-reg-code.html",
    "title": "04-Linear Regression Code Demo",
    "section": "",
    "text": "Show/Hideadvertising_data &lt;- read.csv(\"../data/Advertising.csv\")\nadvertising_data &lt;- advertising_data[, 2:5]\nhead(advertising_data)\n\n     TV radio newspaper sales\n1 230.1  37.8      69.2  22.1\n2  44.5  39.3      45.1  10.4\n3  17.2  45.9      69.3   9.3\n4 151.5  41.3      58.5  18.5\n5 180.8  10.8      58.4  12.9\n6   8.7  48.9      75.0   7.2\n\nShow/Hidelm_out &lt;- lm(advertising_data$sales ~ ., data = advertising_data)\nsummary(lm_out)\n\n\nCall:\nlm(formula = advertising_data$sales ~ ., data = advertising_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\nShow/Hideconfint(lm_out)\n\n                  2.5 %     97.5 %\n(Intercept)  2.32376228 3.55401646\nTV           0.04301371 0.04851558\nradio        0.17154745 0.20551259\nnewspaper   -0.01261595 0.01054097"
  },
  {
    "objectID": "code/04-linear-reg-code.html#r-implementation",
    "href": "code/04-linear-reg-code.html#r-implementation",
    "title": "04-Linear Regression Code Demo",
    "section": "",
    "text": "Show/Hideadvertising_data &lt;- read.csv(\"../data/Advertising.csv\")\nadvertising_data &lt;- advertising_data[, 2:5]\nhead(advertising_data)\n\n     TV radio newspaper sales\n1 230.1  37.8      69.2  22.1\n2  44.5  39.3      45.1  10.4\n3  17.2  45.9      69.3   9.3\n4 151.5  41.3      58.5  18.5\n5 180.8  10.8      58.4  12.9\n6   8.7  48.9      75.0   7.2\n\nShow/Hidelm_out &lt;- lm(advertising_data$sales ~ ., data = advertising_data)\nsummary(lm_out)\n\n\nCall:\nlm(formula = advertising_data$sales ~ ., data = advertising_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\nShow/Hideconfint(lm_out)\n\n                  2.5 %     97.5 %\n(Intercept)  2.32376228 3.55401646\nTV           0.04301371 0.04851558\nradio        0.17154745 0.20551259\nnewspaper   -0.01261595 0.01054097"
  },
  {
    "objectID": "code/04-linear-reg-code.html#python-implementation",
    "href": "code/04-linear-reg-code.html#python-implementation",
    "title": "04-Linear Regression Code Demo",
    "section": "Python implementation",
    "text": "Python implementation\n\nShow/Hideimport pandas as pd\nimport numpy as np\n\n\n\nShow/Hideadvertising_data = pd.read_csv(\"../data/Advertising.csv\")\nadvertising_data = advertising_data.iloc[:, 1:5]\nX = advertising_data.drop(columns=[\"sales\"])\ny = advertising_data[\"sales\"]\n\n\nscikit-learn\n\nShow/Hidefrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nreg.intercept_\n\n2.9388893694594014\n\nShow/Hidereg.coef_\n\narray([ 0.04576465,  0.18853002, -0.00103749])\n\n\nstatsmodels\n\nShow/Hidefrom statsmodels.formula.api import ols\nols_out = ols(formula='sales ~ TV + radio + newspaper', data=advertising_data).fit()\nols_out.params\n\nIntercept    2.938889\nTV           0.045765\nradio        0.188530\nnewspaper   -0.001037\ndtype: float64\n\nShow/Hideprint(ols_out.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  sales   R-squared:                       0.897\nModel:                            OLS   Adj. R-squared:                  0.896\nMethod:                 Least Squares   F-statistic:                     570.3\nDate:                Mon, 13 Jan 2025   Prob (F-statistic):           1.58e-96\nTime:                        11:56:31   Log-Likelihood:                -386.18\nNo. Observations:                 200   AIC:                             780.4\nDf Residuals:                     196   BIC:                             793.6\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.9389      0.312      9.422      0.000       2.324       3.554\nTV             0.0458      0.001     32.809      0.000       0.043       0.049\nradio          0.1885      0.009     21.893      0.000       0.172       0.206\nnewspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n==============================================================================\nOmnibus:                       60.414   Durbin-Watson:                   2.084\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\nSkew:                          -1.327   Prob(JB):                     1.44e-33\nKurtosis:                       6.332   Cond. No.                         454.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nShow/Hidecoef_summary = ols_out.summary2().tables[1]  # Get the coefficients table\nprint(coef_summary)\n\n              Coef.  Std.Err.          t         P&gt;|t|    [0.025    0.975]\nIntercept  2.938889  0.311908   9.422288  1.267295e-17  2.323762  3.554016\nTV         0.045765  0.001395  32.808624  1.509960e-81  0.043014  0.048516\nradio      0.188530  0.008611  21.893496  1.505339e-54  0.171547  0.205513\nnewspaper -0.001037  0.005871  -0.176715  8.599151e-01 -0.012616  0.010541\n\nShow/Hideconf_intervals = ols_out.conf_int()\nprint(conf_intervals)\n\n                  0         1\nIntercept  2.323762  3.554016\nTV         0.043014  0.048516\nradio      0.171547  0.205513\nnewspaper -0.012616  0.010541"
  },
  {
    "objectID": "code/12-gp-code.html",
    "href": "code/12-gp-code.html",
    "title": "12 - Gaussian Process Regression Code Demo",
    "section": "",
    "text": "Codelibrary(plgp)\n\nLoading required package: mvtnorm\n\n\nLoading required package: tgp\n\n\n\nCodelen &lt;- 100\nn_path &lt;- 5\nX &lt;- seq(0, 5, length = len)\nD &lt;- distance(X)\neps &lt;- sqrt(.Machine$double.eps) \nSigma &lt;- exp(-D/2) + diag(eps, len) \ny &lt;- mvnfast::rmvn(n_path, mu = rep(0, len), sigma = Sigma)\nmatplot(X, t(y), type = \"l\", main = \"Prior\", lwd = 2,\n        ylim = c(-3, 3), las = 1, lty = 1, xlab = \"x\", ylab = \"f(x)\")\n\n\n\n\n\n\nCoden &lt;- 5\nX &lt;- matrix(seq(0, 2*pi, length=n), ncol=1)\ny &lt;- sin(X)\nD &lt;- distance(X) \nSigma &lt;- exp(-D/2) + diag(eps, ncol(D))\nXX &lt;- matrix(seq(-0.5, 2*pi + 0.5, length=100), ncol=1)\nDXX &lt;- distance(XX)\nSXX &lt;- exp(-DXX/2) + diag(eps, ncol(DXX))\nDX &lt;- distance(XX, X)\nSX &lt;- exp(-DX/2) \nSi &lt;- solve(Sigma)\nmup &lt;- SX %*% Si %*% y\nSigmap &lt;- SXX - SX %*% Si %*% t(SX)\nYY &lt;- rmvnorm(100, mup, Sigmap)\nq1 &lt;- mup + qnorm(0.025, 0, sqrt(diag(Sigmap)))\nq2 &lt;- mup + qnorm(0.975, 0, sqrt(diag(Sigmap)))\nmatplot(XX, t(YY)[, 1:5], type=\"l\", col=1:5, lty=1, xlab=\"x\", ylab=\"f(x)\",\n        las = 1, main = \"Posterior\", lwd = 2)\npoints(X, y, pch=20, cex=2)\n\n\n\n\n\n\nCodematplot(XX, t(YY), type=\"l\", col=c(rep(\"grey\", nrow(XX))), \n        lty=1, xlab=\"x\", ylab=\"f(x)\", las = 1, main = \"Prediction with Uncertainty\")\npoints(X, y, pch=20, cex=2)\nlines(XX, t(YY)[, 1], col=3, lty = 1)\nlines(XX, mup, lwd=2, col = \"blue\", lty=2)\nlines(XX, q1, lwd=2, lty=2, col=2)\nlines(XX, q2, lwd=2, lty=2, col=2)"
  },
  {
    "objectID": "code/12-gp-code.html#r-implementation",
    "href": "code/12-gp-code.html#r-implementation",
    "title": "12 - Gaussian Process Regression Code Demo",
    "section": "",
    "text": "Codelibrary(plgp)\n\nLoading required package: mvtnorm\n\n\nLoading required package: tgp\n\n\n\nCodelen &lt;- 100\nn_path &lt;- 5\nX &lt;- seq(0, 5, length = len)\nD &lt;- distance(X)\neps &lt;- sqrt(.Machine$double.eps) \nSigma &lt;- exp(-D/2) + diag(eps, len) \ny &lt;- mvnfast::rmvn(n_path, mu = rep(0, len), sigma = Sigma)\nmatplot(X, t(y), type = \"l\", main = \"Prior\", lwd = 2,\n        ylim = c(-3, 3), las = 1, lty = 1, xlab = \"x\", ylab = \"f(x)\")\n\n\n\n\n\n\nCoden &lt;- 5\nX &lt;- matrix(seq(0, 2*pi, length=n), ncol=1)\ny &lt;- sin(X)\nD &lt;- distance(X) \nSigma &lt;- exp(-D/2) + diag(eps, ncol(D))\nXX &lt;- matrix(seq(-0.5, 2*pi + 0.5, length=100), ncol=1)\nDXX &lt;- distance(XX)\nSXX &lt;- exp(-DXX/2) + diag(eps, ncol(DXX))\nDX &lt;- distance(XX, X)\nSX &lt;- exp(-DX/2) \nSi &lt;- solve(Sigma)\nmup &lt;- SX %*% Si %*% y\nSigmap &lt;- SXX - SX %*% Si %*% t(SX)\nYY &lt;- rmvnorm(100, mup, Sigmap)\nq1 &lt;- mup + qnorm(0.025, 0, sqrt(diag(Sigmap)))\nq2 &lt;- mup + qnorm(0.975, 0, sqrt(diag(Sigmap)))\nmatplot(XX, t(YY)[, 1:5], type=\"l\", col=1:5, lty=1, xlab=\"x\", ylab=\"f(x)\",\n        las = 1, main = \"Posterior\", lwd = 2)\npoints(X, y, pch=20, cex=2)\n\n\n\n\n\n\nCodematplot(XX, t(YY), type=\"l\", col=c(rep(\"grey\", nrow(XX))), \n        lty=1, xlab=\"x\", ylab=\"f(x)\", las = 1, main = \"Prediction with Uncertainty\")\npoints(X, y, pch=20, cex=2)\nlines(XX, t(YY)[, 1], col=3, lty = 1)\nlines(XX, mup, lwd=2, col = \"blue\", lty=2)\nlines(XX, q1, lwd=2, lty=2, col=2)\nlines(XX, q2, lwd=2, lty=2, col=2)"
  },
  {
    "objectID": "code/12-gp-code.html#python-implementation",
    "href": "code/12-gp-code.html#python-implementation",
    "title": "12 - Gaussian Process Regression Code Demo",
    "section": "\n2 Python implementation",
    "text": "2 Python implementation\n\nCodeimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\n\n\n\nCodedef plot_gpr_samples(gpr_model, n_samples, ax):\n    \"\"\"Plot samples drawn from the Gaussian process model.\n\n    If the Gaussian process model is not trained then the drawn samples are\n    drawn from the prior distribution. Otherwise, the samples are drawn from\n    the posterior distribution. Be aware that a sample here corresponds to a\n    function.\n\n    Parameters\n    ----------\n    gpr_model : `GaussianProcessRegressor`\n        A :class:`~sklearn.gaussian_process.GaussianProcessRegressor` model.\n    n_samples : int\n        The number of samples to draw from the Gaussian process distribution.\n    ax : matplotlib axis\n        The matplotlib axis where to plot the samples.\n    \"\"\"\n    x = np.linspace(0, 2 * np.pi, 100)\n    X = x.reshape(-1, 1)\n\n    y_mean, y_std = gpr_model.predict(X, return_std=True)\n    y_samples = gpr_model.sample_y(X, n_samples)\n\n    for idx, single_prior in enumerate(y_samples.T):\n        ax.plot(\n            x,\n            single_prior,\n            linestyle=\"--\",\n            alpha=0.7,\n            label=f\"Sampled function #{idx + 1}\",\n        )\n    ax.plot(x, y_mean, color=\"black\", label=\"Mean\")\n    ax.fill_between(\n        x,\n        y_mean - y_std,\n        y_mean + y_std,\n        alpha=0.1,\n        color=\"black\",\n        label=r\"$\\pm$ 1 std. dev.\",\n    )\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.set_ylim([-3, 3])\n\n\n\nCoderng = np.random.RandomState(4)\n\nX_train = np.linspace(0, 2 * np.pi, 5).reshape(-1, 1)\ny_train = np.sin(X_train)\n\nn_samples = 5\n\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\n\nkernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0))\ngpr = GaussianProcessRegressor(kernel=kernel, random_state=0)\n\nfig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))\n\n# plot prior\nplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\naxs[0].set_title(\"Samples from prior distribution\")\n\n# plot posterior\ngpr.fit(X_train, y_train)\n\n\n\n\nGaussianProcessRegressor(kernel=1**2 * RBF(length_scale=1), random_state=0)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nÂ Â GaussianProcessRegressor?Documentation for GaussianProcessRegressoriFittedGaussianProcessRegressor(kernel=1**2 * RBF(length_scale=1), random_state=0) \n\n\nCodeplot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\naxs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\naxs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\naxs[1].set_title(\"Samples from posterior distribution\")\n\nfig.suptitle(\"Radial Basis Function kernel\", fontsize=18)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MSSC 6250 - Statistical Machine Learning (Spring 2025)",
    "section": "",
    "text": "Homework 4 released, and due Friday, 5/2, 11:59 PM.\n\nThis schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nTo Do\nSlides\nCode\nHomework\nProject\n\n\n\n\n1\nTue, Jan 14\nSyllabus/Overview of Statistical Learning\nð\nð¥ï¸ð¥ï¸\n\n\n\n\n\n\nThu, Jan 16\nBias-variance tradeoff\n\nð¥ï¸\n\n\n\n\n\n2\nTue, Jan 21\nNO CLASS: Cold Weather\nð\nð¥ï¸\n\n\n\n\n\n\nThu, Jan 23\nLinear Regression\n\n\n\n\n\n\n\n3\nTue, Jan 28\nNumerical Optimization\nð\n\n\nâï¸ HW1\n\n\n\n\nThu, Jan 30\nStochastic Gradient Descen; Ridge Regression\n\nð¥ï¸\n\n\n\n\n\n4\nTue, Feb 4\nCross-Validation; Feature Selection\nð\nð¥ï¸\n\n\n\n\n\n\nThu, Feb 6\nLASSO\n\n\n\n\n\n\n\n5\nTue, Feb 11\nRegression Splines\nð\nð¥ï¸\n\n\n\n\n\n\nThu, Feb 13\nSmoothing Splines and Generalized Additive Models\n\n\n\nHW 1 Due\n\n\n\n6\nTue, Feb 18\nBayesian Inference and Linear Regression\nð\nð¥ï¸\n\nâï¸ HW2\n\n\n\n\nThu, Feb 20\nBayesian Linear Regression\n\n\n\n\nâ Team up Due\n\n\n7\nTue, Feb 25\nBinary Logistic Regression\nð\nð¥ï¸\n\n\n\n\n\n\nThu, Feb 27\nMultinomial Logistic Regression\n\n\n\nHW 2 Due\nâ Proposal Due\n\n\n8\nTue, Mar 4\nDiscriminant Analysis\nð\nð¥ï¸\n\n\n\n\n\n\nThu, Mar 6\nMidterm Presentation I\n\n\n\nâï¸ HW3\nâ Materials Due\n\n\n9\nTue, Mar 11\nNO CLASS: Spring break\n\n\n\n\n\n\n\n\nThu, Mar 13\nNO CLASS: Spring break Deep Learning Workshop (Fri, Mar 14)\n\n\n\n\n\n\n\n10\nTue, Mar 18\nNaive Bayes\nð\n\n\n\n\n\n\n\nThu, Mar 20\nK-Nearest Neighbors\n\nð¥ï¸\n\n\n\n\n\n11\nTue, Mar 25\nGaussian Process Regression\nð\nð¥ï¸\n\n\n\n\n\n\nThu, Mar 27\nGaussian Process Classification\n\n\n\n\n\n\n\n12\nTue, Apr 1\nSupport Vector Machine\nð\nð¥ï¸\n\n\n\n\n\n\nThu, Apr 3\nSupport Vector Machine\n\n\n\nHW 3 Due\n\n\n\n13\nTue, Apr 8\nCART and Bagging\nð\nð¥ï¸\n\n\nProposal-2 Due\n\n\n\nThu, Apr 10\nRandom Forests and Boosting\n\n\n\nâï¸ HW4\n\n\n\n14\nTue, Apr 15\nMidterm Presentation II\nð\n\n\n\nâ Project-2 Guideline\n\n\n\nThu, Apr 17\nNO CLASS: Easter break\n\n\n\n\n\n\n\n15\nTue, Apr 22\nPrincipal Component Analysis\nð\nð¥ï¸\n\n\n\n\n\n\nThu, Apr 24\nK-Means Clustering\n\nð¥ï¸\n\n\n\n\n\n16\nTue, Apr 29\nModel-based Clustering\nð\n\n\n\n\n\n\n\nThu, May 1\nNeural Networks\n\n\n\nHW 4 Due\n\n\n\n17\nThu, May 8\nFinal Project Submission\n\n\n\n\nâ Final project guideline\n\n\n\n\nI reserve the right to make changes to the schedule.",
    "crumbs": [
      "Course Information",
      "Schedule"
    ]
  },
  {
    "objectID": "slides/04-lin-reg.html#linear-regression",
    "href": "slides/04-lin-reg.html#linear-regression",
    "title": "Linear Regression ð",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nLinear regression is an approach to supervised learning.\nIt assumes that the dependence of \\(Y\\) on \\(X_1, X_2, \\dots, X_k\\)1 is linear.\nTrue regression functions \\(f(x)\\) are never linear!\nAlthough it is overly simplistic, the linear model has distinct advantages in terms of its interpretability and often shows good predictive performance.\n\nlinear regression is extremely useful both conceptually and practically\nThe textbook ISL uses \\(p\\) instead of \\(k\\) to denote the number of predictors in the model. We use \\(k\\) here, but switch to \\(p\\) starting next week."
  },
  {
    "objectID": "slides/04-lin-reg.html#why-multiple-linear-regression-mlr-model",
    "href": "slides/04-lin-reg.html#why-multiple-linear-regression-mlr-model",
    "title": "Linear Regression ð",
    "section": "Why Multiple Linear Regression (MLR) Model",
    "text": "Why Multiple Linear Regression (MLR) Model\n\nOur target response may be affected by several factors.\nTotal sales \\((Y)\\) and amount of money spent on advertising on YouTube (YT) \\((X_1)\\), Facebook (FB) \\((X_2)\\), Instagram (IG) \\((X_3)\\).1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredict sales based on the three advertising expenditures and see which medium is more effective.\nIn ISL, the predictors are TV, radio and newspaper."
  },
  {
    "objectID": "slides/04-lin-reg.html#fit-separate-simple-linear-regression-slr-models",
    "href": "slides/04-lin-reg.html#fit-separate-simple-linear-regression-slr-models",
    "title": "Linear Regression ð",
    "section": "Fit Separate Simple Linear Regression (SLR) Models",
    "text": "Fit Separate Simple Linear Regression (SLR) Models\n\n\n\nâ Fitting a separate SLR model for each predictor is not satisfactory."
  },
  {
    "objectID": "slides/04-lin-reg.html#dont-fit-a-separate-simple-linear-regression",
    "href": "slides/04-lin-reg.html#dont-fit-a-separate-simple-linear-regression",
    "title": "Linear Regression ð",
    "section": "Donât Fit a Separate Simple Linear Regression",
    "text": "Donât Fit a Separate Simple Linear Regression\n\nð How to make a single prediction of sales given levels of the 3 advertising media budgets?\n\n How to predict the sales when the amount spent on YT is 50, 100 on FB and 30 on IG? \n\n\n\n\n\nð Each regression equation ignores the other 2 media in forming coefficient estimates.\n\n The effect of FB advertising on sales may be increased or decreased when YT and IG advertising are in the model. \n IG advertising may have no impact on sales when YT and FB advertising are in the model. \n\n\n\n\n\n\nðð Better approach: extend the SLR model so that it can directly accommodate multiple predictors."
  },
  {
    "objectID": "slides/04-lin-reg.html#multiple-linear-regression-model",
    "href": "slides/04-lin-reg.html#multiple-linear-regression-model",
    "title": "Linear Regression ð",
    "section": "Multiple Linear Regression Model",
    "text": "Multiple Linear Regression Model\n\nThe population MLR model: \\[Y_i= \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\dots + \\beta_kX_{ik} + \\epsilon_i\\]  \n\nIn the advertising example, \\(k = 3\\) and \\[\\texttt{sales} = \\beta_0 + \\beta_1 \\times \\texttt{YouTube} + \\beta_2 \\times  \\texttt{Facebook} + \\beta_3 \\times \\texttt{Instagram} + \\epsilon\\]\n\nModel assumptions:\n\n\\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\)\n\n\n\n\n\nHow many parameters are there in the model?"
  },
  {
    "objectID": "slides/04-lin-reg.html#sample-mlr-model",
    "href": "slides/04-lin-reg.html#sample-mlr-model",
    "title": "Linear Regression ð",
    "section": "Sample MLR Model",
    "text": "Sample MLR Model\n\nGiven the training sample \\((x_{11}, \\dots, x_{1k}, y_1), (x_{21}, \\dots, x_{2k}, y_2), \\dots, (x_{n1}, \\dots, x_{nk}, y_n),\\)\n\nThe sample MLR model to be trained: \\[\\begin{align*}\ny_i &= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_k x_{ik} + \\epsilon_i \\\\\n&= \\beta_0 + \\sum_{j=1}^k\\beta_j x_{ij} + \\epsilon_i, \\quad i = 1, 2, \\dots, n\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/04-lin-reg.html#regression-hyperplane",
    "href": "slides/04-lin-reg.html#regression-hyperplane",
    "title": "Linear Regression ð",
    "section": "Regression Hyperplane",
    "text": "Regression Hyperplane\n\n\nSLR: regression line\nMLR: regression hyperplane or response surface\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_{2}x_2 + \\epsilon\\)\n\\(E(y \\mid x_1, x_2) = 50 + 10x_1 + 7x_2\\)"
  },
  {
    "objectID": "slides/04-lin-reg.html#response-surface",
    "href": "slides/04-lin-reg.html#response-surface",
    "title": "Linear Regression ð",
    "section": "Response Surface",
    "text": "Response Surface\n\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_{2}x_2 + \\beta_{11}x_1^2 + \\beta_{22}x_2^2 + \\beta_{12}x_1x_2 + \\epsilon\\)\n\\(E(y \\mid x_1, x_2) = 800+10x_1+7x_2 -8.5x_1^2-5x_2^2- 4x_1x_2\\)\nð ð¤ A linear regression model can describe a complex nonlinear relationship between the response and predictors!"
  },
  {
    "objectID": "slides/04-lin-reg.html#ordinary-least-squares-estimation-of-the-coefficients",
    "href": "slides/04-lin-reg.html#ordinary-least-squares-estimation-of-the-coefficients",
    "title": "Linear Regression ð",
    "section": "Ordinary Least Squares Estimation of the Coefficients",
    "text": "Ordinary Least Squares Estimation of the Coefficients\n\\[\\begin{align*}\ny_i &= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_k x_{ik} + \\epsilon_i \\\\\n&= \\beta_0 + \\sum_{j=1}^k\\beta_j x_{ij} + \\epsilon_i, \\quad i = 1, 2, \\dots, n\n\\end{align*}\\]\n\nThe least-squares function Sum of Squared Residuals (\\(SS_{res}\\))1 is \\[SS_{res}(\\alpha_0, \\alpha_1, \\dots, \\alpha_k) = \\sum_{i=1}^n\\left(y_i - \\alpha_0 - \\sum_{j=1}^k\\alpha_j x_{ij}\\right)^2\\]\n\n\n\\(SS_{res}\\) must be minimized with respect to the coefficients, i.e., \\[(b_0, b_1, \\dots, b_k) = \\underset{{\\alpha_0, \\alpha_1, \\dots, \\alpha_k}}{\\mathrm{arg \\, min}}  SS_{res}(\\alpha_0, \\alpha_1, \\dots, \\alpha_k)\\]\n\nIn ISL, RSS is used for Residual Sum of Squares."
  },
  {
    "objectID": "slides/04-lin-reg.html#geometry-of-least-squares-estimation",
    "href": "slides/04-lin-reg.html#geometry-of-least-squares-estimation",
    "title": "Linear Regression ð",
    "section": "Geometry of Least Squares Estimation",
    "text": "Geometry of Least Squares Estimation"
  },
  {
    "objectID": "slides/04-lin-reg.html#least-squares-normal-equations",
    "href": "slides/04-lin-reg.html#least-squares-normal-equations",
    "title": "Linear Regression ð",
    "section": "Least-squares Normal Equations",
    "text": "Least-squares Normal Equations\n\\[\\begin{align*}\n\\left.\\frac{\\partial SS_{res}}{\\partial\\alpha_0}\\right\\vert_{b_0, b_1, \\dots, b_k} &= -2 \\sum_{i=1}^n\\left(y_i - \\alpha_0 - \\sum_{j=1}^k \\alpha_j x_{ij}\\right) = 0\\\\\n\\left.\\frac{\\partial SS_{res}}{\\partial\\alpha_j}\\right\\vert_{b_0, b_1, \\dots, b_k} &= -2 \\sum_{i=1}^n\\left(y_i - \\alpha_0 - \\sum_{j=1}^k \\alpha_j x_{ij}\\right)x_{ij} = 0, \\quad j = 1, 2, \\dots, k\n\\end{align*}\\]\n\n\\(k + 1\\) equations with \\(k + 1\\) unknown parameters.\nThe ordinary least squares estimators are the solutions to the normal equations.\n\n\n\nð¹ ðº ð¸ ð¥ I buy you a drink if you solve the equations by hand without using matrix notations or operations!"
  },
  {
    "objectID": "slides/04-lin-reg.html#interpreting-coefficients",
    "href": "slides/04-lin-reg.html#interpreting-coefficients",
    "title": "Linear Regression ð",
    "section": "Interpreting Coefficients",
    "text": "Interpreting Coefficients\n\n\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    2.939      0.312   9.422     0.00\nyoutube        0.046      0.001  32.809     0.00\nfacebook       0.189      0.009  21.893     0.00\ninstagram     -0.001      0.006  -0.177     0.86\n\n\n\n\\[\\hat{y} = b_0 + b_1 x_1 + \\cdots + b_kx_k\\]\n\\[\\widehat{\\texttt{sales}} = 2.939 + 0.046 \\times \\texttt{YouTube} + 0.189 \\times  \\texttt{Facebook} - 0.001 \\times \\texttt{Instagram}\\]\n\n\n\\(b_1\\): Holding all other predictors fixed, for one unit increase of Youtube, the sales is expected to be increased, on average, by 0.046 units.\n\n\\(b_2\\): All else held constant, one unit increase of Facebook leads to, on average, 0.189 unit increase of sales.\n\n\\(b_0\\): The sales with no expenditures on Youtube, Facebook, and Instagram is expected to be 2.939. (Make sense?!)"
  },
  {
    "objectID": "slides/04-lin-reg.html#inference-on-coefficients",
    "href": "slides/04-lin-reg.html#inference-on-coefficients",
    "title": "Linear Regression ð",
    "section": "Inference on Coefficients",
    "text": "Inference on Coefficients\n\n\n\n\n\nThe \\((1-\\alpha)100\\%\\) Wald confidence interval (CI) for \\(\\beta_j\\), \\(j = 0, 1, \\dots, k\\) is\n\n\\[\\left(b_j- t_{\\alpha/2, n-p}~se(b_j), \\quad b_j + t_{\\alpha/2, n-p}~ se(b_j)\\right)\\]\n\n\n\n            2.5 % 97.5 %\n(Intercept)  2.32   3.55\nyoutube      0.04   0.05\nfacebook     0.17   0.21\ninstagram   -0.01   0.01\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nThese are marginal CIs seperately for each \\(b_j\\).\nIndividual CI ignores the correlation between \\(b_j\\)s.\nBetter to consider elliptically-shaped regions."
  },
  {
    "objectID": "slides/04-lin-reg.html#collinearity",
    "href": "slides/04-lin-reg.html#collinearity",
    "title": "Linear Regression ð",
    "section": "Collinearity",
    "text": "Collinearity\n\nInterpretation makes sense when predictors are not highly correlated.\n\nCorrelations amongst predictors cause problems (Collinearity).\n\n\nLarge variance of coefficients.\n\nLarge magnitude of coefficients.\n\nInstable and wrong signed coefficients.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThink about the standard error size. Do we tend to conclude \\(\\beta_j = 0\\) or not?"
  },
  {
    "objectID": "slides/04-lin-reg.html#extrapolation",
    "href": "slides/04-lin-reg.html#extrapolation",
    "title": "Linear Regression ð",
    "section": "Extrapolation",
    "text": "Extrapolation\n\nRegression models are intended as interpolation equations over the range of the regressors used to fit the model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDo not use regression for time series forecasting!"
  },
  {
    "objectID": "slides/04-lin-reg.html#regression-is-not-for-causal-inference",
    "href": "slides/04-lin-reg.html#regression-is-not-for-causal-inference",
    "title": "Linear Regression ð",
    "section": "Regression is Not for Causal Inference",
    "text": "Regression is Not for Causal Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClaims of causality should be avoided for observational data."
  },
  {
    "objectID": "slides/04-lin-reg.html#practical-significance-vs.-statisical-significance",
    "href": "slides/04-lin-reg.html#practical-significance-vs.-statisical-significance",
    "title": "Linear Regression ð",
    "section": "Practical Significance vs.Â Statisical Significance",
    "text": "Practical Significance vs.Â Statisical Significance\n\n\n\n\n\n\nImportant\n\n\n\n\n\n\\(H_0:\\beta_j = 0\\) will always be rejected as long as the sample size is large enough, even \\(x_j\\) has a very small effect on \\(y\\).\n\nConsider the practical significance of the result, not just the statistical significance.\nUse confidence intervals to draw conclusions instead of relying only on p-values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nIf the sample size is small, there may not be enough evidence to reject \\(H_0:\\beta_j = 0\\).\n\n\nDo Not immediately conclude that the variable has no association with the response.\nThere may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association.\n\n\n\n\n\n\n\np-value is a dichotomous approach"
  },
  {
    "objectID": "slides/04-lin-reg.html#regression-model-in-matrix-form",
    "href": "slides/04-lin-reg.html#regression-model-in-matrix-form",
    "title": "Linear Regression ð",
    "section": "Regression Model in Matrix Form",
    "text": "Regression Model in Matrix Form\n\\[y_i= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_kx_{ik} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2), \\quad i = 1, 2, \\dots, n\\]\n\n\n\\[{\\bf y} = {\\bf X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\] where\n\\[\\begin{align}\n\\bf y = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n \\end{bmatrix},\\quad\n\\bf X = \\begin{bmatrix}\n  1 & x_{11} & x_{12} & \\cdots & x_{1k} \\\\\n  1 & x_{21} & x_{22} & \\cdots & x_{2k} \\\\\n  \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  1 & x_{n1} & x_{n2} & \\cdots & x_{nk} \\end{bmatrix}, \\quad\n\\boldsymbol{\\beta} = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_k \\end{bmatrix} , \\quad\n\\boldsymbol{\\epsilon} = \\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\vdots \\\\\n\\epsilon_n\\end{bmatrix}\n\\end{align}\\]\n\n\n\\({\\bf X}_{n \\times p}\\): Design matrix\n\n\\(\\boldsymbol{\\epsilon} \\sim MVN_n({\\bf 0}, \\sigma^2 {\\bf I}_n)\\)1\n\n\n\nFor simplicity and convenience, \\(N({\\bf a}, {\\bf B})\\) represents a multivariate normal distribution with mean vector \\({\\bf a}\\) and covariance matrix \\({\\bf B}\\)."
  },
  {
    "objectID": "slides/04-lin-reg.html#least-squares-estimation-in-matrix-form",
    "href": "slides/04-lin-reg.html#least-squares-estimation-in-matrix-form",
    "title": "Linear Regression ð",
    "section": "Least Squares Estimation in Matrix Form",
    "text": "Least Squares Estimation in Matrix Form\n\n\\[\\begin{align}\nS(\\boldsymbol{\\beta}) = \\sum_{i=1}^n\\epsilon_i^2 = \\boldsymbol{\\epsilon}'\\boldsymbol{\\epsilon} &= ({\\bf y} - {\\bf X} \\boldsymbol{\\beta})'({\\bf y} - {\\bf X} \\boldsymbol{\\beta}) \\\\\n&={\\bf y}'{\\bf y} - \\boldsymbol{\\beta}'{\\bf X}'{\\bf y} - {\\bf y}'{\\bf X} \\boldsymbol{\\beta} + \\boldsymbol{\\beta}' {\\bf X}' {\\bf X} \\boldsymbol{\\beta} \\\\\n&={\\bf y}'{\\bf y} - 2\\boldsymbol{\\beta}'{\\bf X}'{\\bf y} + \\boldsymbol{\\beta}' {\\bf X}' {\\bf X} \\boldsymbol{\\beta}\n\\end{align}\\]\n\n\nLet \\({\\bf t}\\) and \\({\\bf a}\\) be \\(n \\times 1\\) column vectors, and \\({\\bf A}_{n \\times n}\\) is a symmetric matrix.\n\n\\(\\frac{\\partial {\\bf t}'{\\bf a} }{\\partial {\\bf t}} = \\frac{\\partial {\\bf a}'{\\bf t} }{\\partial {\\bf t}} = {\\bf a}\\)\n\\(\\frac{\\partial {\\bf t}'{\\bf A} {\\bf t}}{\\partial {\\bf t}} = 2{\\bf A} {\\bf t}\\)\n\n\n\nNormal equations: \\(\\begin{align} \\left.\\frac{\\partial S}{\\partial \\boldsymbol{\\beta}}\\right \\vert_{\\bf b} = -2 {\\bf X}' {\\bf y} + 2 {\\bf X}' {\\bf X} {\\bf b} = \\boldsymbol{0} \\end{align}\\)\n\n\n\n\nLSE for \\(\\boldsymbol{\\beta}\\):  \\(\\begin{align} \\boxed{{\\bf b} = \\arg \\min _{\\boldsymbol{\\beta}} ({\\bf y} - {\\bf X} \\boldsymbol{\\beta})'({\\bf y} - {\\bf X} \\boldsymbol{\\beta}) = ({\\bf X}' {\\bf X})^{-1} {\\bf X}' \\bf y} \\end{align}\\)  provided that \\({\\bf X}' {\\bf X}\\) is full rank."
  },
  {
    "objectID": "slides/04-lin-reg.html#normal-equations",
    "href": "slides/04-lin-reg.html#normal-equations",
    "title": "Linear Regression ð",
    "section": "Normal Equations",
    "text": "Normal Equations\n\\(({\\bf X}' {\\bf X}) {\\bf b} = {\\bf X}' {\\bf y}\\)"
  },
  {
    "objectID": "slides/04-lin-reg.html#hat-matrix",
    "href": "slides/04-lin-reg.html#hat-matrix",
    "title": "Linear Regression ð",
    "section": "Hat Matrix",
    "text": "Hat Matrix\n\\[\\hat{\\bf y} = {\\bf X} {\\bf b} = {\\bf X} ({\\bf X}' {\\bf X}) ^{-1} {\\bf X}' {\\bf y} = {\\bf H} {\\bf y}\\]\n\nThe hat matrix \\({\\bf H}_{n \\times n} = {\\bf X} ({\\bf X}' {\\bf X}) ^{-1} {\\bf X}'\\)\nThe vector of residuals \\(e_i = y_i - \\hat{y}_i\\) is \\[{\\bf e} = {\\bf y} - \\hat{\\bf y} = {\\bf y} - {\\bf X} {\\bf b} = {\\bf y} - {\\bf H} {\\bf y} = ({\\bf I} - {\\bf H}) {\\bf y}\\]\n\n\n\n\nBoth \\({\\bf H}\\) and \\({\\bf I} - {\\bf H}\\) are symmetric and idempotent. They are projection matrices.\n\\(\\bf H\\) projects \\(\\bf y\\) to \\(\\hat{\\bf y}\\) on the \\((k+1)\\)-dimensional space spanned by columns of \\(\\bf X\\), or the column space of \\(\\bf X\\), \\(Col({\\bf X})\\).\n\\({\\bf I} - {\\bf H}\\) projects \\(\\bf y\\) to \\(\\bf e\\) on the space perpendicular to \\(Col({\\bf X})\\), or \\(Col({\\bf X})^{\\bot}\\)."
  },
  {
    "objectID": "slides/04-lin-reg.html#geometrical-interpretation-of-least-squares",
    "href": "slides/04-lin-reg.html#geometrical-interpretation-of-least-squares",
    "title": "Linear Regression ð",
    "section": "Geometrical Interpretation of Least Squares",
    "text": "Geometrical Interpretation of Least Squares\n\n\n\n\\(Col({\\bf X}) = \\{ {\\bf Xb}: {\\bf b} \\in {\\bf R}^{k+1} \\}\\)\n\\({\\bf y} \\notin Col({\\bf X})\\)\n\\(\\hat{{\\bf y}} = {\\bf Xb} = {\\bf H} {\\bf y} \\in Col({\\bf X})\\)\n\\(\\small {\\bf e} = ({\\bf y} - \\hat{{\\bf y}}) = ({\\bf y} - {\\bf X b}) = ({\\bf I} - {\\bf H}) {\\bf y} \\perp Col({\\bf X})\\)\n\\({\\bf X}'{\\bf e} = 0\\)\n\n\nSearching for \\(\\bf b\\) that minimizes \\(SS_{res}\\) is equivalent to locating the point \\({\\bf Xb} \\in Col({\\bf X})\\) (C) that is as close to \\(\\bf y\\) (A) as possible!\n\n\n\n\n\n\nhttps://commons.wikimedia.org/wiki/File:OLS_geometric_interpretation.svg\n\n\n\n\n\n\nMinimize the distance of \\(\\color{red}{A}\\) to \\(Col(\\bf X)\\): Find the point in \\(Col(\\bf X)\\) that is closest to \\(A\\). The distance is minimized when the point in the space is the foot of the line from \\(A\\) normal to the space. This is point \\(C\\)."
  },
  {
    "objectID": "slides/04-lin-reg.html#multivariate-gaussiannormal-distribution",
    "href": "slides/04-lin-reg.html#multivariate-gaussiannormal-distribution",
    "title": "Linear Regression ð",
    "section": "Multivariate Gaussian/Normal Distribution",
    "text": "Multivariate Gaussian/Normal Distribution\n\n\n\\({\\bf y} \\sim N_n(\\boldsymbol \\mu, {\\bf \\Sigma})\\)\n\n\n\\(\\boldsymbol \\mu\\): \\(n \\times 1\\) mean vector\n\n\\({\\bf \\Sigma}\\): \\(n \\times n\\) covariance matrix\n\n\n\n\\(\\boldsymbol \\mu= (2, 1)'\\); \\({\\bf \\Sigma} = \\begin{pmatrix} 2 & -1 \\\\ -1 & 2\\end{pmatrix}\\)\n\n\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "slides/04-lin-reg.html#sampling-distribution-of-bf-b",
    "href": "slides/04-lin-reg.html#sampling-distribution-of-bf-b",
    "title": "Linear Regression ð",
    "section": "Sampling Distribution of \\({\\bf b}\\)\n",
    "text": "Sampling Distribution of \\({\\bf b}\\)\n\n\n\\({\\bf y} \\sim N(\\boldsymbol \\mu, {\\bf \\Sigma})\\), and \\({\\bf Z} = {\\bf By} + {\\bf c}\\) with a constant matrix \\({\\bf B}\\) and vector \\(\\bf c\\), then \\[{\\bf Z} \\sim N({\\bf B\\boldsymbol \\mu}, {\\bf B \\Sigma B}')\\]\n\n\\({\\bf b} = ({\\bf X}' {\\bf X}) ^{-1} {\\bf X}' \\bf y\\)\n\\[\\textbf{b} \\sim N \\left( \\boldsymbol \\beta, \\sigma^2 ( {\\bf X}' {\\bf X})^{-1}  \\right)\\] \\[E(\\textbf{b}) = E\\left[ ({\\bf X}' {\\bf X})^{-1} {\\bf X}' {\\bf y}\\right] = \\boldsymbol \\beta\\] \\[\\mathrm{Var}(\\textbf{b}) = \\mathrm{Var}\\left[({\\bf X}' {\\bf X})^{-1} {\\bf X}' {\\bf y} \\right] = \\sigma^2 ({\\bf X}' {\\bf X})^{-1}\\]\n\nThe unknown \\(\\sigma^2\\) is estimated by \\(\\hat{\\sigma}^2 = MS_{res} = \\frac{SS_{res}}{n - k - 1}\\).\n\\(\\textbf{b}\\) is Best Linear Unbiased Estimator (BLUE) (Gauss-Markov Theorem).\n\nThe standard error of \\(b_j\\) is \\({\\sqrt{s^2C_{jj}}}\\), where \\(C_{jj}\\) is the diagonal element of \\(({\\bf X'X})^{-1}\\) corresponding to \\(b_j\\)."
  },
  {
    "objectID": "slides/04-lin-reg.html#what-this-review-is-not-covered",
    "href": "slides/04-lin-reg.html#what-this-review-is-not-covered",
    "title": "Linear Regression ð",
    "section": "What This Review is Not Covered",
    "text": "What This Review is Not Covered\n\nDetailed statistical inference\nRegression Diagnostics (Usual Data: Outliers, leverage points, influential points; Non-normality; Non-constant variance; Non-linearity)\nCategorical variables\nModel/Variable Selection\nCode for doing regression (lm() in R and linear_model.LinearRegression() in sklearn of Python)\nMaximum likelihood estimation\n\n\nWhere to learn these stuff?\n\nDr.Â Yuâs MSSC 5780 slides https://math4780-f23.github.io/website/\nISL Ch 3, 6.1."
  },
  {
    "objectID": "slides/04-lin-reg.html#generalizations-of-the-linear-model",
    "href": "slides/04-lin-reg.html#generalizations-of-the-linear-model",
    "title": "Linear Regression ð",
    "section": "Generalizations of the Linear Model",
    "text": "Generalizations of the Linear Model\n\nClassification: logistic regression, support vector machines\nNon-linearity: kernel smoothing, splines and generalized additive models, nearest neighbor methods.\nInteractions: Tree-based methods, bagging, random forests and boosting\nShrinkage and Regularization: Ridge regression and LASSO"
  },
  {
    "objectID": "slides/04-lin-reg.html#basic-concepts",
    "href": "slides/04-lin-reg.html#basic-concepts",
    "title": "Linear Regression ð",
    "section": "Basic Concepts",
    "text": "Basic Concepts\n\nAlthough we have the closed form for \\({\\bf b} = ({\\bf X}' {\\bf X}) ^ {-1} {\\bf X}' {\\bf y}\\), we can solve for \\(\\bf b\\) numerically.\n\n\\[\\begin{align}\n\\underset{\\boldsymbol \\beta}{\\text{min}} \\quad \\ell(\\boldsymbol \\beta) = \\frac{1}{n} \\sum_i (y_i - \\beta_0 - x_i\\beta_1)^2 \\\\\n\\end{align}\\]\n\n\n\\(\\ell(\\boldsymbol \\beta)\\), the \\(\\text{MSE}_{\\texttt{Tr}}\\), is called the (squared) loss function.\n\n\n# generate data from a simple linear regression beta0 = 0.5, beta1 = 1\nset.seed(2025)\nn &lt;- 1000\nx &lt;- rnorm(n)\ny &lt;- 0.5 + x + rnorm(n)\n\n\nStart with an initial value of \\(\\boldsymbol \\beta\\), say \\(\\widehat{\\boldsymbol \\beta} = (0.3, 1.5)\\), we compute the \\(\\text{MSE}_{\\texttt{Tr}}\\)\n\n\n\\[\\begin{align}\n\\frac{1}{n}\\sum_i \\left( y_i - 0.3 - 1.5 x_i \\right)^2 \\\\\n\\end{align}\\]\nLetâs first generate a set of data. We have two parameters, an intercept \\(\\beta_= 0.5\\) and a slope \\(\\beta_1 = 1\\)."
  },
  {
    "objectID": "slides/04-lin-reg.html#basic-concepts-1",
    "href": "slides/04-lin-reg.html#basic-concepts-1",
    "title": "Linear Regression ð",
    "section": "Basic Concepts",
    "text": "Basic Concepts\n\n# calculate the residual sum of squares for a grid of beta values\nmse &lt;- function(b, trainx, trainy) mean((trainy - b[1] - trainx * b[2]) ^ 2)\nmse(b = c(0.3, 1.5), trainx = x, trainy = y)\n\n[1] 1.354548\n\n\n\nThe initial point \\((0.3, 1.5)\\) (red) is not at the bottom of the surface.\n\n\n\n\n\n\n\nDoing this on all such Î² values would allow us to create a surface of the RSS, as a function of the parameters. - Our goal is to minimize the \\(RR_{res}\\), knowing the corresponding \\(\\boldsymbol \\beta\\) values. Numerical optimization is a research field that investigates such problems and their properties."
  },
  {
    "objectID": "slides/04-lin-reg.html#optim",
    "href": "slides/04-lin-reg.html#optim",
    "title": "Linear Regression ð",
    "section": "\noptim()1\n",
    "text": "optim()1\n\n\n(lm_optim &lt;- optim(par = c(0.3, 1.5), fn = mse, trainx = x, trainy = y))\n\n$par\n[1] 0.4571491 0.9723691\n\n$value\n[1] 1.055419\n\n$counts\nfunction gradient \n      53       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\n\nlm(y ~ x)$coef\n\n(Intercept)           x \n  0.4572794   0.9723262 \n\n\n\nThe par argument specifies an initial value. In this case, it is \\(\\beta_0 = \\beta_1 = 2\\).\nThe fn argument specifies the name of a function (mse in this case) that can calculate the objective function. This function may have multiple arguments. However, the first argument has to be the parameter(s) that is being optimized. In addition, the parameters need to be supplied to the function as a vector, but not matrix, list or other formats.\n\nThe arguments trainx = x, trainy = y specifies any additional arguments that the objective function fn (mse) needs. It behaves the same as if you are supplying this to the function mse it self.\n\n\n\n\nscipy.optimize for Python."
  },
  {
    "objectID": "slides/04-lin-reg.html#basic-principles",
    "href": "slides/04-lin-reg.html#basic-principles",
    "title": "Linear Regression ð",
    "section": "Basic Principles",
    "text": "Basic Principles\nFor a general function \\(f(\\mathbf{x})\\) to be minimized with respect to (w.r.t.) \\(\\mathbf{x}\\in \\mathbf{R}^{p}\\), we have\n\n\nFirst-Order Necessary Condition:\n\n\nIf \\(f\\) is continuously differentiable in an open neighborhood of local minimum \\(\\mathbf{x}^\\ast\\), then \\(\\nabla f(\\mathbf{x}^\\ast) = \\mathbf{0}\\)."
  },
  {
    "objectID": "slides/04-lin-reg.html#taylor-expansion",
    "href": "slides/04-lin-reg.html#taylor-expansion",
    "title": "Linear Regression ð",
    "section": "Taylor Expansion",
    "text": "Taylor Expansion\n\\[f(\\mathbf{x}^\\text{new}) \\approx f(\\mathbf{x}) + \\nabla f(\\mathbf{x})' (\\mathbf{x}^\\text{new} - \\mathbf{x})\\]\n\nWhether \\(\\nabla f(\\mathbf{x})\\) is positive or negative, we can always find a new point \\(\\mathbf{x}^\\text{new}\\) that makes \\(\\nabla f(\\mathbf{x})' (\\mathbf{x}^\\text{new} - \\mathbf{x})\\) less than 0, so that \\(f(\\mathbf{x}^\\text{new}) &lt; f(\\mathbf{x})\\).\n\n\n\n\n\\(\\nabla f(\\mathbf{x}^\\ast) = \\mathbf{0}\\) is only a necessary condition, not a sufficient condition.\n\\(x = 1\\) is a local minimizer, not a global one.\n\nOn the left hand side, we have a convex function, which looks like a bowl. The intuition is that, if \\(f(\\mathbf{x})\\) is a function that is smooth enough, and \\(\\mathbf{x}\\) is a point with \\(\\nabla f(\\mathbf{x}^\\ast) \\neq 0\\), then by the Taylor expansion, we have, for any new point \\(\\mathbf{x}^{new}\\) in the neighborhood of \\(\\mathbf{x}\\), we can approximate its function value - Since we only checked if the slope if âflatâ but didnât care if its facing upward or downward, our condition cannot tell the difference."
  },
  {
    "objectID": "slides/04-lin-reg.html#second-order-property",
    "href": "slides/04-lin-reg.html#second-order-property",
    "title": "Linear Regression ð",
    "section": "Second-order Property",
    "text": "Second-order Property\nSecond-order Sufficient Condition:\n\n\\(f\\) is twice continuously differentiable in an open neighborhood of \\(\\mathbf{x}^\\ast\\). If \\(\\nabla f(\\mathbf{x}^\\ast) = \\mathbf{0}\\) and \\(\\nabla^2 f(\\mathbf{x}^\\ast),\\) the Hessian matrix, is positive definite, i.e., \\[\n\\nabla^2 f(\\mathbf{x}) = \\left(\\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_i \\partial x_j}\\right) = \\mathbf{H}(\\mathbf{x}) \\succeq 0,\n\\] then \\(\\mathbf{x}^\\ast\\) is a strict local minimizer of \\(f\\).\n\n\n\n\\[\\begin{align}\n\\text{Left:}& f_1(x) = x ^ 2; \\qquad \\nabla^2 f_1(x) = 2 \\\\\n\\text{Right:}& f_2(x) = x ^ 4 + 2 x ^ 3 - 5 x ^ 2; \\qquad \\nabla^2 f_2(x) = 12x^2 + 12 x - 10\\\\\n\\end{align}\\]\n\nFor \\(f_1\\), \\(\\mathbf{H}(\\mathbf{x}) \\succeq 0\\), and the solution is a minimizer.\nFor \\(f_2\\), \\(\\nabla^2 f_2(-2.5) = 35\\), \\(\\nabla^2 f_2(0) = -10\\) and \\(\\nabla^2 f_2(1) = 14\\). So \\(x = -2.5\\) and \\(1\\) are local minimizers and \\(0\\) is a local maximizer.\n\n\n\n\\(\\mathbf{H}(\\mathbf{x})\\) is called the Hessian matrix, which will be frequently used in second-order methods. We can easily check this property for our examples: These conditions are sufficient, but again, they only discuss local properties, not global properties."
  },
  {
    "objectID": "slides/04-lin-reg.html#optimization-algorithm",
    "href": "slides/04-lin-reg.html#optimization-algorithm",
    "title": "Linear Regression ð",
    "section": "Optimization Algorithm",
    "text": "Optimization Algorithm\n\n\n1. Start with \\(\\mathbf{x}^{(0)}\\)\n\nFor \\(i = 1, 2, \\dots\\) until convergence, find \\(\\mathbf{x}^{(i)}\\) s.t. \\(f(\\mathbf{x}^{(i)}) &lt; f(\\mathbf{x}^{(i-1)})\\)\n\n\n\n\n\nStopping criterion:\n\nGradient of the objective function: \\(\\lVert \\nabla f(\\mathbf{x}^{(i)}) \\rVert &lt; \\epsilon\\)\n\n(Relative) change of distance: \\(\\frac{\\lVert \\mathbf{x}^{(i)} - \\mathbf{x}^{(i-1)} \\rVert} {\\lVert \\mathbf{x}^{(i-1)}\\rVert}&lt; \\epsilon\\) or \\(\\lVert \\mathbf{x}^{(i)} - \\mathbf{x}^{(i-1)} \\rVert &lt; \\epsilon\\)\n\n(Relative) change of functional value: \\(\\frac{| f(\\mathbf{x}^{(i)}) - f(\\mathbf{x}^{(i-1)})|}{|f(\\mathbf{x}^{(i)})|} &lt; \\epsilon\\) or \\(| f(\\mathbf{x}^{(i)}) - f(\\mathbf{x}^{(i-1)})| &lt; \\epsilon\\)\n\nStop at a pre-specified number of iterations\n\n\n\n\n\nMost optimization algorithms follow the same idea: starting from a point x(0) (which is usually specified by the user) and move to a new point x(1) that improves the objective function value. Repeatedly performing this to get a sequence of points x(0),x(1),x(2),x(3),â¦ until the certain stopping criterion is reached. Most algorithms differ in terms of how to move from the current point x(k) to the next, better target point x(k+1). This may depend on the smoothness or structure of f, constrains on the domain, computational complexity, memory limitation, and many others."
  },
  {
    "objectID": "slides/04-lin-reg.html#bfgs-demo",
    "href": "slides/04-lin-reg.html#bfgs-demo",
    "title": "Linear Regression ð",
    "section": "\nBFGS Demo",
    "text": "BFGS Demo\n\n\n\nf1 &lt;- function(x) x ^ 2\nf2 &lt;- function(x) {\n    x ^ 4 + 2 * x ^ 3 - 5 * x ^ 2\n}\noptim(par = 3, fn = f1, method = \"BFGS\")\n\n$par\n[1] -8.384004e-16\n\n$value\n[1] 1.75742e-29\n\n$counts\nfunction gradient \n       8        3 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\n\noptim(par = 10, fn = f2, method = \"BFGS\")\n\n$par\n[1] 0.9999996\n\n$value\n[1] -2\n\n$counts\nfunction gradient \n      37       12 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\noptim(par = 3, fn = f2, method = \"BFGS\")$par\n\n[1] -2.5"
  },
  {
    "objectID": "slides/04-lin-reg.html#second-order-newtons-method",
    "href": "slides/04-lin-reg.html#second-order-newtons-method",
    "title": "Linear Regression ð",
    "section": "Second-order Newtonâs Method\n",
    "text": "Second-order Newtonâs Method\n\n\nSecond order Taylor expansion at a current point \\(\\mathbf{x}\\):\n\n\\[f(\\mathbf{x}^\\ast) \\approx f(\\mathbf{x}) + \\nabla f(\\mathbf{x})' (\\mathbf{x}^\\ast - \\mathbf{x}) + \\frac{1}{2} (\\mathbf{x}^\\ast - \\mathbf{x})' \\mathbf{H}(\\mathbf{x}) (\\mathbf{x}^\\ast - \\mathbf{x})\\]\n\nTake derivative w.r.t \\(\\mathbf{x}^\\ast\\) on both sides: \\[0 = \\nabla f(\\mathbf{x}^\\ast) = 0 + \\nabla f(\\mathbf{x}) + \\mathbf{H}(\\mathbf{x}) (\\mathbf{x}^\\ast - \\mathbf{x})\\] \\[\\boxed{\\mathbf{x}^{(i+1)} = \\mathbf{x}^{(i)} -  \\mathbf{H}(\\mathbf{x}^{(i)})^{-1} \\nabla f(\\mathbf{x}^{(i)})}\\]\nFor numerical stability, introduce a step size \\(\\delta \\in (0, 1)\\): \\[\\mathbf{x}^{(i+1)} = \\mathbf{x}^{(i)} -  {\\color{red}{\\delta}} \\, \\mathbf{H}(\\mathbf{x}^{(i)})^{-1} \\nabla f(\\mathbf{x}^{(i)})\\]\n\nIf \\(\\mathbf{H}\\) is difficult to compute, we may use some matrix to substitute it. For example, if we simplify use the identity matrix \\(\\mathbf{I}\\), then this reduces to a first-order method to be introduced later. However, if we can get a good approximation, we can still solve this linear system and get to a better point. Then the question is how to obtain such matrix in a computationally inexpensive way. The BroydenâFletcherâGoldfarbâShanno (BFSG) algorithm is such an approach by iteratively updating its (inverse) estimation. For details, please see the SMLR book. We have already used the BFGS method previously in the optim() example. - when x(k+1) is not too far away from x(k), the quadratic approximation is fairly accurate."
  },
  {
    "objectID": "slides/04-lin-reg.html#first-order-gradient-descent",
    "href": "slides/04-lin-reg.html#first-order-gradient-descent",
    "title": "Linear Regression ð",
    "section": "First-Order Gradient Descent\n",
    "text": "First-Order Gradient Descent\n\n\n\n\n\n\n\nNote\n\n\n\nWhen \\(\\mathbf{H}\\) or \\(\\mathbf{H}^{-1}\\) is difficult to compute (usually in deep learning)\n\nGet an approximate one in a computationally inexpensive way. The BFGS algorithm is such an approach by iteratively updating its (inverse) estimation.\nUse first-order methods\n\n\n\n\n\n\nWhen using \\(\\mathbf{H}= \\mathbf{I}\\), we update \\[\\mathbf{x}^{(i+1)} = \\mathbf{x}^{(i)} - \\delta \\nabla f(\\mathbf{x}^{(i)}).\\]\n\nIt is crucial to figure out a good step size \\(\\delta\\), usually \\(&lt;1\\).\n\nA too large \\(\\delta\\) may not converge.\nA too small \\(\\delta\\) takes too many iterations.\n\n\n\nAlternatively, line search could be used."
  },
  {
    "objectID": "slides/04-lin-reg.html#demo-ss_res-contour",
    "href": "slides/04-lin-reg.html#demo-ss_res-contour",
    "title": "Linear Regression ð",
    "section": "\nDemo \\(SS_{res}\\) Contour",
    "text": "Demo \\(SS_{res}\\) Contour\nThe objective function is \\(\\ell(\\boldsymbol \\beta) = \\frac{1}{2n}||\\mathbf{y} - \\mathbf{X} \\boldsymbol \\beta ||^2\\) with solution \\({\\bf b} = \\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1} \\mathbf{X}'\\mathbf{y}\\).\n\nCodeset.seed(2025)\nn &lt;- 200\n\n# create some data with linear model\nX &lt;- MASS::mvrnorm(n, c(0, 0), matrix(c(1, 0.7, 0.7, 1), 2, 2))\ny &lt;- rnorm(n, mean = 2 * X[, 1] + X[, 2])\n  \nbeta1 &lt;- seq(-1, 4, 0.005)\nbeta2 &lt;- seq(-1, 4, 0.005)\nallbeta &lt;- data.matrix(expand.grid(beta1, beta2))\nrss &lt;- matrix(apply(allbeta, 1, \n                    function(b, X, y) sum((y - X %*% b) ^ 2), X, y),\n              length(beta1), length(beta2))\n  \n# quantile levels for drawing contour\nquanlvl = c(0.01, 0.025, 0.05, 0.2, 0.5, 0.75)\n  \n# plot the contour\ncontour(beta1, beta2, rss, levels = quantile(rss, quanlvl), las = 1)\nbox()\n  \n# the truth\nb &lt;- solve(t(X) %*% X) %*% t(X) %*% y\npoints(b[1], b[2], pch = 19, col = \"blue\", cex = 2)"
  },
  {
    "objectID": "slides/04-lin-reg.html#demo-gradient-descent",
    "href": "slides/04-lin-reg.html#demo-gradient-descent",
    "title": "Linear Regression ð",
    "section": "\nDemo Gradient Descent",
    "text": "Demo Gradient Descent\n\\[\n\\begin{align}\n\\frac{\\partial \\ell(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta} = -\\frac{1}{n} \\sum_{i=1}^n (y_i - x_i' \\boldsymbol \\beta) x_i = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial \\ell_i(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta}.\n\\end{align}\n\\] where \\(\\ell_i(\\boldsymbol \\beta) = \\frac{1}{2}(y_i - x_i' \\boldsymbol \\beta)^2\\).\nFirst set an initial beta value, say \\(\\boldsymbol \\beta = \\mathbf{0}\\) for all entries, then proceed with the update\n\\[\\begin{align}\n\\boldsymbol \\beta^\\text{new} =& \\boldsymbol \\beta^\\text{old} - \\delta \\frac{\\partial \\ell(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta}\\\\\n=&\\boldsymbol \\beta^\\text{old} + \\delta \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i' \\boldsymbol \\beta) x_i.\\\\\n\\end{align}\\]"
  },
  {
    "objectID": "slides/04-lin-reg.html#demo-gradient-descent-1",
    "href": "slides/04-lin-reg.html#demo-gradient-descent-1",
    "title": "Linear Regression ð",
    "section": "\nDemo Gradient Descent",
    "text": "Demo Gradient Descent\n\nSet \\(\\boldsymbol{\\beta}^{(0)} = \\mathbf{0}\\), \\(\\delta = 0.2\\)"
  },
  {
    "objectID": "slides/04-lin-reg.html#demo-effect-of-delta",
    "href": "slides/04-lin-reg.html#demo-effect-of-delta",
    "title": "Linear Regression ð",
    "section": "\nDemo Effect of \\(\\delta\\)\n",
    "text": "Demo Effect of \\(\\delta\\)\n\n\nThe descent path is very smooth because we choose a very small step size. However, if the step size is too large, we may observe unstable results or even unable to converge. For example, if We set \\(\\delta = 1\\) or \\(\\delta = 1.5\\)."
  },
  {
    "objectID": "slides/04-lin-reg.html#mini-batch-stochastic-gradient-descent-sgd",
    "href": "slides/04-lin-reg.html#mini-batch-stochastic-gradient-descent-sgd",
    "title": "Linear Regression ð",
    "section": "Mini-batch Stochastic Gradient Descent (SGD)",
    "text": "Mini-batch Stochastic Gradient Descent (SGD)\nIn deep learning,\n\nCalculating the gradient using entire data can be costly (memory limit).\nConsider update the parameter using a subset of data of size \\(m \\ll n\\), called minibatch: \\(\\mathcal{D}_B = \\{x_j, y_j \\}_{j = 1}^m \\subset \\mathcal{D}\\)\n\n\n\\[\n\\begin{align}\n\\frac{\\partial \\ell_B(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta} = -\\frac{1}{m} \\sum_{j=1}^m (y_j - x_j' \\boldsymbol \\beta) x_j = \\frac{1}{m} \\sum_{j=1}^m \\frac{\\partial \\ell_j(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta}\n\\end{align}\n\\] where \\(\\ell_j(\\boldsymbol \\beta) = \\frac{1}{2}(y_j - x_j' \\boldsymbol \\beta)^2\\).\n\\[\\begin{align}\n\\boldsymbol \\beta^\\text{new} =& \\boldsymbol \\beta^\\text{old} - \\delta \\frac{\\partial \\ell_B(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta}\\\\\n=&\\boldsymbol \\beta^\\text{old} + \\delta \\frac{1}{m} \\sum_{j=1}^m (y_j - x_j' \\boldsymbol \\beta) x_j.\\\\\n\\end{align}\\]"
  },
  {
    "objectID": "slides/04-lin-reg.html#sgd",
    "href": "slides/04-lin-reg.html#sgd",
    "title": "Linear Regression ð",
    "section": "SGD",
    "text": "SGD\n\n\\(\\nabla_{\\boldsymbol \\beta} \\ell_B(\\boldsymbol \\beta)\\) approximates \\(\\nabla_{\\boldsymbol \\beta} \\ell(\\boldsymbol \\beta)\\) with stochasticity due to random sampling.\nDifferent samples give us a different gradient value, making it stochastic!\nSuppose we have \\(n=1000\\) training points, and batch size is \\(m=100\\), it needs \\(n/m = 10\\) updates (iterations can be parallelized) to pass all data into the algorithm, which completes one epoch.\n\nTo gaurantee convergence, the learning rate \\(\\delta\\) should\n\nbe decreasing (with epochs or iterations)\nhave \\(\\sum_{k=1}^{\\infty}\\delta_k = \\infty\\) and \\(\\sum_{k=1}^{\\infty}\\delta_k^2 &lt; \\infty\\)\n\n\n\n\nIterations: Decay happens more frequently (every batch). This is more precise and often used for large datasets. Epochs: Decay happens less frequently (after all batches of one epoch). This is simpler and often used for smaller datasets or in step-based schedules."
  },
  {
    "objectID": "slides/04-lin-reg.html#demo-sgd",
    "href": "slides/04-lin-reg.html#demo-sgd",
    "title": "Linear Regression ð",
    "section": "\nDemo SGD",
    "text": "Demo SGD\n\n\n\n\n\n\n\n\n\n\n\n\nmssc6250-s25.github.io/website"
  },
  {
    "objectID": "slides/15-pca.html#unsupervised-learning-1",
    "href": "slides/15-pca.html#unsupervised-learning-1",
    "title": "Dimension Reduction \n",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nSupervised Learning: response \\(Y\\) and features \\(X_1, X_2, \\dots, X_p\\) measured on \\(n\\) observations.\n\nUnsupervised Learning: only features \\(X_1, X_2, \\dots, X_p\\) measured on \\(n\\) observations.\n\nNot interested in prediction (no response to be predicted)\nDiscover any interesting pattern or relationships among these features.\n\n\n\n\n\nEstimate the density, covariance, graph (network), etc. of \\(\\mathbf{X}\\)\n\n\n\n\n\n\nDimension reduction for effective data visualization or extracting most important information those features contain.\n\nplot a bunch of points of \\(\\boldsymbol{x} = (x_1, x_2, \\dots, x_p)\\) in a 2-D scatter plot (manifold). (reduce dimension from \\(p\\) to 2)\n use 2 variables to explain most variations or represents high data density in the \\(p\\) variables \n\n\n\n\n\n\n\nClustering discovers unknown subgroups/clusters in data\n\nfind 3 sub-groups of people based on variables income, occupation, age, etc"
  },
  {
    "objectID": "slides/15-pca.html#one-dimension-1d-number-line",
    "href": "slides/15-pca.html#one-dimension-1d-number-line",
    "title": "Dimension Reduction \n",
    "section": "One-Dimension (1D) Number line",
    "text": "One-Dimension (1D) Number line\n\n\n\n\n# A tibble: 50 Ã 1\n   English\n     &lt;int&gt;\n 1      41\n 2      65\n 3      55\n 4      94\n 5      66\n 6      85\n 7      44\n 8      44\n 9      67\n10      73\n# â¹ 40 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLetâs go back to your first grade. You remember that one-dimension equals a number line.\nNow suppose we have 50 English grades. The first 3 scores are 41, 65, and 55.\nWe can plot these values on the number line just like we did in the elementary school.\n1st student with score 41 itâs a dot at 41, 2nd student with score 65, itâs a dot at 65, and so on."
  },
  {
    "objectID": "slides/15-pca.html#one-dimension-1d-number-line-uniform-students",
    "href": "slides/15-pca.html#one-dimension-1d-number-line-uniform-students",
    "title": "Dimension Reduction \n",
    "section": "One-Dimension (1D) Number line: Uniform students",
    "text": "One-Dimension (1D) Number line: Uniform students\n\n\n\n\n# A tibble: 50 Ã 1\n   English\n     &lt;int&gt;\n 1      41\n 2      65\n 3      55\n 4      94\n 5      66\n 6      85\n 7      44\n 8      44\n 9      67\n10      73\n# â¹ 40 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you plot all students scores, we might see something like this: an uniform distribution of English scores."
  },
  {
    "objectID": "slides/15-pca.html#d-number-line-non-uniform-students",
    "href": "slides/15-pca.html#d-number-line-non-uniform-students",
    "title": "Dimension Reduction \n",
    "section": "1D Number line: Non-uniform students",
    "text": "1D Number line: Non-uniform students\n\n\n\n\n# A tibble: 50 Ã 1\n   English\n     &lt;int&gt;\n 1      77\n 2      78\n 3      81\n 4      78\n 5      52\n 6      62\n 7      47\n 8      58\n 9      43\n10      59\n# â¹ 40 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOr we might get a non-uniform distribution of English scores. Some students obtain a very high grade, but some students are falling behind and get a very low grade."
  },
  {
    "objectID": "slides/15-pca.html#two-dimensions-2d-x-y-scatter-plot-high-correlated",
    "href": "slides/15-pca.html#two-dimensions-2d-x-y-scatter-plot-high-correlated",
    "title": "Dimension Reduction \n",
    "section": "Two-Dimensions (2D) X-Y Scatter plot: High Correlated",
    "text": "Two-Dimensions (2D) X-Y Scatter plot: High Correlated\nEnglish and Math measure an overall academic performance.\n\n\n\n\n# A tibble: 50 Ã 2\n   English  Math\n     &lt;int&gt; &lt;dbl&gt;\n 1      41  33.2\n 2      65  63.6\n 3      55  44.6\n 4      94  95  \n 5      66  65.6\n 6      85  73.1\n 7      44  46.6\n 8      44  51  \n 9      67  69.4\n10      73  66.5\n# â¹ 40 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow letâs go to sixth grade when we learned about 2 dimensional graphs.\nNow we have two axes instead of one, and now we can plot data from two different subjects instead of just one.\nAdditional to English scores, here we also have Math scores of some students.\nAnd we all know how to map a English and math score pair to a point on a 2D plot.\nIf we put all students English and Math scores on the plot, we might see something like this that English and Math scores are positively correlated, meaning that a student having high English score tends to have high Math score too.\nThis might be due to the fact that English and Math measure an overall academic performance. A good student tends to have both high English and Math scores."
  },
  {
    "objectID": "slides/15-pca.html#two-dimensions-2d-x-y-scatter-plot-no-correlated",
    "href": "slides/15-pca.html#two-dimensions-2d-x-y-scatter-plot-no-correlated",
    "title": "Dimension Reduction \n",
    "section": "Two-Dimensions (2D) X-Y Scatter plot: No correlated",
    "text": "Two-Dimensions (2D) X-Y Scatter plot: No correlated\nEnglish and Math measure different abilities.\n\n\n\n\n# A tibble: 50 Ã 2\n   English  Math\n     &lt;int&gt; &lt;int&gt;\n 1      41    27\n 2      65    47\n 3      55    32\n 4      94    44\n 5      66    20\n 6      85    30\n 7      44    16\n 8      44    72\n 9      67    86\n10      73    82\n# â¹ 40 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOr we might see English and Math scores are not correlated, meaning that a high English score does not tell us anything about Math score, whether it is high or low.\nBecause English and Math might measure different abilities.\nEnglish measures verbal and communication skills and Math measures logic and quantitative skills"
  },
  {
    "objectID": "slides/15-pca.html#three-dimensions-3d-x-y-z-scatter-plot",
    "href": "slides/15-pca.html#three-dimensions-3d-x-y-z-scatter-plot",
    "title": "Dimension Reduction \n",
    "section": "Three-Dimensions (3D) X-Y-Z Scatter plot",
    "text": "Three-Dimensions (3D) X-Y-Z Scatter plot\n\n\n\n\n# A tibble: 50 Ã 3\n   English  Math Biology\n     &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1      41  33.2    39.2\n 2      65  63.6    61.6\n 3      55  44.6    41.6\n 4      94  95      92  \n 5      66  65.6    73.6\n 6      85  73.1    71.1\n 7      44  46.6    56.6\n 8      44  51      56  \n 9      67  69.4    79.4\n10      73  66.5    59.5\n# â¹ 40 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOK. Now maybe in college, we started drawing 3D graph, having height, width and depth.\nWith 3 separate axes, we can now plot data from three different subjects.\nSo now we have three subjects, English, Math and Biology.\nWith the same logic as 2D plot, we put English score on X-axis, Math score on Y-axis, and biology score on Z-axis, then draw lines perpendicular to each axis to put the point in the 3D space.\nAnd this is our 3D scatter plot.\nSo, if we have one subject data, we can make a 1D graph.\nIf we have two subject data, we can make a 2D graph.\nIf we have three subject data, we can make a 3D graph."
  },
  {
    "objectID": "slides/15-pca.html#four-dimension-4d-x-y-z--scatter-plot",
    "href": "slides/15-pca.html#four-dimension-4d-x-y-z--scatter-plot",
    "title": "Dimension Reduction \n",
    "section": "Four-Dimension (4D) X-Y-Z-? Scatter plot",
    "text": "Four-Dimension (4D) X-Y-Z-? Scatter plot\n\n\n\n\n# A tibble: 50 Ã 4\n   English  Math Biology History\n     &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1      41  33.2    39.2      51\n 2      65  63.6    61.6      53\n 3      55  44.6    41.6      63\n 4      94  95      92        83\n 5      66  65.6    73.6      51\n 6      85  73.1    71.1      74\n 7      44  46.6    56.6      34\n 8      44  51      56        33\n 9      67  69.4    79.4      76\n10      73  66.5    59.5      74\n# â¹ 40 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut what happens if we have data from 4 academic subjects?? here English, Math, Biology and History.\nWell we need 4D graph. The problem is You canât draw a 4D plot on paper!\nWhat if a student has 20 different grades from 20 different courses?? We need a 20 dimensional graph?\nBut what is that?? There is no way we can draw that."
  },
  {
    "objectID": "slides/15-pca.html#how-about-pair-plots",
    "href": "slides/15-pca.html#how-about-pair-plots",
    "title": "Dimension Reduction \n",
    "section": "How about Pair Plots?",
    "text": "How about Pair Plots?\n\n\nPlotting all 2-D scatter plots of all possible pairs might be a solution to check the relationship between variables, or exploratory data analysis.\nLike here we see basically any two variables or two subject scores are positively correlated."
  },
  {
    "objectID": "slides/15-pca.html#tooooo-many-pair-plots",
    "href": "slides/15-pca.html#tooooo-many-pair-plots",
    "title": "Dimension Reduction \n",
    "section": "Tooooo Many Pair Plots!",
    "text": "Tooooo Many Pair Plots!\n\nIf we have \\(p\\) variables, there are \\({p \\choose 2} = p(p-1)/2\\) pairs.\nIf \\(p = 10\\), we have 45 such scatter plots to look at!\nIn practice, we may encounter over 100 variables!!\n\n\nBut still, there is a problem.\nIf you have \\(p\\) variables, you are gonna have \\({p \\choose 2} = p(p-1)/2\\) pairs.\nIf \\(p = 10\\), you have 45 such scatter plots to look at!\nNot to mention that in real data science work, you may encounter over 100 variables!!"
  },
  {
    "objectID": "slides/15-pca.html#dimension-reduction",
    "href": "slides/15-pca.html#dimension-reduction",
    "title": "Dimension Reduction \n",
    "section": "Dimension Reduction",
    "text": "Dimension Reduction\n\nOne variable represents one dimension.\nWith many variables in the data, we live in a high dimensional world.\n\n\nGOAL:\n\nFind a low-dimensional (usually 2D) representation of the data that captures as much of the information all of those variables provide as possible.\nUse two created variables to represent all \\(p\\) variables, and make a scatter plot of the two created variables to learn what our observations look like as if they lived in the high dimensional space. \n\n\n\n\nWhy and when can we omit dimensions?\n\n\nSo in order to meaningfully represent the relationship of all variables, we need a technique, Dimension Reduction.\nIn mathematics, One variable represents one dimension, so with many variables in the data, we live in a high dimensional world.\nWe would like to find a low-dimensional (usually 2D) representation of the data that captures as much of the information all of those variables provide as possible.\nWe use two created variables to represent all \\(p\\) variables, and make a scatter plot of the two created variables to learn what our observations look like as if they lived in the high dimensional space.\nOf course, itâs not always a good idea to just use two variables to represent all \\(p\\) variables. Some information will be missing when we just use two variables to represent or explain all the relationships of \\(p\\) variables.\nBut sometimes, a low-dimensional representation looks very like a high dimensional space, and does not lose much information. In this situation, a low-dimensional representation is very useful.\nSo Letâs see why and when can we omit dimensions?"
  },
  {
    "objectID": "slides/15-pca.html#variation-mostly-from-one-variable",
    "href": "slides/15-pca.html#variation-mostly-from-one-variable",
    "title": "Dimension Reduction \n",
    "section": "Variation mostly from One Variable",
    "text": "Variation mostly from One Variable\n\nAlmost all of the variation in the data is from left to right.\n\n\n\nLetâs suppose we have two variables, and their data look like this.\nHere we see almost all of the variation in the data from left to right.\nIt means that for variable X, some observations have low values, and some observations have high values.\nBut it looks like all observations have variable Y at about the same level.\nSince X varies more than Y, having larger variation, X contains more information than Y in the data."
  },
  {
    "objectID": "slides/15-pca.html#variation-mostly-from-one-variable-1",
    "href": "slides/15-pca.html#variation-mostly-from-one-variable-1",
    "title": "Dimension Reduction \n",
    "section": "Variation mostly from One Variable",
    "text": "Variation mostly from One Variable\n\nIf we flattened the data, the graph would not look much different.\n\n\n\nIf we flattened the data, removing the up and down variation, our graph would not look much different from what it look like before.\nMost of the variation, or information is still kept in the data."
  },
  {
    "objectID": "slides/15-pca.html#variation-mostly-from-one-variable-2",
    "href": "slides/15-pca.html#variation-mostly-from-one-variable-2",
    "title": "Dimension Reduction \n",
    "section": "Variation mostly from One Variable",
    "text": "Variation mostly from One Variable\n\nIf we flattened the data, we could graph it with a 1D number line!\n\n\n\nNow, with up-and-down variation removed, the variable Y becomes redundant, and we can just represent the flattened data using 1D single number line."
  },
  {
    "objectID": "slides/15-pca.html#variation-mostly-from-one-variable-3",
    "href": "slides/15-pca.html#variation-mostly-from-one-variable-3",
    "title": "Dimension Reduction \n",
    "section": "Variation mostly from One Variable",
    "text": "Variation mostly from One Variable\n\nBoth graphs say âthe important variation is left to right.â\n\n\n\n\nSo in this case, we can display 2D data on a 1D plot without losing too much information!\nBoth graphs say âthe important variation is left to right.â\n\nAnother example is watching TV. TV is a 2D thing, but we watch TV for 3D shows. The 2D TV represents the 3D shows, programs or drama very well because the 2D representation does not lose much information the 3D shows provide.\nSo through the example, we know some dimensions are more important than others. Like here Variable X is more important than variable Y because variable X explains most of the variation stored in the data."
  },
  {
    "objectID": "slides/15-pca.html#idea-of-pca",
    "href": "slides/15-pca.html#idea-of-pca",
    "title": "Dimension Reduction \n",
    "section": "Idea of PCA",
    "text": "Idea of PCA\n\nPCA is a dimension reduction tool that finds a low-dimensional representation of a data set that contains as much as possible of variation.\nEach observation lives in a high-dimensional space (lots of variables), but not all of these dimensions (variables) are equally interesting/important.\nThe concept of interesting/important is measured by the amount that the observations vary along each dimension.\n\n\nPCA is a dimension reduction tool that finds a low-dimensional representation of a data set that contains as much as possible of variation stored in the data set.\nAs weâve seen before, each of the observations lives in a high-dimensional space, meaning that each observation has lots of variables associated with it, but not all of these dimensions (variables) are equally interesting/important.\nThe concept of interesting/important is measured by the amount that the observations vary along each dimension.\nA characteristic or attribute of observations is called variable because its value varies from sample to sample.\nIf the variable does not vary, it becomes an irrelevant or un-important variable because we cannot use the variable to differentiate or distinguish observations. If everyone in this class gets grade A, then the data science grade is not an important variable to learn which students perform academically better than others."
  },
  {
    "objectID": "slides/15-pca.html#pca-illustration-2-variable-example",
    "href": "slides/15-pca.html#pca-illustration-2-variable-example",
    "title": "Dimension Reduction \n",
    "section": "PCA Illustration: 2 Variable Example",
    "text": "PCA Illustration: 2 Variable Example\n\n\n\n\n# A tibble: 50 Ã 2\n   English  Math\n     &lt;int&gt; &lt;dbl&gt;\n 1      41  33.2\n 2      65  63.6\n 3      55  44.6\n 4      94  95  \n 5      66  65.6\n 6      85  73.1\n 7      44  46.6\n 8      44  51  \n 9      67  69.4\n10      73  66.5\n11      47  58.9\n12      66  66.5\n13      57  51  \n14      57  33.2\n15      77  83.6\n16      83  78.8\n# â¹ 34 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe use the English and Math score example to illustrate how PCA works.\nFrom the data, we have a 2D scatter plot like this.\nWith the data, we can have the average of English score and the average of math score, shown as the red squared point in the scatter plot."
  },
  {
    "objectID": "slides/15-pca.html#step-1-shift-or-standardize-the-data",
    "href": "slides/15-pca.html#step-1-shift-or-standardize-the-data",
    "title": "Dimension Reduction \n",
    "section": "Step 1: Shift (or standardize) the Data",
    "text": "Step 1: Shift (or standardize) the Data\n\nSo the two variables have both mean 0. If the variables are measured in a different unit, consider standardization, \\(\\frac{x_i - \\bar{x}}{s_x}\\).\nShifting does not change how the data points are positioned relative to each other.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA first shift or standardize our data so that the center is on top of the origin (0, 0) in the graph.\nIf the two variables are measured in a different scale, standardization is highly recommended. Here because both English and Math scores are measured using the same scale. We only do the shifting, and no scaling.\nAfter standardization, all variables will use the same scale to measure their variation, and that makes more sense because PCA wants to find some axis or coordinate lower dimensional representation that contains the most variation, and we donât want the variation depends on variableâs measurement units.\nNotice that Shifting does not change how the data points are positioned relative to each other. The scattering shape remains the same."
  },
  {
    "objectID": "slides/15-pca.html#step-2-find-a-line-that-fits-the-data-the-best",
    "href": "slides/15-pca.html#step-2-find-a-line-that-fits-the-data-the-best",
    "title": "Dimension Reduction \n",
    "section": "Step 2: Find a Line that Fits the Data the Best",
    "text": "Step 2: Find a Line that Fits the Data the Best\n\nStart with a line going through the origin.\n\nRotate the line until it fits the data as well as it can, given that it goes through the origin.\n\n\n\nThe second step of PCA is going to fit a line to the centered or normalized data set. \n\nTo do this, we start by drawing a random line that goes through the origin."
  },
  {
    "objectID": "slides/15-pca.html#step-2-find-a-line-that-fits-the-data-the-best-1",
    "href": "slides/15-pca.html#step-2-find-a-line-that-fits-the-data-the-best-1",
    "title": "Dimension Reduction \n",
    "section": "Step 2: Find a Line that Fits the Data the Best",
    "text": "Step 2: Find a Line that Fits the Data the Best\n\nStart with a line going through the origin.\n\nRotate the line until it fits the data as well as it can, given that it goes through the origin.\n\n\n\nThen we rotate the line until if fits the data as well as it can, given that it has to go through the origin."
  },
  {
    "objectID": "slides/15-pca.html#step-2-find-a-line-that-fits-the-data-the-best-2",
    "href": "slides/15-pca.html#step-2-find-a-line-that-fits-the-data-the-best-2",
    "title": "Dimension Reduction \n",
    "section": "Step 2: Find a Line that Fits the Data the Best",
    "text": "Step 2: Find a Line that Fits the Data the Best\n\nStart with a line going through the origin.\n\nRotate the line until it fits the data as well as it can, given that it goes through the origin.\n\n\n\nUltimately, this line fits best. \n\nSo letâs see what makes this line the best fit, and the meaning of this line."
  },
  {
    "objectID": "slides/15-pca.html#the-meaning-of-the-best-line",
    "href": "slides/15-pca.html#the-meaning-of-the-best-line",
    "title": "Dimension Reduction \n",
    "section": "The Meaning of the Best line",
    "text": "The Meaning of the Best line\n\n\n\n\nPrincipal Component 1 (PC1): maximizes the variance of the projected points.\n\nPC1 is the line in the Eng-Math space that is closest to the \\(n\\) observations\n\nPC1 minimizes the sum of squared distances between the data points and the PC1.\n\n\n\nPC1 is the best 1D representation of the 2D data\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo quantify how good this line fits the data, PCA projects the data onto it.\nThen the idea is that we can either measure the distances from the data to the line and try to find the line that minimizes those distances.\nOr we can try to find the line that maximizes the distances from the projected points to the origin.\nThe two criteria are equivalent.\nDEMO\nSo we are find the line that maximizes the variation of the projected points from the data points onto the line!\nThe best line is called Principal Component 1 (PC1), which maximizes the sum of squared distances between projected points and the origin.\n\nRegression line: minimizes the sum of squared residuals (vertical lines from the data points to the line)\n\nPrincipal Component 1 (PC1): maximizes the sum of squared distances between between projected points and the origin.\nPC1 is the line in the Eng-Math 2 dimensional space that is closest to the \\(n\\) observations, i.e., PC1 minimizes the sum of squared distances between the data points and the PC1.\nPC1 is the best 1D representation of the 2D data"
  },
  {
    "objectID": "slides/15-pca.html#the-meaning-of-the-best-line-1",
    "href": "slides/15-pca.html#the-meaning-of-the-best-line-1",
    "title": "Dimension Reduction \n",
    "section": "The Meaning of the Best line",
    "text": "The Meaning of the Best line\n\n\n\n\n\nSource: https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\n\n\n\n\n\nI am going to conclude the idea of PCA using this gif.\nThis gif shows you the idea of finding principal components.\nAs the line rotates, you can see the locations of the projected points on the line keeps changing as well.\nAnd the PC1 is the line that maximizes the variation of the projected points.\nAlso, the PC1 is the line that minimizes the distance between the data points and the line. In the figure, the sum of those red lines will be the smallest.\nQuestions?"
  },
  {
    "objectID": "slides/15-pca.html#pc1-and-pc2",
    "href": "slides/15-pca.html#pc1-and-pc2",
    "title": "Dimension Reduction \n",
    "section": "PC1 and PC2",
    "text": "PC1 and PC2\n\n\n\nThe data points are also spread out a little above and below the PC1.\nThere are some variation that is not explained by the PC1.\nFind the second PC, PC2, that\n\nexplains the remaining variation\nis the line through the origin and perpendicular to PC1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlong with the PC1, the data points are also spread out a little above and below the PC1.\nThere are some variation of the two variables that is not explained by the PC1.\nTo find another PC that explains the remaining variation, we find the second PC, called PC2 that is the line through the origin and perpendicular to PC1."
  },
  {
    "objectID": "slides/15-pca.html#linear-combinations",
    "href": "slides/15-pca.html#linear-combinations",
    "title": "Dimension Reduction \n",
    "section": "Linear Combinations",
    "text": "Linear Combinations\n\n\n\n\n\n\n\n\n\n\n\n\nPC1 = 0.68 \\(\\times\\) English \\(+\\) 0.74 \\(\\times\\) Math\nPC2 = 0.74 \\(\\times\\) English \\(-\\) 0.68 \\(\\times\\) Math\nPC1 is like an overall intelligence index as it is a weighted average combining verbal and quantitative abilities.\nPC2 accounts for individual difference in English and Math scores.\n\n\\(0.68^2 + 0.74^2 = 1\\) (Pythagorean theorem)\nThe combination weights 0.68, 0.74, etc are called PC loadings.\n\n\n\n\nNow letâs look at PC1 and PC2 a little more carefully.\nFirst PC1 and PC2 are just a vector in 2 dimensional space, right?\nIn other words, they are linear combinations of two standard basis, here our English axis and Math axis.\nPCA help us get the linear combinations.\nHere PC1 = 0.68 \\(\\times\\) English + 0.74 \\(\\times\\) Math\nPC2 = 0.74 \\(\\times\\) English - 0.68 \\(\\times\\) Math\nSo to make PC1, we mix 0.68 part of English score with 0.74 parts of Math score.\nOne unit of PC1 consists of 0.68 parts of English and 0.74 parts of Math.\nAnd because the weight of math score is a little bit larger, Math score is a little bit more important when it comes to describing how the data are spread out.\nPC1 can be viewed as an overall intelligence index because it is an weighted average combining both verbal and quantitative reasoning abilities.\nPC2 accounts for individual difference in English and Math scores.\n\n\\(0.68^2 + 0.74^2 = 1\\) (Pythagorean theorem)\nThe combination weights 0.68, 0.74, etc are called loadings of PC."
  },
  {
    "objectID": "slides/15-pca.html#rotate-everything-so-that-pc1-is-horizontal",
    "href": "slides/15-pca.html#rotate-everything-so-that-pc1-is-horizontal",
    "title": "Dimension Reduction \n",
    "section": "Rotate Everything so that PC1 is Horizontal",
    "text": "Rotate Everything so that PC1 is Horizontal\n\n\n1D representation\n\nPC1 is our 1D number line that explains the most variation contained in 2D data using a 1D line.\nPoints on the PC1 are the projected points of data onto PC1.\n\n\n\n\n\n\n\n\n\n\n2D representation\n\nThe new coordinates PC1 and PC2 are ordered by variation size of the English and Math scores\n\n\n\n\n\n\n\n\n\n\n\n\nIf we make PC1 and PC2 the new X-Y coordinates, the new coordinates PC1 and PC2 are ordered by variation size of the English and Math scores.\nPC1 is our 1D number line that explains the most variation contained in 2D data using a 1D line.\nPoints on the PC1 are the projected points of data onto PC1.\nPC1 explains the most variation, and PC2 explains the second most variation.\nAnd PC2 explains the remaining variations that are not explained by PC1.\nHere, because we only have 2 dimensions, we have at most 2 PCs."
  },
  {
    "objectID": "slides/15-pca.html#variation",
    "href": "slides/15-pca.html#variation",
    "title": "Dimension Reduction \n",
    "section": "Variation",
    "text": "Variation\n\n\n\nIf the variation for PC1 is \\(17\\) and the variation for PC2 is \\(2\\), the total variation presented in the data is \\(17+2=19\\).\nPC1 accounts for \\(17/19 = 89\\%\\) of the total variation, and PC2 accounts for \\(2/19 = 11\\%\\) of the total variation.\n\n\nOK how do we quantify variation. Letâs give it a definition.\nVariation for PC1 \\(= \\frac{\\text{Sum of squared distances of projected points on PC1}}{n-1}\\)\n\nVariation for PC2 \\(= \\frac{\\text{Sum of squared distances of projected points on PC2}}{n-1}\\)\n\nIf the variation for PC1 is \\(17\\) and the variation for PC2 is \\(2\\), the total variation presented in the data is \\(17+2=19\\).\nPC1 accounts for \\(17/19 = 89\\%\\) of the total variation, and PC2 accounts for \\(2/19 = 11\\%\\) of the total variation."
  },
  {
    "objectID": "slides/15-pca.html#how-about-3-or-more-variables",
    "href": "slides/15-pca.html#how-about-3-or-more-variables",
    "title": "Dimension Reduction \n",
    "section": "How about 3 or More Variables?",
    "text": "How about 3 or More Variables?\n\nPC1 spans the direction of the most variation\n\n\n\nPC2 spans the direction of the 2nd most variation\n\n\n\n\nPC3 spans the direction of the 3rd most variation\n\n\n\n\nPC4 spans the direction of the 4th most variation\n\n\n\n\nIf we have \\(n\\) observations and \\(p\\) variables (dimensions), there are at most \\(\\min(n - 1, p)\\) PCs.\n\n\nOK. We use a 2D data to illustrate PCA. How about 3 or More Variables?\nWell the idea of 2D data can be exactly applied to 3 or more dimensional data."
  },
  {
    "objectID": "slides/15-pca.html#us-arrest-data-in-1973",
    "href": "slides/15-pca.html#us-arrest-data-in-1973",
    "title": "Dimension Reduction \n",
    "section": "US Arrest Data in 1973",
    "text": "US Arrest Data in 1973\n\ndim(USArrests)\n\n[1] 50  4\n\n\n\nhead(USArrests, 16)\n\n            Murder Assault UrbanPop Rape\nAlabama       13.2     236       58   21\nAlaska        10.0     263       48   44\nArizona        8.1     294       80   31\nArkansas       8.8     190       50   20\nCalifornia     9.0     276       91   41\nColorado       7.9     204       78   39\nConnecticut    3.3     110       77   11\nDelaware       5.9     238       72   16\nFlorida       15.4     335       80   32\nGeorgia       17.4     211       60   26\nHawaii         5.3      46       83   20\nIdaho          2.6     120       54   14\nIllinois      10.4     249       83   24\nIndiana        7.2     113       65   21\nIowa           2.2      56       57   11\nKansas         6.0     115       66   18\n\n\n\nOK. Time to learn how to perform PCA in R.\nThe data set used for PCA is USArrests data set in 1973.\nEach observation or subject is a state, and we have 4 features or dimensions, Murder, Assault, UrbanPop, and Rape."
  },
  {
    "objectID": "slides/15-pca.html#pc-loading-vectors-on-usarrests",
    "href": "slides/15-pca.html#pc-loading-vectors-on-usarrests",
    "title": "Dimension Reduction \n",
    "section": "PC Loading Vectors on USArrests\n",
    "text": "PC Loading Vectors on USArrests\n\n\npca_output &lt;- prcomp(USArrests, scale = TRUE)\n\n## rotation matrix provides PC loadings\n(pca_output$rotation &lt;- -pca_output$rotation)\n\n          PC1   PC2   PC3    PC4\nMurder   0.54  0.42 -0.34 -0.649\nAssault  0.58  0.19 -0.27  0.743\nUrbanPop 0.28 -0.87 -0.38 -0.134\nRape     0.54 -0.17  0.82 -0.089\n\n\n\n\nPCs are unique up to a sign change, so -pca_output$rotation gives us the same PCs as pca_output$rotation does. The sign just change the direction, not the angle.\n\n\\(\\text{PC1} = 0.54 \\times \\text{Murder} + 0.58 \\times \\text{Assault} + 0.28 \\times \\text{UrbanPop} + 0.54 \\times \\text{Rape}\\)\n\n\\(\\text{PC2} = 0.42 \\times \\text{Murder} + 0.19 \\times \\text{Assault} - 0.87 \\times \\text{UrbanPop} - 0.17 \\times \\text{Rape}\\)\n\n\n\nWe have 4 PCs because \\(\\min(n-1, p) = \\min(50-1, 4) = 4\\).\n\n\nTo perform PCA in R, it cannot be easier.\nWe just need to use the function prcomp(), and put the data set in the function. Then we get everything we want.\nHere I choose to scale the data because variables are not measured in the same scale. For example, Murder rate and UrbanPop are measured in different units.\nThis makes sure that every variable has variance 1, and our analysis is not affected by units.\nThe PCA results are stored as a list in pca_output object.\nOK first we can look at the rotation matrix because it provides PC loadings, and so we know what PC1 and PC2 are.\nChanging signs for easier interpretation of PCs\nThose PC loadings define how we rotates the coordinates to obtain the PCs. ???\nAgain, PC1 is just a linear combination or weighted average of the 4 variables, same as PC2. ???\nAgain, PC1 is just a linear combination or weighted average of the 4 variables, same as PC2.\nPCs are unique up to a sign change, so -pca_output$rotation gives us the same PCs as pca_output$rotation does.\nWe have 4 PCs because \\(\\min(n-1, k) = \\min(50-1, 4) = 4\\)."
  },
  {
    "objectID": "slides/15-pca.html#pc-scores",
    "href": "slides/15-pca.html#pc-scores",
    "title": "Dimension Reduction \n",
    "section": "PC Scores",
    "text": "PC Scores\n\nThe value of the rotated data, the data values of each PC are stored in pca_output$x\n\n\n\nhead(pca_output$x &lt;- -pca_output$x, 16) |&gt; round(2)\n\n              PC1   PC2   PC3   PC4\nAlabama      0.98  1.12 -0.44 -0.15\nAlaska       1.93  1.06  2.02  0.43\nArizona      1.75 -0.74  0.05  0.83\nArkansas    -0.14  1.11  0.11  0.18\nCalifornia   2.50 -1.53  0.59  0.34\nColorado     1.50 -0.98  1.08  0.00\nConnecticut -1.34 -1.08 -0.64  0.12\nDelaware     0.05 -0.32 -0.71  0.87\nFlorida      2.98  0.04 -0.57  0.10\nGeorgia      1.62  1.27 -0.34 -1.07\nHawaii      -0.90 -1.55  0.05 -0.89\nIdaho       -1.62  0.21  0.26  0.49\nIllinois     1.37 -0.67 -0.67  0.12\nIndiana     -0.50 -0.15  0.23 -0.42\nIowa        -2.23 -0.10  0.16 -0.02\nKansas      -0.79 -0.27  0.03 -0.20\n\n\n\nThe value of the rotated data, the data values of each PC are stored in pca_output$x.\nIn other words, PC1 column here shows the projected values of observations onto PC1. PC2 column shows the projected values of observations onto PC2, and so on."
  },
  {
    "objectID": "slides/15-pca.html#interpretation-of-pcs",
    "href": "slides/15-pca.html#interpretation-of-pcs",
    "title": "Dimension Reduction \n",
    "section": "Interpretation of PCs",
    "text": "Interpretation of PCs\n\npca_output$rotation\n\n          PC1   PC2   PC3    PC4\nMurder   0.54  0.42 -0.34 -0.649\nAssault  0.58  0.19 -0.27  0.743\nUrbanPop 0.28 -0.87 -0.38 -0.134\nRape     0.54 -0.17  0.82 -0.089\n\n\n\nPCs are less interpretable than original features.\nThe first loading vector places approximately equal weight on Assualt, Murder and Rape, with much less weights on UrbanPop.\nPC1 roughly corresponds to a overall serious crime rate.\n\n\n\nThe second loading vector places most of its weight on UrbanPop, and much less weight on the other 3 features.\nPC2 roughly corresponds to the level of urbanization.\n\n\nIntepretability decreases with the order of PCs.\nSo itâs easier to give PC1 a meaningful name than PC2, and PC2 is more meaningful than PC3, and so on. Because the PCs after the first 2 PCs usually explain quite small variation in the data, and some of them may be just noises.\nLetâs see if we can interpret these PCs.\nFirst keep in mind that PCs are less interpretable than original features. Sometimes we even donât know how to interpret it, especially for PCs that explain small variations. So this is the price we pay for dimension reduction.\nBut letâs look at this example.\nThe first loading vector places approximately equal weight on Assualt, Murder and Rape, with much less weights on UrbanPop.\nSo PC1 roughly corresponds to a overall serious crime rate because PC1 explains the variations of data caused by those crime variables Assualt, Murder and Rape.\nOn the contrary, the second loading vector places most of its weight on UrbanPop, and much less weight on the other 3 features.\nSo we can say PC2 roughly corresponds to the level of urbanization.\nSo you get the idea, Assualt, Murder and Rape are similar each other because they all are measures of crime rate.\nSo when reducing dimensions, we sort of combine the three similar variables together to become a one single index that measures an overall crime rate.\nUrban population measures a totally different thing. So the variation created by this variable cannot be explained well by the crime rate, and it should be absorbed in PC2."
  },
  {
    "objectID": "slides/15-pca.html#d-representation-of-the-4d-data",
    "href": "slides/15-pca.html#d-representation-of-the-4d-data",
    "title": "Dimension Reduction \n",
    "section": "2D Representation of the 4D data",
    "text": "2D Representation of the 4D data\n\n\n\n\n            PC1   PC2   PC3   PC4\nWisconsin -2.06 -0.61 -0.14 -0.18\nWyoming   -0.62  0.32 -0.24  0.16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigher value of PC1 means higher crime rates (roughly).\nHigher value of PC2 means higher level of urbanization (roughly).\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd we can show our 2D Representation of the 4D data.\nHigher value of PC1 means higher crime rates (roughly).\nHigher value of PC2 means higher level of urbanization (roughly).\nWe may be able to further analysis on the reduced dimension data, for example, we may want to partition the data into 2 clusters.\nOne cluster has high crime rate and low urbanization, the other group shows low crime rate and high urbanization.\nOne cluster may be the low economically-developed states, and the other high economically-developed states.\nSo you see, we can tell lots of stories by analyzing our data."
  },
  {
    "objectID": "slides/15-pca.html#d-representation-of-the-4d-data-biplot",
    "href": "slides/15-pca.html#d-representation-of-the-4d-data-biplot",
    "title": "Dimension Reduction \n",
    "section": "2D Representation of the 4D data: biplot",
    "text": "2D Representation of the 4D data: biplot\n\n\n\nbiplot(pca_output, xlabs = state.abb, \n       scale = 0)\n\n\nTop axis: PC1 loadings\nRight axis: PC2 loadings\nRed arrows: PC1 and PC2 loading vector, e.g., (0.28, -0.87) for UrbanPop.\nCrime-related variables (Assualt, Murder and Rape) are located close to each other.\n\nUrbanPop is far from the other three.\n\nAssualt, Murder and Rape are more correlated, and UrbanPop is less correlated with the other three.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can simply use the function biplot() to show the 2D Representation of the data.\nThis function also provide loading vector of PC1 and PC2 that gives us an idea of which direction means a large value of a variable/feature.\nThis is why it is called biplot because we plot two things in one single plot.\nHere, Top axis is for PC1 loadings\nRight axis is PC2 loadings\nRed arrows: PC1 and PC2 loading vector, e.g., (0.28, 0.87) for UrbanPop.\nSo NJ and Ca have pretty high urban pop rate because they are large in the UrbanPop arrow direction.\nCrime-related variables (Assualt, Murder and Rape) are located close to each other.\n\nUrbanPop is far from the other three.\n\nAssualt, Murder and Rape are more correlated, and UrbanPop is less correlated with the other three.\n\nAssualt, Murder and Rape sort of point to the same direction as PC1 and UrbanPop points to the same direction as PC2."
  },
  {
    "objectID": "slides/15-pca.html#proportion-of-variance-explained",
    "href": "slides/15-pca.html#proportion-of-variance-explained",
    "title": "Dimension Reduction \n",
    "section": "Proportion of Variance Explained",
    "text": "Proportion of Variance Explained\n\n(pc_var &lt;- pca_output$sdev ^ 2)\n\n[1] 2.48 0.99 0.36 0.17\n\n(pc_var_prop &lt;- pc_var / sum(pc_var))\n\n[1] 0.620 0.247 0.089 0.043\n\n\n\nPC1 explains \\(62\\%\\) of the variations in the data, and PC2 explains \\(24.7\\%\\) of the variance.\nPC1 and PC2 explain about \\(87\\%\\) of the variance, and the last two PCs explain only \\(13\\%\\).\n2D plot provides pretty accurate summary of the data.\n\n\nFinally I want to talk a little bit about Proportion of Variance Explained.\nIn the pca_output, we have SD of each PC.\nWe square it to get variance.\nThen if we divided by the sum of variance, we get the Proportion of Variance Explained by each PC.\nSo PC1 explains \\(62\\%\\) of the variations in the data, and PC2 explains \\(24.7\\%\\) of the variance.\nPC1 and PC2 explain about \\(87\\%\\) of the variance, and the last two PCs explain only \\(13\\%\\).\n2D plot provides pretty accurate summary of the data."
  },
  {
    "objectID": "slides/15-pca.html#scree-plot",
    "href": "slides/15-pca.html#scree-plot",
    "title": "Dimension Reduction \n",
    "section": "Scree Plot",
    "text": "Scree Plot\nLook for a point at which the proportion of variance explained by each subsequent PC drops off."
  },
  {
    "objectID": "slides/15-pca.html#principal-components",
    "href": "slides/15-pca.html#principal-components",
    "title": "Dimension Reduction \n",
    "section": "Principal Components",
    "text": "Principal Components\n\nThe \\(k\\)th PC is\n\n\\[Z_k = \\phi_{1k}X_1 + \\phi_{2k}X_2 + \\dots + \\phi_{pk}X_p,\\] where \\(\\sum_{j=1}^p\\phi_{jk}^2=1\\).\n\n\\((\\phi_{1k}, \\phi_{2k}, \\dots, \\phi_{pk})'\\) is the PC loading vector.\nThe PC1 loading vector solves\n\n\\[\\max_{\\phi_{11}, \\phi_{21}, \\dots, \\phi_{p1}} \\left\\{ \\frac{1}{n}\\sum_{i=1}^n z_{i1}^2\\right\\} = \\left\\{ \\frac{1}{n}\\sum_{i=1}^n \\left( \\sum_{j=1}^p  \\phi_{j1} x_{ij}\\right)^2\\right\\} \\quad \\text{s.t.} \\quad \\sum_{j=1}^p\\phi_{j1}^2 = 1\\]\n\n\nMaximize the sample variance of the projected points, or the scores \\(z_{11}, z_{21}, \\dots, z_{n1}\\).\nThe PC loading vector defines a direction in feature space along which the data vary the most."
  },
  {
    "objectID": "slides/15-pca.html#principal-components-1",
    "href": "slides/15-pca.html#principal-components-1",
    "title": "Dimension Reduction \n",
    "section": "Principal Components",
    "text": "Principal Components\nFor \\(k\\)th PC, \\(k &gt; 1\\),\n\n\\[\\max_{\\phi_{1k}, \\phi_{2k}, \\dots, \\phi_{pk}} \\left\\{ \\frac{1}{n}\\sum_{i=1}^n z_{ik}^2\\right\\} = \\left\\{ \\frac{1}{n}\\sum_{i=1}^n \\left( \\sum_{j=1}^p  \\phi_{jk} x_{ij}\\right)^2\\right\\} \\quad \\text{s.t.} \\quad \\sum_{j=1}^p\\phi_{jk}^2 = 1, \\text{ and } {\\mathbf{z}_m}'\\mathbf{z}_k = 0, \\, m = 1, \\dots, k-1\\]\n\nwhere\n\\(\\mathbf{z}_l = (z_{1l}, z_{2l}, \\dots, z_{nl})'\\)"
  },
  {
    "objectID": "slides/15-pca.html#low-rank-approximation",
    "href": "slides/15-pca.html#low-rank-approximation",
    "title": "Dimension Reduction \n",
    "section": "Low-Rank Approximation",
    "text": "Low-Rank Approximation\n\nPCs provide low-dimensional planes that are closest to the observations.\n\\(x_{ij} \\approx \\sum_{m=1}^M z_{im}\\phi_{jm}\\) with equality when \\(M = \\min(n-1, p)\\)\n\n\n\\[(z_{im}, \\phi_{jm}) = \\mathop{\\mathrm{arg\\,min}}_{a_{im}, b_{jm}} \\left\\{ \\sum_{j=1}^p\\sum_{i=1}^n\\left( x_{ij} - \\sum_{m=1}^Ma_{im}b_{jm}\\right)^2\\right\\}\\]\n\n\n\n\n\n\nSource: ISL Fig 12.2"
  },
  {
    "objectID": "slides/15-pca.html#scaling-the-variables",
    "href": "slides/15-pca.html#scaling-the-variables",
    "title": "Dimension Reduction \n",
    "section": "Scaling the Variables",
    "text": "Scaling the Variables\n\napply(USArrests, 2, var)\n\n  Murder  Assault UrbanPop     Rape \n      19     6945      210       88 \n\n\n\nIf we perform PCA on the unscaled variables, PC1 loading vector will have a large loading for Assault.\nWhen all the variables are of the same type, no need to scale the variables."
  },
  {
    "objectID": "slides/15-pca.html#pca-and-svd",
    "href": "slides/15-pca.html#pca-and-svd",
    "title": "Dimension Reduction \n",
    "section": "PCA and SVD",
    "text": "PCA and SVD\nPCA is equivalent to singular value decomposition (SVD) of \\(\\mathbf{X}\\)\n\\[\\mathbf{X}_{n\\times p} = \\mathbf{U}_{n \\times n} \\mathbf{D}_{n \\times p} \\mathbf{V}'_{p \\times p}\\] where \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are orthogonal matrices, and \\(\\mathbf{D}\\) is diagonal with diagonal elements singular values \\(d_1 \\ge d_2 \\ge \\cdots \\ge d_p\\)."
  },
  {
    "objectID": "slides/15-pca.html#pc-scores-and-svd",
    "href": "slides/15-pca.html#pc-scores-and-svd",
    "title": "Dimension Reduction \n",
    "section": "PC Scores and SVD",
    "text": "PC Scores and SVD\n\n\\({\\bf Z}_{n \\times p} = [{\\bf z}_1 \\, {\\bf z}_2 \\, \\cdots \\, {\\bf z}_p] = \\mathbf{X}\\mathbf{V}\\)\nThe \\(j\\)th PC is the \\(j\\)th column of \\({\\bf Z}\\) given by \\({\\bf z}_j = (z_{1j}, z_{2j}, \\dots, z_{nj})' = {\\bf Xv}_j\\).\nProject \\(\\mathbf{X}\\) onto the space spanned by \\(\\mathbf{v}_j\\)s\n\\(\\mathbf{v}_j\\)s are loading vectors.\n\n\n\n\\({\\bf Z}_{n \\times p} = \\mathbf{U}\\mathbf{D}\\)\n\\({\\bf z}_j = d_j\\mathbf{u}_j\\).\n\\(\\mathbf{u}_j\\)s are the unit PC vectors, and \\(d_j\\) controls the variation along the \\(\\mathbf{u}_j\\) direction."
  },
  {
    "objectID": "slides/15-pca.html#low-rank-approximation-and-svd",
    "href": "slides/15-pca.html#low-rank-approximation-and-svd",
    "title": "Dimension Reduction \n",
    "section": "Low-Rank Approximation and SVD",
    "text": "Low-Rank Approximation and SVD\n\\({\\mathbf{X}} = {\\mathbf{U}}{\\mathbf{D}}{\\mathbf{V}}'\\)\n\\(\\mathbf{A}= \\mathbf{U}_{n\\times M}\\mathbf{D}_{M \\times M}\\) and \\(\\mathbf{B}' = \\mathbf{V}_{M\\times p}'\\) are the minimizer of\n\\[ \\min_{\\mathbf{A}\\in \\mathbf{R}^{n \\times M}, \\mathbf{B}\\in \\mathbf{R}^{p \\times M}} \\|\\mathbf{X}- \\mathbf{A}\\mathbf{B}' \\|\\] where\n\n\n\\(\\mathbf{U}_{n\\times M}\\) is \\(\\mathbf{U}\\) with the first \\(M\\) columns\n\n\\(\\mathbf{D}_{M \\times M}\\) is \\(\\mathbf{D}\\) of the first \\(M\\) rows and columns\n\n\\(\\mathbf{V}_{M\\times p}'\\) is \\(\\mathbf{V}'\\) with the first \\(M\\) rows\n\n\n\n\\(x_{ij} = \\sum_{m = 1}^p d_mu_{im}v_{jm}\\)\n\\(x_{ij} \\approx \\sum_{m = 1}^M d_mu_{im}v_{jm}\\)"
  },
  {
    "objectID": "slides/15-pca.html#pca-and-eigendecomposition",
    "href": "slides/15-pca.html#pca-and-eigendecomposition",
    "title": "Dimension Reduction \n",
    "section": "PCA and Eigendecomposition",
    "text": "PCA and Eigendecomposition\nPCA is equivalent to eigendecomposition of \\(\\mathbf{X}'\\mathbf{X}\\) or \\(\\boldsymbol \\Sigma= \\text{Cov}(\\mathbf{X}) = \\dfrac{\\mathbf{X}'\\mathbf{X}}{n-1}\\)1, the covariance matrix of \\(\\mathbf{X}\\).\n\n\\[\\mathbf{X}'\\mathbf{X}= \\mathbf{V}\\mathbf{D}^2\\mathbf{V}' = d_1^2\\mathbf{v}_1\\mathbf{v}_1' + \\dots + d_p^2\\mathbf{v}_p\\mathbf{v}_p'\\]\n\nTotal variation: \\(\\sum_{j=1}^p \\text{Var}(\\mathbf{x}_j) = \\frac{1}{n-1}\\sum_{j=1}^pd_j^2 = p\\)\nVariation of \\(m\\)th PC: \\(\\text{Var}(\\mathbf{z}_m) = \\frac{d_m^2}{n-1}\\)\nUnit normal scaling (normalization or standardization) is such that the variance is one, and unit length scaling makes the length one. If \\(\\mathbf{W}\\) is the unit length scaled version, \\(\\text{Corr}(\\mathbf{W}) = \\mathbf{W}'\\mathbf{W}= \\frac{\\mathbf{X}'\\mathbf{X}}{n-1}\\)"
  },
  {
    "objectID": "slides/15-pca.html#transformed-ols-regression",
    "href": "slides/15-pca.html#transformed-ols-regression",
    "title": "Dimension Reduction \n",
    "section": "Transformed OLS Regression",
    "text": "Transformed OLS Regression\nTransform \\({\\bf y = X\\boldsymbol \\beta+ \\boldsymbol \\epsilon}\\) into \\[{\\bf y = XVV'\\boldsymbol \\beta+ \\boldsymbol \\epsilon= Z\\boldsymbol \\alpha} + \\boldsymbol \\epsilon\\] where \\(\\mathbf{Z}= \\mathbf{X}\\mathbf{V}\\) and \\(\\boldsymbol \\alpha= \\mathbf{V}'\\boldsymbol \\beta\\), or \\(\\boldsymbol \\beta= \\mathbf{V}\\boldsymbol \\alpha\\).\n\nThe least- squares estimator \\(\\hat{\\boldsymbol \\alpha} = (\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{y}= \\mathbf{D}^{-2}\\mathbf{Z}'\\mathbf{y}\\)\n\\(\\text{Var}\\left(\\hat{\\boldsymbol \\alpha} \\right) = \\sigma^2 (\\mathbf{Z}'\\mathbf{Z})^{-1} = \\sigma^2 \\mathbf{D}^{-2}\\)\nA small \\(d_j\\) means that the variance of \\(\\alpha_j\\) will be large.\n\nThe PC regression combats multicollinearity by using less PCs \\((m \\ll p)\\) in the model.\n\nThe PCs corresponding to tiny \\(d_j\\)s (huge variance) are removed and least-squares is applied to the remaining PCs.\n\n\n\n\nEven though the new variables are orthogonal, the same magnitude of variance due to the ill-conditioning in \\(X'X\\) is retained. The total variance has merely been redistributed.\n\\({\\bf y = X\\boldsymbol \\beta+ \\boldsymbol \\epsilon}\\)\n\n\\({\\bf y = XVV'\\boldsymbol \\beta+ \\boldsymbol \\epsilon= Z\\boldsymbol \\alpha} + \\boldsymbol \\epsilon\\) where \\({\\bf Z  =XV}\\) and \\({\\bf \\boldsymbol \\alpha= V'\\boldsymbol \\beta}\\).\nThe least- squares estimator \\(\\hat{\\boldsymbol \\alpha} = {\\bf (Z'Z)^{-1}Z'y = \\Lambda^{-1}Z'y}\\)\n\n\\(\\var\\left(\\hat{\\boldsymbol \\alpha} \\right) = \\sigma^2 {\\bf (Z'Z)}^{-1} = \\sigma^2 {\\bf \\Lambda}^{-1}\\)\nA small \\(\\lambda_j\\) means that the variance of \\(\\alpha_j\\) will be large.\nThe PC regression combats multicollinearity by using less PCs in the model.\nAssume that the regressors are arranged in order of decreasing eigenvalues, \\(\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_k &gt; 0\\).\nThe PCs corresponding to near-zero eigenvalues are removed and least squares applied to the remaining PCs.\nIf all PCs are in the model, it is just a rotation of the regressors. The same magnitude of variance is retained.\nThe small variation in the \\(z_2\\) direction is responsible for the large variance in \\(\\hat{\\alpha}_2\\).\nA large \\(\\lambda_1\\) allows variance of \\(\\hat{\\alpha}_1\\) to be relatively unaffected by the dependency of \\(x_1\\) and \\(x_2\\)."
  },
  {
    "objectID": "slides/15-pca.html#principal-component-regression-pcr",
    "href": "slides/15-pca.html#principal-component-regression-pcr",
    "title": "Dimension Reduction \n",
    "section": "Principal Component Regression (PCR)",
    "text": "Principal Component Regression (PCR)\n\nKeep \\(m &lt; p\\) PCs with the largest \\(m\\) singular values \\(d_1, d_2, \\dots, d_m\\).\nThe least- squares estimator is \\[\\hat{\\boldsymbol \\alpha}_m = ({\\bf Z} _m ' {\\bf Z} _m) ^ {-1} {\\bf Z}_m ' {\\bf y} = \\mathbf{D}_m ^ {-2} {\\bf Z}_m '{\\bf y}\\] where \\({\\bf Z}_m = [{\\bf z}_1 \\, {\\bf z}_2 \\, \\cdots \\, {\\bf z}_m]\\) and \\({\\bf \\mathbf{D}}_m = \\text{diag}(d_1, d_2, \\dots, d_m)\\)\n\n\n\n\n\\({\\bf b}_{pc} = {\\bf V}_m\\hat{\\boldsymbol \\alpha}_m = [{\\bf v}_1 \\, {\\bf v}_2 \\, \\cdots \\, {\\bf v}_m] \\begin{bmatrix} \\hat{\\alpha}_1 \\\\ \\hat{\\alpha}_2 \\\\ \\vdots \\\\ \\hat{\\alpha}_m \\end{bmatrix}\\)\n\n\\({\\bf b}_{pc}\\) is biased but has smaller variance than \\({\\bf b}\\).\n\n\nKeep the first \\(m &lt; k\\) PCs.\nThe least- squares estimator is \\[\\hat{\\boldsymbol \\alpha}_m = ({\\bf Z} _m ' {\\bf Z} _m) ^ {-1} {\\bf Z}_m ' {\\bf y} = {\\bf \\Lambda} _m ^ {-1} {\\bf Z}_m '{\\bf y}\\] where \\({\\bf Z}_m = [{\\bf z}_1 \\, {\\bf z}_2 \\, \\cdots \\, {\\bf z}_m]\\) and \\({\\bf \\Lambda}_m = \\text{diag}(\\lambda_1, \\lambda_2, \\dots, \\lambda_m)\\)\n\nTo transform back to the original coefficients, since \\({\\bf \\boldsymbol \\alpha= V'\\boldsymbol \\beta}\\), \\({\\bf \\boldsymbol \\beta= V\\boldsymbol \\alpha}\\)\n\n\\({\\bf b}_{pc} = {\\bf V}_m\\hat{\\boldsymbol \\alpha}_m = [{\\bf v}_1 \\, {\\bf v}_2 \\, \\cdots \\, {\\bf v}_m] \\begin{bmatrix} \\hat{\\alpha}_1 \\\\ \\hat{\\alpha}_2 \\\\ \\vdots \\\\ \\hat{\\alpha}_m \\end{bmatrix}\\)\n\n\\({\\bf b}_{pc}\\) is biased but has smaller variance than \\({\\bf b}\\). Deleting PCs does not imply deletion of any of original regressors. The selected \\(m\\) PCs do contain information provided by all \\(k\\) original regressors."
  },
  {
    "objectID": "slides/15-pca.html#principal-component-regression",
    "href": "slides/15-pca.html#principal-component-regression",
    "title": "Dimension Reduction \n",
    "section": "Principal Component Regression",
    "text": "Principal Component Regression\n\n\nPCR performs well when the directions in which \\(X_1, \\dots ,X_p\\) show the most variation (the first few PCs) are the directions that are associated with \\(Y\\).\n\n\n\n\n\n\nDeleting PCs does not imply deletion of any of original regressors. The selected \\(m\\) PCs contain information provided by all \\(k\\) original regressors. (Not a feature selection method)\n\n\n\n\n\n\nPCR and ridge regression are closely related. Ridge regression is a continuous version of PCR."
  },
  {
    "objectID": "slides/15-pca.html#plspcr",
    "href": "slides/15-pca.html#plspcr",
    "title": "Dimension Reduction \n",
    "section": "pls::pcr()",
    "text": "pls::pcr()\n\nlibrary(pls)\nset.seed(1)\npcr.fit &lt;- pcr(Salary ~ ., data = Hitters, \n               subset = train,\n               scale = TRUE, validation = \"CV\")\nvalidationplot(pcr.fit, val.type = \"MSEP\")\n\n\nhttps://stats.stackexchange.com/questions/371054/what-is-adjusted-cv-or-bias-corrected-cv"
  },
  {
    "objectID": "slides/15-pca.html#plspcr-1",
    "href": "slides/15-pca.html#plspcr-1",
    "title": "Dimension Reduction \n",
    "section": "pls::pcr()",
    "text": "pls::pcr()\n\npcr.pred &lt;- predict(pcr.fit, x[test, ], ncomp = 5)\nmean((pcr.pred - y.test)^2)\n\n[1] 142812\n\npcr.fit &lt;- pcr(y ~ x, scale = TRUE, ncomp = 5)\nsummary(pcr.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: svdpc\nNumber of components considered: 5\nTRAINING: % variance explained\n   1 comps  2 comps  3 comps  4 comps  5 comps\nX    38.31    60.16    70.84    79.03    84.29\ny    40.63    41.58    42.17    43.22    44.90\n\n\ncoefficients are in scaled unit, not original units."
  },
  {
    "objectID": "slides/15-pca.html#other-dimension-reduction-latent-variable-methods",
    "href": "slides/15-pca.html#other-dimension-reduction-latent-variable-methods",
    "title": "Dimension Reduction \n",
    "section": "Other Dimension Reduction (Latent Variable) Methods",
    "text": "Other Dimension Reduction (Latent Variable) Methods\n\nKernel Principal Component Analysis https://ml-explained.com/blog/kernel-pca-explained\nProbabilistic PCA\nFactor Analysis\nAutoencoders\nt-SNE (t-distributed stochastic neighbor embedding)\nUMAP (Uniform manifold approximation and projection)\n\n\n\n\n\nmssc6250-s25.github.io/website"
  },
  {
    "objectID": "slides/14-tree.html#tree-based-methods",
    "href": "slides/14-tree.html#tree-based-methods",
    "title": "Tree-based Methods \n",
    "section": "Tree-based Methods",
    "text": "Tree-based Methods\n\nCan be used for regression and classification.\nIDEA: Segmenting the predictor space into many simple regions.\nSimple, useful for interpretation, and has nice graphical representation.\nNot competitive with the best supervised learning approaches in terms of prediction accuracy. (Large bias)\nCombining a large number of trees (ensembles) often results in improvements in prediction accuracy, at the expense of some loss interpretation."
  },
  {
    "objectID": "slides/14-tree.html#decision-trees-classification-and-regression-trees-cart",
    "href": "slides/14-tree.html#decision-trees-classification-and-regression-trees-cart",
    "title": "Tree-based Methods \n",
    "section": "Decision Trees: Classification and Regression Trees (CART)",
    "text": "Decision Trees: Classification and Regression Trees (CART)\n\nCART is a nonparametric method that recursively partitions the feature space into hyper-rectangular subsets (boxes), and make prediction on each subset.\nDivide the predictor space â the set of possible values for \\(X_1, X_2, \\dots, X_p\\) â into \\(J\\) distinct and non-overlapping regions, \\(R_1, R_2, \\dots, R_J\\).\n\n\n\n\nFor every test point that falls into the region \\(R_j\\) , we make the same prediction:\n\n\nRegression: the mean of the response values for the training points in \\(R_j\\), i.e., \\(\\hat{y}_{R_j} = \\sum_{k \\in R_j} y_k / |R_j|\\)\n\n\nClassification: the most commonly occurring class of training points in \\(R_j\\), i.e., \\(\\hat{y}_{R_j} = \\underset{c}{\\text{arg max}} \\, \\,\\# (y_k = c)\\), \\(y_k \\in R_j\\)."
  },
  {
    "objectID": "slides/14-tree.html#recursive-binary-splitting",
    "href": "slides/14-tree.html#recursive-binary-splitting",
    "title": "Tree-based Methods \n",
    "section": "Recursive Binary Splitting",
    "text": "Recursive Binary Splitting\n\nComputationally infeasible to consider every possible partition of the feature space into arbitrary \\(J\\) boxes.\n\n\n\n\n\nThe recursive binary splitting is top-down and greedy:\n\n\nTop-down: begins at the top of the tree (the entire \\(X\\) space)\n\nGreedy: at each step, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelect \\(X_j\\) and a cutoff \\(s\\) so that splitting the predictor space into \\(\\{\\mathbf{X}\\mid X_j &lt; s \\}\\) and \\(\\{\\mathbf{X}\\mid X_j \\ge s \\}\\) leads to the greatest reduction in\n\n\n\\(SS_{res}\\) for regression\n\nGini index, entropy or misclassification rate for classification\n\n\nRepeatedly split one of the two previously identified regions until a stopping criterion is reached.\n\nat each step of the tree-building process"
  },
  {
    "objectID": "slides/14-tree.html#classification-tree",
    "href": "slides/14-tree.html#classification-tree",
    "title": "Tree-based Methods \n",
    "section": "Classification Tree",
    "text": "Classification Tree\n\n\n\nKNN requires K and a distance measure.\nSVM requires kernels.\nTree solves this by recursively partitioning the feature space using a binary splitting rule \\(\\mathbf{1}\\{x \\le c \\}\\)\n0: Red; 1: Blue"
  },
  {
    "objectID": "slides/14-tree.html#classification-tree-1",
    "href": "slides/14-tree.html#classification-tree-1",
    "title": "Tree-based Methods \n",
    "section": "Classification Tree",
    "text": "Classification Tree\nIf \\(x_2 &lt; -0.64\\), \\(y = 0\\)."
  },
  {
    "objectID": "slides/14-tree.html#classification-tree-2",
    "href": "slides/14-tree.html#classification-tree-2",
    "title": "Tree-based Methods \n",
    "section": "Classification Tree",
    "text": "Classification Tree\nIf \\(x_2 \\ge -0.64\\) and \\(x_1 \\ge 0.69\\), \\(y = 0\\)."
  },
  {
    "objectID": "slides/14-tree.html#classification-tree-3",
    "href": "slides/14-tree.html#classification-tree-3",
    "title": "Tree-based Methods \n",
    "section": "Classification Tree",
    "text": "Classification Tree\nIf \\(x_2 \\ge -0.64\\), \\(x_1 &lt; 0.69\\), and \\(x_2 \\ge 0.75\\), \\(y = 0\\)."
  },
  {
    "objectID": "slides/14-tree.html#classification-tree-4",
    "href": "slides/14-tree.html#classification-tree-4",
    "title": "Tree-based Methods \n",
    "section": "Classification Tree",
    "text": "Classification Tree\nIf \\(x_2 \\ge -0.64\\), \\(x_1 &lt; 0.69\\), \\(x_2 &lt; 0.75\\), and \\(x_1 &lt; -0.69\\), \\(y = 0\\)."
  },
  {
    "objectID": "slides/14-tree.html#classification-tree-5",
    "href": "slides/14-tree.html#classification-tree-5",
    "title": "Tree-based Methods \n",
    "section": "Classification Tree",
    "text": "Classification Tree\nStep 5 may not be beneficial."
  },
  {
    "objectID": "slides/14-tree.html#classification-tree-6",
    "href": "slides/14-tree.html#classification-tree-6",
    "title": "Tree-based Methods \n",
    "section": "Classification Tree",
    "text": "Classification Tree\nStep 6 may not be beneficial. (Could overfit)"
  },
  {
    "objectID": "slides/14-tree.html#classification-tree-7",
    "href": "slides/14-tree.html#classification-tree-7",
    "title": "Tree-based Methods \n",
    "section": "Classification Tree",
    "text": "Classification Tree\nStep 7 may not be beneficial. (Could overfit)"
  },
  {
    "objectID": "slides/14-tree.html#misclassification-rate",
    "href": "slides/14-tree.html#misclassification-rate",
    "title": "Tree-based Methods \n",
    "section": "Misclassification Rate",
    "text": "Misclassification Rate\n\nThe classification error rate is the fraction of the training observations in the region that do not belong to the most common class: \\[1 - \\max_{k} (\\hat{p}_{mk})\\] where \\(\\hat{p}_{mk}\\) is the proportion of training observations in the \\(m\\)th region that are from the \\(k\\)th class.\nIt is not sensitive for tree-growing.\nHope to have nodes (regions) including training points that belong to only one class."
  },
  {
    "objectID": "slides/14-tree.html#gini-index-impurity",
    "href": "slides/14-tree.html#gini-index-impurity",
    "title": "Tree-based Methods \n",
    "section": "Gini Index (Impurity)",
    "text": "Gini Index (Impurity)\nThe Gini index is defined by\n\\[\\sum_{k=1}^K \\hat{p}_{mk}(1 - \\hat{p}_{mk})\\] which is a measure of total variance across the \\(K\\) classes.\n\nGini is small if all of the \\(\\hat{p}_{mk}\\)s are close to zero or one.\nNode purity: a small value indicates that a node contains predominantly observations from a single class."
  },
  {
    "objectID": "slides/14-tree.html#shannon-entropy",
    "href": "slides/14-tree.html#shannon-entropy",
    "title": "Tree-based Methods \n",
    "section": "Shannon Entropy",
    "text": "Shannon Entropy\nThe Shannon entropy is defined as\n\\[- \\sum_{k=1}^K \\hat{p}_{mk} \\log(\\hat{p}_{mk}).\\]\n\nThe entropy is near zero if the \\(\\hat{p}_{mk}\\)s are all near zero or one.\nGini index and the entropy are similar numerically.\n\nAny of these three approaches might be used when pruning the tree, but the classification error rate is preferable if prediction accuracy of the final pruned tree is the goal."
  },
  {
    "objectID": "slides/14-tree.html#comparing-measures",
    "href": "slides/14-tree.html#comparing-measures",
    "title": "Tree-based Methods \n",
    "section": "Comparing Measures",
    "text": "Comparing Measures\n\nUse Gini and Entropy for training (building a tree), and use error rate for evaluating predictive accuracy.\n\n\nFor each quantity, smaller value means that the node is more âpureâ, hence, there is a higher certainty when we predict a new value. The idea of splitting a node is that, we want the two resulting child node to contain less variation. In other words, we want each child node to be as âpureâ as possible."
  },
  {
    "objectID": "slides/14-tree.html#regression-tree",
    "href": "slides/14-tree.html#regression-tree",
    "title": "Tree-based Methods \n",
    "section": "Regression Tree",
    "text": "Regression Tree\nThe goal is to find boxes \\(R_1, \\dots ,R_J\\) that minimize the \\(SS_{res}\\), given by \\[\\sum_{j=1}^J\\sum_{i \\in R_j}\\left( y_i - \\hat{y}_{R_j}\\right)^2\\] where \\(\\hat{y}_{R_j}\\) is the mean response for the training observations within \\(R_j\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: ISL Fig 8.3"
  },
  {
    "objectID": "slides/14-tree.html#tree-pruning",
    "href": "slides/14-tree.html#tree-pruning",
    "title": "Tree-based Methods \n",
    "section": "Tree Pruning",
    "text": "Tree Pruning\n\nUsing regression and classification performance measures to grow trees with no penalty on the tree size leads to overfitting.\n\n\n\n\nCost complexity pruning:\n\nGiven the largest tree \\(T_{max}\\),\n\\[\\begin{align}\n\\min_{T \\subset T_{max}} \\sum_{m=1}^{|T|}\\sum_{i:x_i\\in R_m} \\left( y_i - \\hat{y}_{R_m}\\right)^2 +  \\alpha|T|\n\\end{align}\\] where \\(|T|\\) indicates the number of terminal nodes of the tree \\(T\\).\n\nLarge \\(\\alpha\\) results in small trees\nChoose \\(\\alpha\\) using CV\nAlgorithm 8.1 in ISL for building a regression tree.\nFor classification, replace \\(SS_{res}\\) with a classification performance measure."
  },
  {
    "objectID": "slides/14-tree.html#implementation",
    "href": "slides/14-tree.html#implementation",
    "title": "Tree-based Methods \n",
    "section": "Implementation",
    "text": "Implementation\n\nrpart::rpart()\n\n\nlibrary(rpart)\nrpart::rpart(formula = y ~ x1 + x2, data)\n\n\n\ntree::tree()\n\n\nlibrary(tree)\ntree::tree(formula = y ~ x1 + x2, data)\n\n\n\nsklearn tree\n\n\nfrom sklearn import tree\ndtc = tree.DecisionTreeClassifier()\ndtc = dtc.fit(X, y)\ndtr = tree.DecisionTreeRegressor()\ndtr = dtr.fit(X, y)"
  },
  {
    "objectID": "slides/14-tree.html#trees-v.s.-linear-regression",
    "href": "slides/14-tree.html#trees-v.s.-linear-regression",
    "title": "Tree-based Methods \n",
    "section": "Trees v.s. Linear Regression",
    "text": "Trees v.s. Linear Regression\n\n\nLinear regression\n\\[f(X) = \\beta_0 + \\sum_{j=1}^pX_j\\beta_j\\]\n\nPerforms better when the relationship between \\(y\\) and \\(x\\) is approximately linear.\n\n\nRegression tree\n\\[f(X) = \\sum_{j=1}^J \\hat{y}_{R_j}\\mathbf{1}(\\mathbf{X}\\in R_j)\\]\n\nPerforms better when there is a highly nonlinear and complex relationship between \\(y\\) and \\(x\\).\nPreferred for interpretability and visualization."
  },
  {
    "objectID": "slides/14-tree.html#trees-v.s.-linear-models",
    "href": "slides/14-tree.html#trees-v.s.-linear-models",
    "title": "Tree-based Methods \n",
    "section": "Trees v.s. Linear Models",
    "text": "Trees v.s. Linear Models\n\n\n\n\n\nSource: ISL Fig 8.7"
  },
  {
    "objectID": "slides/14-tree.html#ensemble-methods",
    "href": "slides/14-tree.html#ensemble-methods",
    "title": "Tree-based Methods \n",
    "section": "Ensemble Methods",
    "text": "Ensemble Methods\n\nAn ensemble method combines many weak learners (unstable, less accurate) to obtain a single and powerful model.\nThe CARTs suffer from high variance.\nIf independent \\(Z_1, \\dots, Z_n\\) have variance \\(\\sigma^2\\), then \\(\\bar{Z}\\) has variance \\(\\sigma^2/n\\).\nAveraging a set of observations reduces variance!\n\n\nWith \\(B\\) separate training sets,\n\\[\\hat{f}_{avg}(x) = \\frac{1}{B}\\sum_{b=1}^B\\hat{f}_{b}(x)\\]"
  },
  {
    "objectID": "slides/14-tree.html#bagging",
    "href": "slides/14-tree.html#bagging",
    "title": "Tree-based Methods \n",
    "section": "Bagging",
    "text": "Bagging\n\n\n\n\nBootstrap aggregation, or bagging is a procedure for reducing variance.\n\n\nGenerate \\(B\\) bootstrap samples by repeatedly sampling with replacement from the training set \\(B\\) times.\n\n\\[\\hat{f}_{bag}(x) = \\frac{1}{B}\\sum_{b=1}^B\\hat{f}^*_{b}(x)\\]\n\n\n\n\n\n\nSource: Wiki page of bootstrap aggregating\n\n\n\n\n\n\nBagging works for many regression methods, but it is particularly useful for decision trees."
  },
  {
    "objectID": "slides/14-tree.html#bagging-on-decision-trees",
    "href": "slides/14-tree.html#bagging-on-decision-trees",
    "title": "Tree-based Methods \n",
    "section": "Bagging on Decision Trees",
    "text": "Bagging on Decision Trees\n\n\n\n\n\nSource: Wiki page of ensemble learning"
  },
  {
    "objectID": "slides/14-tree.html#cart-v.s.-bagging",
    "href": "slides/14-tree.html#cart-v.s.-bagging",
    "title": "Tree-based Methods \n",
    "section": "CART v.s. Bagging",
    "text": "CART v.s. Bagging\n\nFor CART, the decision line has to be aligned to axis.\nFor Bagging, \\(B = 200\\) each having 400 training points. Boundaries are smoother."
  },
  {
    "objectID": "slides/14-tree.html#notes-of-bagging",
    "href": "slides/14-tree.html#notes-of-bagging",
    "title": "Tree-based Methods \n",
    "section": "Notes of Bagging",
    "text": "Notes of Bagging\n\nUsing a large \\(B\\) will not lead to overfitting.\nUse \\(B\\) sufficiently large that the error has settled down.\nBagging improves prediction accuracy at the expense of interpretability.\n\n\n\n\nWhen different trees are highly correlated, simply averaging is not very effective.\n\nIf there is one very strong predictor, in the collection of bagged trees, all of the trees will use this strong predictor in the top split. Therefore, all of the bagged trees will look similar to each other.\n\n\nThe predictions from the bagged trees will be highly correlated, and hence averaging does not lead to as large reduction in variance.\n\n\nUse many trees and averaging. It is no longer clear which variables are most important to the procedure.\nIf there is one very strong predictor in the data set, in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Therefore, all of the bagged trees will look quite similar to each other.\nThe predictions from the bagged trees will be highly correlated, and hence averaging does not lead to as large reduction in variance."
  },
  {
    "objectID": "slides/14-tree.html#random-forests",
    "href": "slides/14-tree.html#random-forests",
    "title": "Tree-based Methods \n",
    "section": "Random Forests",
    "text": "Random Forests\n\n\n\nRandom forests improve bagged trees by decorrelating the trees.\n\\(m\\) predictors are randomly sampled as split candidates from the \\(p\\) predictors.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: Multivariate Statistical Machine Learning Methods for Genomic Prediction, Lopez et al.Â (2022)\n\n\n\n\n\n\n\nWhen building decision trees, each time a split in a tree is considered, a random sample of \\(m\\) predictors is chosen as split candidates from the full set of \\(p\\) predictors.\nThe split is allowed to use only one of those \\(m\\) predictors."
  },
  {
    "objectID": "slides/14-tree.html#random-forests-1",
    "href": "slides/14-tree.html#random-forests-1",
    "title": "Tree-based Methods \n",
    "section": "Random Forests",
    "text": "Random Forests\n\n\\(m \\approx \\sqrt{p}\\) for classification; \\(m \\approx p/3\\) for regression.\nDecorrelating: on average \\((p â m)/p\\) of the splits will not even consider the strong predictor, and so other predictors will have more of a chance.\nIf \\(m = p\\), random forests = bagging.\nThe improvement is significant when \\(p\\) is large."
  },
  {
    "objectID": "slides/14-tree.html#cart-vs.-bagging-vs.-random-forests",
    "href": "slides/14-tree.html#cart-vs.-bagging-vs.-random-forests",
    "title": "Tree-based Methods \n",
    "section": "CART vs.Â Bagging vs.Â Random Forests",
    "text": "CART vs.Â Bagging vs.Â Random Forests\n\n\nrandomForest::randomForest(x, y, mtry, ntree, nodesize, sampsize)    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnodesize: Minimum size of terminal nodes. Setting this number larger causes smaller trees to be grown sampsize: Size(s) of sample to draw. ntree: Number of trees to grow. mtry: Number of variables randomly sampled as candidates at each split."
  },
  {
    "objectID": "slides/14-tree.html#boosting",
    "href": "slides/14-tree.html#boosting",
    "title": "Tree-based Methods \n",
    "section": "Boosting",
    "text": "Boosting\n\n\nBagging\n\nTrees are built on independent bootstrap data sets.\nTrees are grown deep.\nLarge number of trees (\\(B\\)) wonât overfit.\n\n\nBoosting\n\nTrees are grown sequentially: each tree is grown using information from previously grown trees.\nEach tree is fit on a modified version of the original data set, the residuals/false predictions!\nTrees are rather small (weak learner).\nLarge \\(B\\) can overfit."
  },
  {
    "objectID": "slides/14-tree.html#boosting-1",
    "href": "slides/14-tree.html#boosting-1",
    "title": "Tree-based Methods \n",
    "section": "Boosting",
    "text": "Boosting\n\n\n\n\n\n\n\nSource: https://pub.towardsai.net/bagging-vs-boosting-the-power-of-ensemble-methods-in-machine-learning-6404e33524e6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://pub.towardsai.net/bagging-vs-boosting-the-power-of-ensemble-methods-in-machine-learning-6404e33524e6 - [Boosting can also be applied to many models, but most effectively applied to decision trees.] - Bagging trees are grown deep, but Boosting trees are rather small."
  },
  {
    "objectID": "slides/14-tree.html#boosting-for-classification",
    "href": "slides/14-tree.html#boosting-for-classification",
    "title": "Tree-based Methods \n",
    "section": "Boosting for Classification",
    "text": "Boosting for Classification\n\n\ndistribution = \"bernoulli\": LogitBoost\n\n\n\ngbm.fit = gbm::gbm(y ~ ., data = data.frame(x1, x2, y), \n                   distribution = \"bernoulli\", \n                   n.trees = 10000, shrinkage = 0.01, bag.fraction = 0.6, \n                   interaction.depth = 2, cv.folds = 10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshrinkage: a shrinkage parameter applied to each tree in the expansion. bag.fraction: the fraction of the training set observations randomly selected to propose the next tree in the expansion. interaction.depth: Integer specifying the maximum depth of each tree n.trees: Integer specifying the total number of trees to fit."
  },
  {
    "objectID": "slides/14-tree.html#boosting-cross-validation",
    "href": "slides/14-tree.html#boosting-cross-validation",
    "title": "Tree-based Methods \n",
    "section": "Boosting Cross Validation",
    "text": "Boosting Cross Validation\n\ngbm.perf(gbm.fit, method = \"cv\")\n\n\n[1] 1181"
  },
  {
    "objectID": "slides/14-tree.html#boosting-for-regression",
    "href": "slides/14-tree.html#boosting-for-regression",
    "title": "Tree-based Methods \n",
    "section": "Boosting for Regression",
    "text": "Boosting for Regression\n\ngbm.fit &lt;- gbm::gbm(y ~ x, data = data.frame(x, y), \n                    distribution = \"gaussian\", n.trees = 300,\n                    shrinkage = 0.5, bag.fraction = 0.8, cv.folds = 10)"
  },
  {
    "objectID": "slides/14-tree.html#boosting-for-regression-1",
    "href": "slides/14-tree.html#boosting-for-regression-1",
    "title": "Tree-based Methods \n",
    "section": "Boosting for Regression",
    "text": "Boosting for Regression"
  },
  {
    "objectID": "slides/14-tree.html#other-topics",
    "href": "slides/14-tree.html#other-topics",
    "title": "Tree-based Methods \n",
    "section": "Other Topics",
    "text": "Other Topics\n\n\nAdaBoost (Adaptive Boosting) gbm(y ~ ., distribution = \"adaboost\")\n\nGradient Boosting/Extreme Gradient Boosting (XGBoost) xgboost\n\nhttp://uc-r.github.io/gbm_regression\nhttps://xgboost.readthedocs.io/en/stable/R-package/xgboostPresentation.html\nhttps://github.com/dmlc/xgboost\nhttps://cran.r-project.org/web/packages/xgboost/xgboost.pdf\n\n\nBayesian Additive Regression Trees (BART) (ISL Sec. 8.2.4)\n\nhttps://jamleecute.web.app/gradient-boosting-machines-gbm/\n\n\n\n\nmssc6250-s25.github.io/website"
  },
  {
    "objectID": "slides/03-bias-var.html#supervised-learning-1",
    "href": "slides/03-bias-var.html#supervised-learning-1",
    "title": "Bias-Variance Tradeoff ð¯",
    "section": "Supervised Learning",
    "text": "Supervised Learning\nSupervised learning investigates and models the relationships between responses and inputs."
  },
  {
    "objectID": "slides/03-bias-var.html#relationship-as-functions",
    "href": "slides/03-bias-var.html#relationship-as-functions",
    "title": "Bias-Variance Tradeoff ð¯",
    "section": "Relationship as Functions",
    "text": "Relationship as Functions\n\nRepresent relationships between variables using functions y = f(x).\n\nPlug in the inputs and receive the output.\n\ny = f(x) = 3x + 7 is a function with input x and output y.\nIf x = 5, y = 3 \\times 5 + 7 = 22.\n\n\n\n\n\nIn mathematics, how do we describe a Relationship Between Variables? We use a function. Right.\nThe function y = f(x) gives us the relationship between an output Y and one or more inputs x.\n\nYou plug in the values of inputs and receive back the output value.\nFor example, the formula y = f(x) = 3x + 7 is a function with input x and output y. If x = 5, y = 3 \\times 5 + 7 = 22.\n\n\nBecause this is a linear function, we know that x and y are linearly related.\nWith a value of x, I can give you 100% correct value of y, which is right on this straight line. right. In other words, their relationship is 100% deterministic."
  },
  {
    "objectID": "slides/03-bias-var.html#different-relationships",
    "href": "slides/03-bias-var.html#different-relationships",
    "title": "Bias-Variance Tradeoff ð¯",
    "section": "Different Relationships",
    "text": "Different Relationships\n\n\n\nCan you come up with any real-world examples describing relationships between variables deterministically?\n\n\nThe relationship between x and y can be more than linear.\nThe relationship can be also quadratic, cubic or any other possible relationship."
  },
  {
    "objectID": "slides/03-bias-var.html#different-relationships-1",
    "href": "slides/03-bias-var.html#different-relationships-1",
    "title": "Bias-Variance Tradeoff ð¯",
    "section": "Different Relationships",
    "text": "Different Relationships\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: https://www.ck12.org/physics/acceleration-due-to-gravity/lesson/Acceleration-Due-to-Gravity-MS-PS/\n\n\n\n\n\n\nHere I give your two examples. The first example is the conversion of F and C degrees. Their relationship is linear and F = 32 + 1.8 C.\nSo you give me a C degree, I can tell you its corresponding F degree fro sure. Right.\nThe second example comes from physics. the displacement of an object is a quadratic function of time.\nSo here s(t) = v0 * t + 0.5 * a * t^2. v0 is the initial velocity, and a is acceleration, and t is time.\nAgain the relationship between displacement and time is 100% deterministic.\nA value of time corresponds to an unique value of displacement given v0 and a."
  },
  {
    "objectID": "slides/03-bias-var.html#relationship-between-variables-is-not-perfect",
    "href": "slides/03-bias-var.html#relationship-between-variables-is-not-perfect",
    "title": "Bias-Variance Tradeoff ð¯",
    "section": "Relationship between Variables is Not Perfect",
    "text": "Relationship between Variables is Not Perfect\n\n\n\nCan you provide some real examples that the variables are related each other, but not perfectly related?"
  },
  {
    "objectID": "slides/03-bias-var.html#relationship-between-variables-is-not-perfect-1",
    "href": "slides/03-bias-var.html#relationship-between-variables-is-not-perfect-1",
    "title": "Bias-Variance Tradeoff ð¯",
    "section": "Relationship between Variables is Not Perfect",
    "text": "Relationship between Variables is Not Perfect\n\n\nðµ In general, one with more years of education earns more.\nðµ Any two with the same years of education may have different annual income.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere I give you a simple example: the relationship between income and years of education.\nðµ In general, one with more years of education earns more.\nðµ Any two with the same years of education may have different annual income.\nBecause your income level depends on so many other factors, not just years of education.\nSo when you plot the scatter plot of the two variables, you will find that there is some trend, but the data are sort of scattered or jittered or variated around some function that describes the relationship between income the years of education.\n\nRed dots are observed values or the years of education and income pairs."
  },
  {
    "objectID": "slides/03-bias-var.html#variation-around-the-functionmodel",
    "href": "slides/03-bias-var.html#variation-around-the-functionmodel",
    "title": "Bias-Variance Tradeoff ð¯",
    "section": "Variation around the Function/Model",
    "text": "Variation around the Function/Model\n\n\nWhat are the unexplained variation coming from?\n\n\n\n\n\n\nOther factors accounting for parts of variability of income.\n\nAdding more explanatory variables to a model can reduce the variation size around the model.\n\n\nPure measurement error.\nJust that randomness plays a big role. ð¤\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat other factors (variables) may affect a personâs income?\n\n\n\nyour income = f(years of education, major, GPA, college, parent's wealth, ...)\n\nAnd the data Variation around the Function, or in general the regression model is just as important as the model, if not more!\n\nBasically, what statistics does is explain variation in the context of what remains unexplained.\nThe scatter plot suggests that there might be other factors that account for large parts of variability.\nIf that is the case, adding more explanatory variables ( Xs ) to a model can sometimes usefully reduce the size of the scatter around the model.\nPerhaps just that randomness plays a big role."
  },
  {
    "objectID": "slides/03-bias-var.html#supervised-learning-mapping",
    "href": "slides/03-bias-var.html#supervised-learning-mapping",
    "title": "Bias-Variance Tradeoff ð¯",
    "section": "Supervised Learning Mapping",
    "text": "Supervised Learning Mapping\n\n\n\nExplain the relationship between X and Y and make predictions through a model Y = f(X) + \\epsilon\n\n\n\\epsilon: irreducible random error (Aleatoric Uncertainty)\n\nindependent of X\n\n\nmean zero with some variance.\n\n\n\nf(\\cdot): unknown function1 describing the relationship between X and (the mean of) Y. (Epistemic Uncertainty)\n\n\n\n\nIn Intro Stats, what is the form of f and what assumptions you made on the random error \\epsilon ?\n\n\n\n\n\nf(X) = \\beta_0 + \\beta_1X with unknown parameters \\beta_0 and \\beta_1.\n\n\\epsilon \\sim N(0, \\sigma^2).\n\n\nOK. Now after collecting the data of the variables we are interested, we know their relationship, most of the time, is not perfect, and stochastic in some way and in some sense.\nAnd how do we model such stochastic relationship? Well the answer is a regression model.\nSuppose we are interested in the relationship between two variables, call X and Y. In particular, we like to know how changes of X affect value of Y, or we want to use X to predict Y.\nIn this sense, Y is called response, outcome, label, dependent variable, e.g., income\n\n\nX is called predictor, covariate, feature, regressor, explanatory or independent variable, e.g., years of education, which is known and fixed.\nExplain the relationship between X and Y and make predictions through a model Y = f(X) + \\epsilon. This is a very general regression model we can built to learn the relationship b/w x and y.\n\nf(\\cdot) is fixed but unknown and describes the true relationship between X and Y. \n\n\n\\epsilon is a irreducible random error which is assumed to be independent of X and has mean zero with some variance.\n\n\\epsilon is used to represent those measurement errors or the variation that cannot be explained or captured by the predictor X.\nIntro Stats:\n\n\nf(X) = \\beta_0 + \\beta_1X with unknown parameters \\beta_0 and \\beta_1.\n\n\\epsilon \\sim N(0, \\sigma^2).\n\n\n\nX and Y are assumed to be linearly related, which may not be correct.\nNext week, we will learn simple linear regression from the scratch and in much more detail. Here I just give you an overview.\n\n\nf(\\cdot) is assumed fixed from frequentist point of view; f(\\cdot) is random in the Bayesian framework."
  },
  {
    "objectID": "slides/03-bias-var.html#true-unknown-function-f-of-the-model-y-fx-epsilon",
    "href": "slides/03-bias-var.html#true-unknown-function-f-of-the-model-y-fx-epsilon",
    "title": "Bias-Variance Tradeoff ð¯",
    "section": "True Unknown Function f of the Model Y = f(X) + \\epsilon\n",
    "text": "True Unknown Function f of the Model Y = f(X) + \\epsilon\n\n\n\n\nBlue curve: true underlying relationship between (the mean) income and years of education.\n\nBlack lines: error associated with each observation\n\n\n\nBig problem: f(x) is unknown and needs to be estimated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLetâs go back to the income-education example. Red dots are observed values or the years of education and income pairs. Suppose we like to use a regression model y = f(x) + \\epsilon to describe the relationship between income and education.\nAnd the *Blue** curve on the right shows the true underlying relationship between income and years of education, which is the function f in our regression model.\nAnd each Black vertical line indicates an error associated with each observation.\nSo again, each red dot or observation is the value of the function f(x) plus some random error with its magnitude shown in a black vertical line.\nAgain, in regression, we assume years of education is fixed. It is income level that varies around the function f."
  },
  {
    "objectID": "slides/03-bias-var.html#how-to-estimate-f",
    "href": "slides/03-bias-var.html#how-to-estimate-f",
    "title": "Bias-Variance Tradeoff ð¯",
    "section": "How to Estimate f?",
    "text": "How to Estimate f?\n\nUse training data \\mathcal{D} = \\{ (x_i, y_i) \\}_{i=1}^n (E) to train or teach our model to learn f (T).\nUse test data \\mathcal{D}_{test} = \\{ (x_j, y_j) \\}_{j=1}^m (E) to test or evaluate how well the model makes inference or prediction (P).\n\n\n\nModels are either parametric or nonparametric.\n\n\n\n\n\nParametric methods involve a two-step model-based approach:\n\n1ï¸â£ Make an assumption about the shape of f, e.g.Â linear regression  f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p \n\n2ï¸â£ Use \\mathcal{D} to train the model, e.g., learn the parameters \\beta_j, j = 0, \\dots, p using least squares.\n\n\n\n\n\n\n\nNonparametric methods do not make assumptions about the shape of f.\n\nSeek an estimate of f that gets close to the data points without being too rough or wiggly."
  },
  {
    "objectID": "slides/03-bias-var.html#parametric-vs.-nonparametric-models",
    "href": "slides/03-bias-var.html#parametric-vs.-nonparametric-models",
    "title": "Bias-Variance Tradeoff ð¯",
    "section": "Parametric vs.Â Nonparametric Models",
    "text": "Parametric vs.Â Nonparametric Models\n\n\n\nParametric (Linear regression)\n\n\n\n\n\n\n\n\n\n\n\nNonparametric (LOESS)"
  },
  {
    "objectID": "slides/03-bias-var.html#no-free-lunch",
    "href": "slides/03-bias-var.html#no-free-lunch",
    "title": "Bias-Variance Tradeoff ð¯",
    "section": "No Free Lunch",
    "text": "No Free Lunch\n\n\nThere is no free lunch in machine learning: no one method dominates all others over all possible data sets.\n\n\n\nAll models are wrong, but some are useful. â George Box (1919-2013)\n\n\n\nFor any given training data, decide which method (model & algorithm) produces the best results.\nSelecting the best approach is one of the most challenging parts of machine learning.\nNeed some way to measure how well its predictions actually match the training/test data.\n\n\n\n\nFor numeric y: mean square error (MSE) for y with \\hat{f}, the estimate of f: \\text{MSE}_{\\texttt{Tr}}(y) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2, \\quad \\quad \\text{MSE}_{\\texttt{Te}}(y) = \\frac{1}{m} \\sum_{j=1}^m (y_j - \\hat{f}(x_j))^2\n\n\n\n\n\nAre \\text{MSE}_{\\texttt{Tr}} and \\text{MSE}_{\\texttt{Te}} the same? When to use which?"
  },
  {
    "objectID": "slides/03-bias-var.html#mean-square-error",
    "href": "slides/03-bias-var.html#mean-square-error",
    "title": "Bias-Variance Tradeoff ð¯",
    "section": "Mean Square Error",
    "text": "Mean Square Error\n\n\n\\text{MSE}_{\\texttt{Tr}} measures how much \\hat{f}(x_i) is close to the training data y_i (goodness of fit). However, most of the time\n\n\n\n\nWe do not care how well the method works on the training data.\n\n\n\n\nWe are interested in the predictive accuracy when we apply our method to previously unseen test data.\n\n\nWe want to know whether \\hat{f}(x_j) is (approximately) equal to y_j, where (x_j, y_j) is previously unseen or a test data point not used in training our model.\n\n\n\n\n\\text{MSE}_{\\texttt{Tr}} or \\text{MSE}_{\\texttt{Te}} is smaller?\n\n\n\n\n\n\\text{MSE}_{\\texttt{Tr}} &lt; \\text{MSE}_{\\texttt{Te}}.\n\n\nTO DO: Provide an EXAMPLE of why training error is smaller\nMost statistical learning methods either directly or indirectly seek to minimize the training MSE.\nTraining data are the information we have and probably only have. Without other constraints or information about how we train the model, we tend to make use of all possible information in the training data to train our model.\nWe want to know whether \\hat{f}(x_j) is (approximately) equal to y_j, where (x_j, y_j) is a previously unseen test observation not used to train the model."
  },
  {
    "objectID": "slides/03-bias-var.html#model-complexityflexibility",
    "href": "slides/03-bias-var.html#model-complexityflexibility",
    "title": "Bias-Variance Tradeoff ð¯",
    "section": "Model Complexity/Flexibility",
    "text": "Model Complexity/Flexibility\n\nA more complex model produces a more flexible or wiggly regression curve \\hat{f}(x) that matches the training data better.\n\ny = \\beta_0+ \\beta_1x + \\beta_2x^2 + \\cdots + \\beta_{10}x^{10} + \\epsilon is more complex than y = \\beta_0+ \\beta_1x + \\epsilon\n\n\n\n\n\nOverfitting: A too complex model fits the training data extremely well and too hard, picking up some patterns and variations simply caused by random noises that are not the properties of the true f, and not existed in the any unseen test data.  \n\n\n\n\nUnderfitting: A model that is too simple to capture complex patterns or shapes of the true f(x). The estimate \\hat{f}(x) is rigid and far away from data."
  },
  {
    "objectID": "slides/03-bias-var.html#section-2",
    "href": "slides/03-bias-var.html#section-2",
    "title": "Bias-Variance Tradeoff ð¯",
    "section": "",
    "text": "How \\text{MSE}_{\\texttt{Tr}} and \\text{MSE}_{\\texttt{Te}} change with model complexity?"
  },
  {
    "objectID": "slides/03-bias-var.html#model-complexityflexibility-and-mse",
    "href": "slides/03-bias-var.html#model-complexityflexibility-and-mse",
    "title": "Bias-Variance Tradeoff ð¯",
    "section": "Model Complexity/Flexibility and MSE",
    "text": "Model Complexity/Flexibility and MSE\n\nItâs common that no test data are available. Can we select a model that minimize \\text{MSE}_{\\texttt{Tr}}, since the training data and test data appear to be closed related?\n\n\n\n\n\nOrange: Under fit\nGreen: Over fit\nBlue: Best fit\n\\color{darkgray}{\\text{MSE}_{\\texttt{Tr}}} is decreasing with the complexity.\n\\color{red}{\\text{MSE}_{\\texttt{Te}}} is U-shaped: goes down then up with the complexity.\n\n\n\nMSE\nOverfit\nUnderfit\n\n\n\nTrain\ntiny\nbig\n\n\nTest\nbig\nbig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo! There is no guarantee that the method with the lowest \\text{MSE}_{\\texttt{Tr}} will also have the lowest \\text{MSE}_{\\texttt{Te}}."
  },
  {
    "objectID": "slides/03-bias-var.html#bias-variance-tradeoff",
    "href": "slides/03-bias-var.html#bias-variance-tradeoff",
    "title": "Bias-Variance Tradeoff ð¯",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\nGiven any new input x_0,\n\n\\text{MSE}_{\\hat{f}} = E\\left[\\left(\\hat{f}(x_0) - f(x_0)\\right)^2\\right] = \\left[\\text{Bias}\\left(\\hat{f}(x_0) \\right)\\right]^2 + \\text{Var}\\left(\\hat{f}(x_0)\\right)\nwhere \\text{Bias}\\left(\\hat{f}(x_0) \\right) = E\\left[ \\hat{f}(x_0)\\right] - f(x_0).\n\n\n\n\n\n\n\nWarning\n\n\n\\hat{f}(x_0) is a random variable! Why?\n\n\n\n\n\nThe expected test MSE of y_0 at x_0 is \\text{MSE}_{y_0} = E\\left[\\left(y_0 - \\hat{f}(x_0)\\right)^2\\right] = \\text{MSE}_{\\hat{f}} + \\text{Var}(\\epsilon)\n\n\n\n\n\n\n\nNote\n\n\n\nWe never know the true expected test MSE, and prefer the model with the smallest expected test MSE estimate.\n\n\n\n\n\nMSE can be decomposed into two effects/measures\nMSE is a combination of two performance measures."
  },
  {
    "objectID": "slides/03-bias-var.html#bias-variance-tradeoff-and-model-complexity",
    "href": "slides/03-bias-var.html#bias-variance-tradeoff-and-model-complexity",
    "title": "Bias-Variance Tradeoff ð¯",
    "section": "Bias-Variance Tradeoff and Model Complexity",
    "text": "Bias-Variance Tradeoff and Model Complexity\n\\text{MSE}_{\\hat{f}} = \\left[\\text{Bias}\\left(\\hat{f}(x_0) \\right)\\right]^2 + \\text{Var}\\left(\\hat{f}(x_0)\\right)\n\nOverfitting: Low bias and High variance\nUnderfitting: High bias and Low variance"
  },
  {
    "objectID": "slides/03-bias-var.html#lab-bias-variance-tradeoff",
    "href": "slides/03-bias-var.html#lab-bias-variance-tradeoff",
    "title": "Bias-Variance Tradeoff ð¯",
    "section": "\nLab: Bias-Variance Tradeoff",
    "text": "Lab: Bias-Variance Tradeoff\n\nModel 1: Under-fitting y = \\beta_0+\\beta_1x+\\epsilon\nModel 2: Right-fitting y = \\beta_0+\\beta_1x+ \\beta_2x^2 + \\epsilon\nModel 3: Over-fitting y = \\beta_0+\\beta_1x+ \\beta_2x^2 + \\cdots + \\beta_9x^9 + \\epsilon\nTo see expectation/bias and variance, we need replicates of training data."
  },
  {
    "objectID": "slides/07-splines.html#when-y-and-x-are-not-linearly-associated",
    "href": "slides/07-splines.html#when-y-and-x-are-not-linearly-associated",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "When \\(y\\) and \\(x\\) are Not Linearly Associated",
    "text": "When \\(y\\) and \\(x\\) are Not Linearly Associated\n\nLinear models can describe non-linear relationship.\nFeature engineering: When \\(y\\) and \\(x\\) are not linearly associated, we transform \\(x\\) (sometimes also \\(y\\)) so that \\(y\\) and the transformed \\(x\\) become linearly related.\n\n\n\nPopular methods\n\nBasis function approach\n\nPolynominal regression (MSSC 5780, ISL 7.1)\nPiecewise regression (MSSC 5780, ISL 7.3, 7.4)\nRegression splines\n\n\nLocal regression (MSSC 5780, ISL 7.6)\nSmoothing splines\nGeneral additive models (GAMs)"
  },
  {
    "objectID": "slides/07-splines.html#polynomial-regression",
    "href": "slides/07-splines.html#polynomial-regression",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\n\nThe \\(d\\)th-order (degree) polynomial model in one variable is \\[y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_dx^d + \\epsilon\\]\n\n\n\n\nWe extend the simple linear regression by mapping \\(x\\) to \\((x, x^2, \\dots, x^d)\\).\n\n\n\nâBayesian Deep Learning and a Probabilistic Perspective of Generalizationâ Wilson and Izmailov (2020) for the rationale of choosing a super high-order polynomial as the regression model.\nExtra flexibility produces undesirable results at the boundaries. Generally speaking, it is unusual to use d greater than 3 or 4 because for large values of d, the polynomial curve can become overly flexible and can take on some very strange shapes. This is especially true near the boundary of the X variable.\ncentering, collinearity"
  },
  {
    "objectID": "slides/07-splines.html#basis-function-approach",
    "href": "slides/07-splines.html#basis-function-approach",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Basis Function Approach",
    "text": "Basis Function Approach\n\nA set of basis functions or transformations that can be applied to a variable \\(x\\):\\[[b_1(x), b_2(x), \\dots , b_K(x)]\\]\n\n\n(Adaptive) basis function model: \\[y = \\beta_0 + \\beta_1 b_1(x) + \\beta_2 b_2(x) + \\cdots + \\beta_K b_K(x) + \\epsilon\\]\n\n\n\nIs polynomial regression a basis function approach?\n\n\n\n\nMonomial basis: \\(\\phi(x) = [1, x, x^2, \\dots, x^K, \\dots]\\)\n\n\nFourier series basis: \\(\\phi(x) = [1, \\cos(\\omega_1x + \\psi_1), \\cos(\\omega_2x + \\psi_2), \\dots]\\)\n\nB-Spline basis\n\n\n\n\n\n\n\nImportant\n\n\n\n\\([b_1(x), b_2(x), \\dots , b_K(x)]\\) are fixed and known."
  },
  {
    "objectID": "slides/07-splines.html#piecewise-regression",
    "href": "slides/07-splines.html#piecewise-regression",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Piecewise regression",
    "text": "Piecewise regression\n\nPolynomial regression imposes a global structure on the non-linear function.\nPiecewise regression allows structural changes in different parts of the range of \\(x\\)\n\n\\[y=\\begin{cases}\n    \\beta_{01} + \\beta_{11}x+ \\beta_{21}x^2 +\\epsilon     & \\quad \\text{if } x &lt; \\xi\\\\\n    \\beta_{02} + \\beta_{12}x+ \\beta_{22}x^2+\\beta_{32}x^3+\\epsilon      & \\quad \\text{if } x \\ge \\xi\n  \\end{cases}\\]\n\nThe joint points of pieces are called knots.\nUsing more knots leads to a more flexible piecewise polynomial.\n\n\nWith \\(K\\) different knots, how many different polynomials do we have?\n\n\nA polynomial regression may provide a poor fit, and increasing the order does not improve the situation.\nThis may happen when the regression function behaves differently in different parts of the range of x ."
  },
  {
    "objectID": "slides/07-splines.html#piecewise-constant-and-linear-regression",
    "href": "slides/07-splines.html#piecewise-constant-and-linear-regression",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Piecewise Constant and Linear Regression",
    "text": "Piecewise Constant and Linear Regression\n\n\n\n\n\nSource: ESL Fig. 5.1"
  },
  {
    "objectID": "slides/07-splines.html#u.s.-birth-rate-from-1917-to-2003",
    "href": "slides/07-splines.html#u.s.-birth-rate-from-1917-to-2003",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "U.S. Birth Rate from 1917 to 20031\n",
    "text": "U.S. Birth Rate from 1917 to 20031\n\n\n\n\n\n\n     Year Birthrate\n1917 1917       183\n1918 1918       184\n1919 1919       163\n1920 1920       180\n1921 1921       181\n1922 1922       173\n1923 1923       168\n1924 1924       177\n1925 1925       172\n1926 1926       170\n1927 1927       164\n1928 1928       152\n1929 1929       145\n1930 1930       145\n1931 1931       139\n1932 1932       132\n1933 1933       126\n1934 1934       130\n1935 1935       130\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe data is only for illustrating ideas of different methods. The methods introduced here are best for inference about relationship between variables of some physical or natural system, not mainly for time series forecasting."
  },
  {
    "objectID": "slides/07-splines.html#a-polynomial-regression-provide-a-poor-fit",
    "href": "slides/07-splines.html#a-polynomial-regression-provide-a-poor-fit",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "A Polynomial Regression Provide a Poor Fit",
    "text": "A Polynomial Regression Provide a Poor Fit\n\nlmfit3 &lt;- lm(Birthrate ~ poly(Year - mean(Year), degree = 3, raw = TRUE),  \n             data = birthrates)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nraw if true, use raw and not orthogonal polynomials. https://stackoverflow.com/questions/19484053/what-does-the-r-function-poly-really-do - A polynomial regression may provide a poor fit, and increasing the order does not improve the situation. - This may happen when the regression function behaves differently in different parts of the range of x."
  },
  {
    "objectID": "slides/07-splines.html#piecewise-polynomials-3-knots-at-1936-60-78",
    "href": "slides/07-splines.html#piecewise-polynomials-3-knots-at-1936-60-78",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Piecewise Polynomials: 3 knots at 1936, 60, 78",
    "text": "Piecewise Polynomials: 3 knots at 1936, 60, 78\n\n\n\n\\[y=\\begin{cases}\n    \\beta_{01} + \\beta_{11}x+ \\epsilon & \\text{if } x &lt; 1936\\\\\n    \\beta_{02} + \\beta_{12}x+\\epsilon  & \\text{if } 1936 \\le x &lt; 1960 \\\\\n    \\beta_{03} + \\beta_{13}x+\\epsilon  & \\text{if } 1960 \\le x &lt; 1978 \\\\\n    \\beta_{04} + \\beta_{14}x+\\epsilon  & \\text{if } 1978 \\le x\n  \\end{cases}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nAny issue of piecewise polynomials?\n\n\nDiscontinuity at knots.\nBecause the regression is too flexible in some sense, allowing two different estimates at knots."
  },
  {
    "objectID": "slides/07-splines.html#splines",
    "href": "slides/07-splines.html#splines",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Splines",
    "text": "Splines\nSplines of degree \\(d\\) are smooth piecewise polynomials of degree \\(d\\) with continuity in derivatives (smoothing) up to degree \\(d-1\\) at each knot.\n\nContinuous Piecewise Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: ESL Fig. 5.1"
  },
  {
    "objectID": "slides/07-splines.html#piecewise-linear-to-continuous-piecewise-linear",
    "href": "slides/07-splines.html#piecewise-linear-to-continuous-piecewise-linear",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Piecewise Linear to Continuous Piecewise Linear",
    "text": "Piecewise Linear to Continuous Piecewise Linear\nHow do we turn the piecewise regression of degree 1 into a regression spline?\n\n\\[y=\\begin{cases}\n    \\beta_{01} + \\beta_{11}x+ \\epsilon & \\quad \\text{if } x &lt; 1936\\\\\n    \\beta_{02} + \\beta_{12}x+\\epsilon  & \\quad \\text{if } 1936 \\le x &lt; 1960 \\\\\n    \\beta_{03} + \\beta_{13}x+\\epsilon  & \\quad \\text{if } 1960 \\le x &lt; 1978 \\\\\n    \\beta_{04} + \\beta_{14}x+\\epsilon  & \\quad \\text{if } 1978 \\le x\n  \\end{cases}\\]\n\n\n\nFor splines of degree 1, we require continuous piecewise linear function\n\n\n\n\\[\\begin{align}\n\\beta_{01} + \\beta_{11}1936 &= \\beta_{02} + \\beta_{12}1936\\\\\n\\beta_{02} + \\beta_{12}1960 &= \\beta_{03} + \\beta_{13}1960\\\\\n\\beta_{03} + \\beta_{13}1978 &= \\beta_{04} + \\beta_{14}1978\n\\end{align}\\]"
  },
  {
    "objectID": "slides/07-splines.html#splinesbs",
    "href": "slides/07-splines.html#splinesbs",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "splines::bs()",
    "text": "splines::bs()\n\nlin_sp &lt;- lm(Birthrate ~ splines::bs(Year, degree = 1, knots = c(1936, 1960, 1978)), \n             data = birthrates)\n\n\n\nThe function is continuous everywhere, also at knots \\(\\xi_1, \\xi_2,\\) and \\(\\xi_3\\), i.e.Â \\(f_{k}(\\xi_k^-) = f_{k+1}(\\xi_k^+)\\).\nLinear everywhere except \\(\\xi_1, \\xi_2,\\) and \\(\\xi_3\\).\nHas a different slope for each region."
  },
  {
    "objectID": "slides/07-splines.html#sec-linearspline",
    "href": "slides/07-splines.html#sec-linearspline",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Splines as Basis Function Model",
    "text": "Splines as Basis Function Model\nFor linear splines with 3 knots,\n\\[\\begin{align}\nb_1(x) &= x\\\\\nb_2(x) &= (x - \\xi_1)_+\\\\\nb_3(x) &= (x - \\xi_2)_+\\\\\nb_4(x) &= (x - \\xi_3)_+\n\\end{align}\\] where\n\n\n\n\\((x - \\xi_k)_+\\) is a truncated power basis function (of power one) such that\n\n\\[(x - \\xi_k)_+=\\begin{cases}\n    x - \\xi_k & \\quad \\text{if }x &gt; \\xi_k\\\\\n    0  & \\quad \\text{otherwise}\n  \\end{cases}\\]\n\\[\\begin{align} y &= \\beta_0 + \\beta_1 b_1(x) + \\beta_2 b_2(x) + \\beta_3 b_3(x) + \\beta_4 b_4(x) + \\epsilon\\\\\n&=\\beta_0 + \\beta_1 x + \\beta_2 (x - \\xi_1)_+ + \\beta_3 (x - \\xi_2)_+ + \\beta_4 (x - \\xi_3)_+ + \\epsilon\n\\end{align}\\]"
  },
  {
    "objectID": "slides/07-splines.html#linear-splines-basis-functions",
    "href": "slides/07-splines.html#linear-splines-basis-functions",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Linear Splines Basis Functions",
    "text": "Linear Splines Basis Functions"
  },
  {
    "objectID": "slides/07-splines.html#splines-as-basis-function-model",
    "href": "slides/07-splines.html#splines-as-basis-function-model",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Splines as Basis Function Model1\n",
    "text": "Splines as Basis Function Model1\n\nWith degree \\(d\\) and \\(K\\) knots, the regression spline with first \\(d-1\\) derivatives being continuous at the knots can be represented as\n\\[\\begin{align} y &= \\beta_0 + \\beta_1 b_1(x) + \\beta_2 b_2(x) + \\dots + \\beta_d b_d(x) + \\beta_{d+1}b_{d+1}(x) + \\dots + \\beta_{d+K}b_{d+K}(x) + \\epsilon\n\\end{align}\\] where\n\n\\(b_j(x) = x^j, j = 1, 2, \\dots, d\\)\n\\(b_{d+k} = (x - \\xi_k)^{d}_+, k = 1, \\dots, K\\) where \\[(x - \\xi_k)^d_+=\\begin{cases}\n  (x - \\xi_k)^d & \\quad \\text{if }x &gt; \\xi_k\\\\\n  0  & \\quad \\text{otherwise}\n\\end{cases}\\]\n\n\nCan you write the model with \\(d = 2\\) and \\(K = 3\\)?\n\nBesides truncated power basis, we can represent the splines using another set of basis functions, for example B-Splines which is more computationally convenient and numerically accurate."
  },
  {
    "objectID": "slides/07-splines.html#cubic-splines",
    "href": "slides/07-splines.html#cubic-splines",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Cubic Splines1\n",
    "text": "Cubic Splines1\n\n\nThe cubic spline is a spline of degree 3 with first 2 derivatives are continuous at the knots.\n\n\\[\\begin{align} y &= \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\sum_{k = 1}^K \\beta_{k+3} (x - \\xi_k)_+^3 + \\epsilon\n\\end{align}\\]\n\nSmooth at knots because\n\n\\(f(\\xi_k^-) = f(\\xi_k^+)\\)\n\\(f'(\\xi_k^-) = f'(\\xi_k^+)\\)\n\\(f''(\\xi_k^-) = f''(\\xi_k^+)\\)\nBut \\(f^{(3)}(\\xi_k^-) \\ne f^{(3)}(\\xi_k^+)\\)\n\n\n\nCubic splines are popular because most human eyes cannot detect the discontinuity at the knots."
  },
  {
    "objectID": "slides/07-splines.html#degrees-of-freedom",
    "href": "slides/07-splines.html#degrees-of-freedom",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\n\nIn regression, (effective) degrees of freedom (df) can be viewed as the number of regression coeffcients that are freely to move.\nIt measures the model complexity/flexiblity. The larger df is, the more flexible the model is.\n\n\n\n\nPiecewise linear: 3 knots, 4 linear regressions, and 8 dfs.\n\n\n\n\\[y=\\begin{cases}\n    \\beta_{01} + \\beta_{11}x+ \\epsilon & \\quad \\text{if } x &lt; 1936\\\\\n    \\beta_{02} + \\beta_{12}x+\\epsilon  & \\quad \\text{if } 1936 \\le x &lt; 1960 \\\\\n    \\beta_{03} + \\beta_{13}x+\\epsilon  & \\quad \\text{if } 1960 \\le x &lt; 1978 \\\\\n    \\beta_{04} + \\beta_{14}x+\\epsilon  & \\quad \\text{if } 1978 \\le x\n  \\end{cases}\\]\n\n\n\n\n\nLinear splines (continuous piecewise linear): 8 - 3 constraints = 5 dfs. (Check its basis representation)\n\n\n\\[\\begin{align}\n\\beta_{01} + \\beta_{11}1936 &= \\beta_{02} + \\beta_{12}1936\\\\\n\\beta_{02} + \\beta_{12}1960 &= \\beta_{03} + \\beta_{13}1960\\\\\n\\beta_{03} + \\beta_{13}1978 &= \\beta_{04} + \\beta_{14}1978\n\\end{align}\\]\n\ntalk about effective df, so that we know the model flexibility and complexity of a regression spline model"
  },
  {
    "objectID": "slides/07-splines.html#degrees-of-freedom-1",
    "href": "slides/07-splines.html#degrees-of-freedom-1",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\n\nWith degree \\(d\\) and \\(K\\) knots,\n\n\\[\\text{df} = K + d + 1\\]\n\n\n\nSo what is the degrees of freedom of cubic splines?\n\n\nWith degree \\(d\\) and \\(K\\) knots,\n\n\\[\\begin{align} \\text{df} &= \\text{(# regions)} \\times \\text{(# coefficients)} - \\text{(# constraints)} \\times \\text{(# knots)}\\\\\n&=(K + 1)(d + 1) - dK\\\\\n&= K + d + 1 \\end{align} \\]\n\n. . .\n\n\nCubic splines:\n\nWe have 3 constraints:\n\ncontinuity \\(f(\\xi ^ {-}) = f(\\xi ^ {+})\\)\n\n1st derivative \\(f'(\\xi ^ {-}) = f'(\\xi ^ {+})\\)\n\n2nd derivative \\(f''(\\xi ^ {-}) = f''(\\xi ^ {+})\\)\n\n\n\\(\\text{df} = \\text{(# regions)} \\times \\text{(# coef)} - \\text{(# constraints)} \\times \\text{(# knots)} = (K + 1)4 - 3K = 4 + K\\)"
  },
  {
    "objectID": "slides/07-splines.html#cubic-splines-1",
    "href": "slides/07-splines.html#cubic-splines-1",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Cubic Splines",
    "text": "Cubic Splines\n\ncub_sp &lt;- lm(Birthrate ~ splines::bs(Year, degree = 3, knots = c(1936, 1960, 1978)), \n             data = birthrates)"
  },
  {
    "objectID": "slides/07-splines.html#natural-splines",
    "href": "slides/07-splines.html#natural-splines",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Natural Splines",
    "text": "Natural Splines\n\nSplines1 tends to produce erratic and undesirable results near the boundaries, and huge variance at the outer range of the predictors.\n\n\n\nA natural spline is a regression spline with additional boundary constraints:\n\nThe spline function is linear at the boundary (in the region where \\(X\\) is smaller than the smallest knot, or larger than the largest knot).\n\n\n\n\n\n\n\nNatural splines generally produce more stable estimates at the boundaries.\nAssuming linearity near the boundary is reasonable since there is less information available there.\n\n\nsplines::ns()\n\n\n\n\nNatural Cubic Spline (NCS) forces the 2nd and 3rd derivatives to be zero at the boundaries.\nThe constraints frees up 4 dfs.\nThe df of NCS is \\(K\\).\n\n\nEspecially for high-degree polynomials."
  },
  {
    "objectID": "slides/07-splines.html#spline-and-natural-spline-comparison",
    "href": "slides/07-splines.html#spline-and-natural-spline-comparison",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Spline and Natural Spline Comparison",
    "text": "Spline and Natural Spline Comparison\n\nCubic Spline vs.Â Natural Cubic Spline with the same degrees of freedom 6.\n\n\nhttps://stackoverflow.com/questions/17930140/extracting-knot-points-from-glm-when-using-bs-in-r-as-a-variable attr(fit.bs$terms, âpredvarsâ)"
  },
  {
    "objectID": "slides/07-splines.html#practical-issues",
    "href": "slides/07-splines.html#practical-issues",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Practical Issues",
    "text": "Practical Issues\n\n\nHow many knots should we use\n\n\nAs few knots as possible, with at least 5 data points per segment (Wold, 1974)\nCross-validation: e.g., choose the \\(K\\) giving the smallest \\(MSE_{CV}\\)\n\n\n\n\n\n\nWhere to place the knots\n\n\nNo more than one extreme or inflection point per segment (Wold, 1974)\nIf possible, the extreme points should be centered in the segment\n\nMore knots in places where the function might vary most rapidly, and fewer knots where it seems more stable\nPlace knots in a uniform fashion\n\nSpecify the desired df, and have the software place the corresponding number of knots at uniform quantiles of the data (bs(), ns())\n\n\n\n\n\n\n\nDegree of functions in each region\n\nUse \\(d &lt; 4\\) to avoid overly flexible curve fitting\nCubic spline is popular\n\n\n\nToo many knots can overfit Regression splines often give superior results to polynomial regression"
  },
  {
    "objectID": "slides/07-splines.html#roughness-penalty-approach",
    "href": "slides/07-splines.html#roughness-penalty-approach",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Roughness Penalty Approach",
    "text": "Roughness Penalty Approach\n\nOur goal is inference or curve fitting rather than out of range prediction or forecasting.\nIs there a method that we can select the number and location of knots automatically?\n\nConsider this criterion for fitting a smooth function \\(g(x)\\) to some data:\n\\[\\begin{equation} \\min_{g} \\sum_{i=1}^n \\left( y_i - g(x_i) \\right)^2 + \\lambda \\int g''(t)^2 \\, dt \\end{equation}\\]\n\nThe first term is \\(SS_{res}\\), and tries to make \\(g(x)\\) match the data at each \\(x_i\\). (Goodness-of-fit)\nThe second term with \\(\\lambda \\ge 0\\) is a roughness penalty and controls how wiggly \\(g(x)\\) is.\n\n\n\n\n\nLoss + Penalty\n\n\n\n\n2nd term: penalizes the variability in g\nthe second derivative of a function is a measure of its roughness\nthe integral simply a measure of the total change in the function gâ²(t), over its entire range"
  },
  {
    "objectID": "slides/07-splines.html#roughness-penalty-approach-1",
    "href": "slides/07-splines.html#roughness-penalty-approach-1",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Roughness Penalty Approach",
    "text": "Roughness Penalty Approach\n\\[\\begin{equation} \\min_{g} \\sum_{i=1}^n \\left( y_i - g(x_i) \\right)^2 + \\lambda \\int g''(t)^2 \\, dt \\end{equation}\\]\n\nHow \\(\\lambda\\) affects the shape of \\(g(x)\\)?\n\n\nThe smaller \\(\\lambda\\) is, the more wiggly \\(g(x)\\) is, eventually interpolating \\(y_i\\) when \\(\\lambda = 0\\).\nAs \\(\\lambda \\rightarrow \\infty\\), \\(g(x)\\) becomes linear.\nThe function \\(g\\) that minimizes the objective is known as a smoothing spline.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe solution \\(\\hat{g}\\) is a natural cubic spline, with knots at \\(x_1, x_2, \\dots , x_n\\)!\nBut, it is not the same NCS gotten from the basis function approach.\nIt is a shrunken version where \\(\\lambda\\) controls the level of shrinkage.\nIts effective df is less than \\(n\\)."
  },
  {
    "objectID": "slides/07-splines.html#properties-of-smoothing-splines",
    "href": "slides/07-splines.html#properties-of-smoothing-splines",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Properties of Smoothing splines",
    "text": "Properties of Smoothing splines\n\nSmoothing splines avoid the knot-selection issue, leaving a single \\(\\lambda\\) to be chosen.\n\n\n\nLinear regression\n\n\\(\\hat{\\mathbf{y}} = \\bf H \\mathbf{y}\\) where \\(\\bf H\\) is the hat matrix\nThe degrees of freedom \\((\\text{# coef})\\) is \\[p = \\sum_{i=1}^n {\\bf H}_{ii}\\]\nPRESS for model selection\n\n\\[\\begin{align} PRESS &= \\sum_{i = 1}^n \\left( y_i - \\hat{y}_i^{(-i)}\\right)^2 \\\\\n&= \\sum_{i = 1}^n \\left[ \\frac{y_i - \\hat{y}_i}{1 - {\\bf H}_{ii}}\\right]^2\\end{align}\\]\n\nSmoothing splines\n\n\\(\\hat{\\mathbf{g}}_{\\lambda} = {\\bf S}_{\\lambda} \\mathbf{y}\\) where \\(\\bf {\\bf S}_{\\lambda}\\) is the smoother matrix.\nThe effective degrees of freedom \\((\\text{# coef})\\) is \\[df_{\\lambda} = \\sum_{i=1}^n \\{{\\bf S}_{\\lambda}\\}_{ii} \\in [2, n]\\]\nLOOCV for choosing \\(\\lambda\\)1\n\n\\[\\begin{align} (SS_{res})_{CV}(\\lambda) &= \\sum_{i = 1}^n \\left( y_i - \\hat{g}_{\\lambda}^{(-i)}(x_i)\\right)^2 \\\\\n&= \\sum_{i = 1}^n \\left[ \\frac{y_i - \\hat{g}_{\\lambda}(x_i)}{1 - \\{{\\bf S}_{\\lambda}\\}_{ii}}\\right]^2 \\end{align}\\]\n\n\nGCV can be used too."
  },
  {
    "objectID": "slides/07-splines.html#smooth.spline",
    "href": "slides/07-splines.html#smooth.spline",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "smooth.spline()",
    "text": "smooth.spline()\n\nsmooth.spline(x, y, df, lambda, cv = FALSE)\n\n\n\ncv = TRUE use LOOCV; cv = FALSE use GCV\n\n\nfit &lt;- smooth.spline(birthrates$Year, birthrates$Birthrate)\n\n\n\nfit$df\n\n[1] 60.8\n\n\noverfitting since the birthrate data has little variation in adjacent years"
  },
  {
    "objectID": "slides/07-splines.html#smooth.spline-1",
    "href": "slides/07-splines.html#smooth.spline-1",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "smooth.spline()",
    "text": "smooth.spline()\n\nfit &lt;- smooth.spline(birthrates$Year, birthrates$Birthrate, df = 15)"
  },
  {
    "objectID": "slides/07-splines.html#extending-splines-to-multiple-variables",
    "href": "slides/07-splines.html#extending-splines-to-multiple-variables",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "Extending Splines to Multiple Variables",
    "text": "Extending Splines to Multiple Variables\n\nAll methods discussed so far are extensions of simple linear regression.\nHow to flexibly predict \\(y\\) or nonlinear regression function on the basis of several predictors \\(x_1, \\dots, x_p\\)?\n\n\nMaintaining the additive structure of linear models, but allowing nonlinear functions of each of the variables.\n\\[y = \\beta_0 + f_1(x_1) + f_2(x_2) + \\dots + f_p(x_p) + \\epsilon\\]\n\nThis is a generalized additive model.\n\nItâs general: it can be modelling response with other distributions, binary, counts, positive values, for example.\nItâs additive: calculate a separate \\(f_j\\) for each \\(x_j\\), and add together all of their contributions.\n\n\n\n\n\n\n\n\\(f_1(x_1) = x_1\\); \\(f_2(x_2) = x_2 + x_2^2\\); \\(f_3(x_3) = \\text{cubic splines}\\)"
  },
  {
    "objectID": "slides/07-splines.html#wage-data-in-isl",
    "href": "slides/07-splines.html#wage-data-in-isl",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "\nWage data in ISL",
    "text": "Wage data in ISL\n\nlibrary(ISLR2)\nattach(Wage)\ndplyr::glimpse(Wage)\n\nRows: 3,000\nColumns: 11\n$ year       &lt;int&gt; 2006, 2004, 2003, 2003, 2005, 2008, 2009, 2008, 2006, 2004,â¦\n$ age        &lt;int&gt; 18, 24, 45, 43, 50, 54, 44, 30, 41, 52, 45, 34, 35, 39, 54,â¦\n$ maritl     &lt;fct&gt; 1. Never Married, 1. Never Married, 2. Married, 2. Married,â¦\n$ race       &lt;fct&gt; 1. White, 1. White, 1. White, 3. Asian, 1. White, 1. White,â¦\n$ education  &lt;fct&gt; 1. &lt; HS Grad, 4. College Grad, 3. Some College, 4. College â¦\n$ region     &lt;fct&gt; 2. Middle Atlantic, 2. Middle Atlantic, 2. Middle Atlantic,â¦\n$ jobclass   &lt;fct&gt; 1. Industrial, 2. Information, 1. Industrial, 2. Informatioâ¦\n$ health     &lt;fct&gt; 1. &lt;=Good, 2. &gt;=Very Good, 1. &lt;=Good, 2. &gt;=Very Good, 1. &lt;=â¦\n$ health_ins &lt;fct&gt; 2. No, 2. No, 1. Yes, 1. Yes, 1. Yes, 1. Yes, 1. Yes, 1. Yeâ¦\n$ logwage    &lt;dbl&gt; 4.32, 4.26, 4.88, 5.04, 4.32, 4.85, 5.13, 4.72, 4.78, 4.86,â¦\n$ wage       &lt;dbl&gt; 75.0, 70.5, 131.0, 154.7, 75.0, 127.1, 169.5, 111.7, 118.9,â¦"
  },
  {
    "objectID": "slides/07-splines.html#gamgam",
    "href": "slides/07-splines.html#gamgam",
    "title": "Splines and General Additive Models ã°ï¸",
    "section": "gam::gam()",
    "text": "gam::gam()\n\nlibrary(gam)\ngam.m3 &lt;- gam(wage ~ s(year, df = 4) + s(age , df = 5) + education, data = Wage)\n\n\ns() for smoothing splines\nCoefficients not that interesting; fitted functions are.\n\n\n\n\n\n\n\n\n\nupper and lower pointwise twice-standard-error curves are included\nThe left-hand panel indicates that holding age and education fixed, wage tends to increase slightly with year; this may be due to inflation. The center panel indicates that holding education and year fixed, wage tends to be highest for intermediate values of age, and lowest for the very young and very old.\n\nBecause the model is additive, we can examine the effect of each \\(X_j\\) on \\(Y\\) individually while holding all of the other variables fixed.\nAllow to fit a non-linear \\(f_j\\) to each \\(X_j\\), so that we can automatically model non-linear relationships that standard linear regression will miss.\nNo need to manually try out many different transformations on each variable individually.\nThe non-linear fits can potentially make more accurate predictions for the response.\nThe smoothness of the function \\(f_j\\) for the variable \\(X_j\\) can be summarized via degrees of freedom.\nThe model is additive. For fully general models, we look for more flexible approaches such as random forests and boosting.\n\n\n\n\nmssc6250-s25.github.io/website"
  },
  {
    "objectID": "slides/09-class-glm.html#non-gaussian-and-non-continuous-response",
    "href": "slides/09-class-glm.html#non-gaussian-and-non-continuous-response",
    "title": "Logistic Regression \n",
    "section": "Non-Gaussian and Non-continuous Response",
    "text": "Non-Gaussian and Non-continuous Response\n\n\nIn many applications, the response or error term are nonnormal.\n\n\nBinary response: how an online banking system determine whether or not a transaction is fraudulent based on the userâs IP address.\n\nCount response: how weather conditions affect the number of users of a bike sharing program during a particular hour of the day.\n\n\n\n\n\nBesides normal, the response in a generalized linear model (GLM) can be Bernoulli, binomial, Poisson, etc.\nThere is no assumption that \\(\\mathrm{Var}(y_i)\\) is homogeneous: Both \\(E(y_i)\\) and \\(\\mathrm{Var}(y_i)\\) may vary with the regressors from data point to data point.\nOrdinary least squares does not apply when the response is not normal."
  },
  {
    "objectID": "slides/09-class-glm.html#classification",
    "href": "slides/09-class-glm.html#classification",
    "title": "Logistic Regression \n",
    "section": "Classification",
    "text": "Classification\n\nLinear regression assumes that the response \\(Y\\) is numerical.\nIn many situations, \\(Y\\) is categorical.\n\n\n\nA process of predicting categorical response is known as classification.\n\n\n\nNormal vs.Â COVID vs.Â Smoking\n\n\n\n\n\n\n\n\n\nfake news vs.Â true news"
  },
  {
    "objectID": "slides/09-class-glm.html#regression-function-fx-vs.-classifier-cx",
    "href": "slides/09-class-glm.html#regression-function-fx-vs.-classifier-cx",
    "title": "Logistic Regression \n",
    "section": "Regression Function \\(f(x)\\) vs.Â Classifier \\(C(x)\\)\n",
    "text": "Regression Function \\(f(x)\\) vs.Â Classifier \\(C(x)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://daviddalpiaz.github.io/r4sl/classification-overview.html"
  },
  {
    "objectID": "slides/09-class-glm.html#soft-and-hard-classifiers",
    "href": "slides/09-class-glm.html#soft-and-hard-classifiers",
    "title": "Logistic Regression \n",
    "section": "Soft and Hard Classifiers",
    "text": "Soft and Hard Classifiers\nTwo popular approaches when modeling binary data\n\n\nSoft classifiers\n\nEstimate the conditional probabilities \\(Pr(Y = k \\mid {\\bf X})\\) for each category \\(k\\).\nUse \\(\\mathbf{1}\\{Pr(Y \\mid {\\bf X}) &gt; c \\}\\) for classification, where \\(c \\in(0, 1)\\) is a threshold or cutoff.\ne.g.Â logistic regression\n\n\n\n\n\n\nHard classifiers\n\nDirectly estimate the classification decision boundary\ne.g.Â support vector machines"
  },
  {
    "objectID": "slides/09-class-glm.html#classification-example",
    "href": "slides/09-class-glm.html#classification-example",
    "title": "Logistic Regression \n",
    "section": "Classification Example",
    "text": "Classification Example\nGiven the training data \\(\\{(x_i, y_i)\\}_{i=1}^n\\), we build a classifier to predict whether people will default on their credit card payment \\((Y)\\) yes or no, based on monthly credit card balance \\((X)\\)."
  },
  {
    "objectID": "slides/09-class-glm.html#why-not-linear-regression",
    "href": "slides/09-class-glm.html#why-not-linear-regression",
    "title": "Logistic Regression \n",
    "section": "Why Not Linear Regression",
    "text": "Why Not Linear Regression\n\\[Y =\\begin{cases}\n    0  & \\quad \\text{if not default}\\\\\n    1  & \\quad \\text{if default}\n     \\end{cases}\\]\n\n\n\\(Y = \\beta_0 + \\beta_1X + \\epsilon\\), \\(\\, X =\\) credit card balance\n\n\nWhat are potential issues with this dummy variable approach?\n\n\n\n\n\\(\\hat{Y} = b_0 + b_1X\\) estimates \\(P(Y = 1 \\mid X) = P(default = yes \\mid balance)\\).\n\n\n\n\nHowever,"
  },
  {
    "objectID": "slides/09-class-glm.html#why-not-linear-regression-1",
    "href": "slides/09-class-glm.html#why-not-linear-regression-1",
    "title": "Logistic Regression \n",
    "section": "Why Not Linear Regression?",
    "text": "Why Not Linear Regression?\n\nProbability estimates can be outside \\([0, 1]\\)."
  },
  {
    "objectID": "slides/09-class-glm.html#why-not-linear-regression-2",
    "href": "slides/09-class-glm.html#why-not-linear-regression-2",
    "title": "Logistic Regression \n",
    "section": "Why Not Linear Regression?",
    "text": "Why Not Linear Regression?\n\nLinear regression generally cannot handle more than two categories.\n\n\\[Y =\\begin{cases}\n    1  & \\quad \\text{stroke}\\\\\n    2  & \\quad \\text{drug overdose} \\\\\n    3  & \\quad \\text{epileptic seisure}\n     \\end{cases}\\]\n\n\nThe coding\n\nsuggests an ordering epileptic seisure \\(&gt;\\) drug overdose \\(&gt;\\) stroke\n\nimplies that stroke \\(-\\) drug overdose \\(=\\) drug overdose \\(-\\) epileptic seizure."
  },
  {
    "objectID": "slides/09-class-glm.html#binary-logistic-regression-1",
    "href": "slides/09-class-glm.html#binary-logistic-regression-1",
    "title": "Logistic Regression \n",
    "section": "Binary Logistic Regression",
    "text": "Binary Logistic Regression\n\nFirst predict the probability of each category of \\(Y\\)\n\nPredict probability of default using a S-shaped curve."
  },
  {
    "objectID": "slides/09-class-glm.html#framing-the-problem-binary-responses",
    "href": "slides/09-class-glm.html#framing-the-problem-binary-responses",
    "title": "Logistic Regression \n",
    "section": "Framing the Problem: Binary Responses",
    "text": "Framing the Problem: Binary Responses\n\nWe use normal distribution for numerical \\(y\\). What distribution we can use for binary \\(y\\) that takes value 0 or 1?\n\n\n\nEach outcome default \\((y = 1)\\) and not default \\((y = 0)\\) is a Bernoulli variable. But,\n\n\n\n\nThe probability of âsuccessâ \\(\\pi\\) is not constant but varies with predictor values!\n\n\n\\[ y_i \\mid x_i \\stackrel{indep}{\\sim} \\text{Bernoulli}(\\pi_i = \\pi(x_i)) = \\text{binomial}(m=1,\\pi = \\pi_i) \\]\n\n\n\n\n\\(X =\\) balance. \\(x_1 = 2000\\) has a larger \\(\\pi_1 = \\pi(2000)\\) than \\(\\pi_2 = \\pi(500)\\) with \\(x_2 = 500\\)\n\nCredit cards with a higher balance are more likely to be defaulted.\n\n\n\n\n\n\n\n\n\n\n\n\nIn the credit card example, - do we have exactly two outcomes? - do we have constant probability? \\(P(y_1 = 1) = P(y_2 = 1) = \\cdots = P(y_n = 1) = \\pi?\\) Exactly the same idea as linear regression, where the the mean response is not constant but varies with the value of predictors, \\(E(y_i) = \\beta_0+\\beta_1x_i\\)."
  },
  {
    "objectID": "slides/09-class-glm.html#logistic-function",
    "href": "slides/09-class-glm.html#logistic-function",
    "title": "Logistic Regression \n",
    "section": "Logistic Function",
    "text": "Logistic Function\n\nInstead of predicting \\(y_i\\) directly, we use the predictors to model its probability of success, \\(\\pi_i\\).\n\n\nBut how?\n\n\n\nTransform \\(\\pi \\in (0, 1)\\) into a variable \\(\\eta \\in (-\\infty, \\infty)\\). Then construct a linear predictor on \\(\\eta\\): \\(\\eta_i = \\mathbf{x}_i'\\boldsymbol \\beta\\)\n\n\n\n\n\nLogit function: For \\(0 &lt; \\pi &lt; 1\\)\n\n\n\\[\\eta = \\text{logit}(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\]"
  },
  {
    "objectID": "slides/09-class-glm.html#logit-function-eta-textlogitpi-lnleftfracpi1-piright",
    "href": "slides/09-class-glm.html#logit-function-eta-textlogitpi-lnleftfracpi1-piright",
    "title": "Logistic Regression \n",
    "section": "Logit function \\(\\eta = \\text{logit}(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\)\n",
    "text": "Logit function \\(\\eta = \\text{logit}(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\)"
  },
  {
    "objectID": "slides/09-class-glm.html#logistic-function-1",
    "href": "slides/09-class-glm.html#logistic-function-1",
    "title": "Logistic Regression \n",
    "section": "Logistic Function",
    "text": "Logistic Function\n\nThe logit function \\(\\eta = \\text{logit}(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\) takes a value \\(\\pi \\in (0, 1)\\) and maps it to a value \\(\\eta \\in (-\\infty, \\infty)\\).\n\nLogistic function: \\[\\pi = \\text{logistic}(\\eta) = \\frac{1}{1+\\exp(-\\eta)} \\in (0, 1)\\]\n\nThe logistic (sigmoid) function takes a value \\(\\eta \\in (-\\infty, \\infty)\\) and maps it to a value \\(\\pi \\in (0, 1)\\).\n\n\n\nOnce \\(\\eta\\) is estimated by the linear regression, we use the logistic function to transform \\(\\eta\\) back to the probability.\n\n\\[\\pi = \\text{logistic}(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)} = \\frac{1}{1+\\exp(-\\eta)} \\in (0, 1)\\]"
  },
  {
    "objectID": "slides/09-class-glm.html#logistic-sigmoid-function-pi-textlogisticeta-frac11exp-eta",
    "href": "slides/09-class-glm.html#logistic-sigmoid-function-pi-textlogisticeta-frac11exp-eta",
    "title": "Logistic Regression \n",
    "section": "Logistic (Sigmoid) Function \\(\\pi = \\text{logistic}(\\eta) = \\frac{1}{1+\\exp(-\\eta)}\\)\n",
    "text": "Logistic (Sigmoid) Function \\(\\pi = \\text{logistic}(\\eta) = \\frac{1}{1+\\exp(-\\eta)}\\)"
  },
  {
    "objectID": "slides/09-class-glm.html#logistic-regression-model",
    "href": "slides/09-class-glm.html#logistic-regression-model",
    "title": "Logistic Regression \n",
    "section": "Logistic Regression Model",
    "text": "Logistic Regression Model\nFor \\(i = 1, \\dots, n\\) and with \\(p\\) predictors: \\[Y_i \\mid \\pi_i({\\bf x}_i) \\stackrel{indep}{\\sim} \\text{Bernoulli}(\\pi_i), \\quad {\\bf x}_i' = (x_{i1}, \\dots, x_{ip})\\] \\[\\text{logit}(\\pi_i) = \\ln \\left( \\frac{\\pi_i}{1 - \\pi_i} \\right) = \\eta_i = \\beta_0+\\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip} = {\\bf x}_i'\\boldsymbol \\beta\\]\n\nThe \\(\\eta_i = \\text{logit}(\\pi_i)\\) is a link function that links the linear predictor and the mean of \\(Y_i\\).\n\n\n\\[\\small E(Y_i) = \\pi_i = \\frac{1}{1 + \\exp(-{\\bf x}_i'\\boldsymbol \\beta)}\\] \\[\\small \\hat{\\pi}_i = \\frac{1}{1 + \\exp(-{\\bf x}_i'\\hat{\\boldsymbol \\beta} )}\\]"
  },
  {
    "objectID": "slides/09-class-glm.html#default-data-in-islr2",
    "href": "slides/09-class-glm.html#default-data-in-islr2",
    "title": "Logistic Regression \n",
    "section": "\nDefault Data in ISLR2\n",
    "text": "Default Data in ISLR2\n\n\ndata(\"Default\"); head(Default, 10)\n\n   default student balance income\n1       No      No     730  44362\n2       No     Yes     817  12106\n3       No      No    1074  31767\n4       No      No     529  35704\n5       No      No     786  38463\n6       No     Yes     920   7492\n7       No      No     826  24905\n8       No     Yes     809  17600\n9       No      No    1161  37469\n10      No      No       0  29275\n\nstr(Default)\n\n'data.frame':   10000 obs. of  4 variables:\n $ default: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ student: Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 2 1 1 ...\n $ balance: num  730 817 1074 529 786 ...\n $ income : num  44362 12106 31767 35704 38463 ..."
  },
  {
    "objectID": "slides/09-class-glm.html#fitting-1",
    "href": "slides/09-class-glm.html#fitting-1",
    "title": "Logistic Regression \n",
    "section": "Fitting 1\n",
    "text": "Fitting 1\n\n\nlogit_fit &lt;- glm(default ~ balance, data = Default, \n                 family = binomial)\ncoef(summary(logit_fit))\n\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -10.6513    0.36116     -29 3.6e-191\nbalance       0.0055    0.00022      25 2.0e-137\n\n\n\\(\\hat{\\eta} = \\text{logit}(\\hat{\\pi}) = \\ln \\left( \\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\right) = -10.65 + 0.0055 \\times \\text{balance}\\)\n\n\n\n\n\n\n\n\n\n\n\n\nIn Python, use statsmodels.formula.api.logit or sklearn.linear_model.LogisticRegression"
  },
  {
    "objectID": "slides/09-class-glm.html#interpretation-of-coefficients",
    "href": "slides/09-class-glm.html#interpretation-of-coefficients",
    "title": "Logistic Regression \n",
    "section": "Interpretation of Coefficients",
    "text": "Interpretation of Coefficients\nThe ratio \\(\\frac{\\pi}{1-\\pi} \\in (0, \\infty)\\) is called the odds of some event.\n\nExample: If 1 in 5 people will default, the odds is 1/4 since \\(\\pi = 0.2\\) implies an odds of \\(0.2/(1â0.2) = 1/4\\).\n\n\\[\\ln \\left( \\frac{\\pi(x)}{1 - \\pi(x)} \\right)= \\beta_0 + \\beta_1x\\]\n\nIncreasing \\(x\\) by one unit\n\nchanges the log-odds by \\(\\beta_1\\)\n\nmultiplies the odds by \\(e^{\\beta_1}\\)\n\n\n\n\n\n\n\n\\(\\beta_1\\) does not correspond to the change in \\(\\pi(x)\\) associated with a one-unit increase in \\(x\\).\n\n\\(\\beta_1\\) is the change in log odds associated with one-unit increase in \\(x\\)."
  },
  {
    "objectID": "slides/09-class-glm.html#interpretation-of-coefficients-1",
    "href": "slides/09-class-glm.html#interpretation-of-coefficients-1",
    "title": "Logistic Regression \n",
    "section": "Interpretation of Coefficients",
    "text": "Interpretation of Coefficients\n\n\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  -10.651       0.36     -29        0\nbalance        0.005       0.00      25        0\n\n\n\n\\(\\hat{\\eta} = \\text{logit}(\\hat{\\pi}) = \\ln \\left( \\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\right) = -10.65 + 0.0055 \\times \\text{balance}\\)\n\n\n\n\\(\\hat{\\eta}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1x\\)\n\\(\\hat{\\eta}(x+1) = \\hat{\\beta}_0 + \\hat{\\beta}_1(x+1)\\)\n\\(\\hat{\\eta}(x+1) - \\hat{\\eta}(x) = \\hat{\\beta}_1 = \\ln(\\text{odds}_{x+1}) - \\ln(\\text{odds}_{x})\\)\nOne-unit increase in balance increases the log odds of default by 0.005 units.\n\n\n\n\nThe odds ratio, \\(\\widehat{OR} = \\frac{\\text{odds}_{x+1}}{\\text{odds}_{x}} = e^{\\hat{\\beta}_1} = e^{0.0055} = 1.005515\\).\nThe odds of default increases by 0.5% with additional one unit of credit card balance."
  },
  {
    "objectID": "slides/09-class-glm.html#prdefault-when-balance-is-2000",
    "href": "slides/09-class-glm.html#prdefault-when-balance-is-2000",
    "title": "Logistic Regression \n",
    "section": "Pr(default) When Balance is 2000",
    "text": "Pr(default) When Balance is 2000\n\\[\\log\\left(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\right) = -10.65+0.0055\\times 2000\\]\n\\[ \\hat{\\pi} = \\frac{1}{1+\\exp(-(-10.65+0.0055 \\times 2000)} = 0.586\\]\n\n\npi_hat &lt;- predict(logit_fit, type = \"response\")\neta_hat &lt;- predict(logit_fit, type = \"link\")  ## default gives us b0 + b1*x\npredict(logit_fit, newdata = data.frame(balance = 2000), type = \"response\")\n\n   1 \n0.59"
  },
  {
    "objectID": "slides/09-class-glm.html#probability-curve",
    "href": "slides/09-class-glm.html#probability-curve",
    "title": "Logistic Regression \n",
    "section": "Probability Curve",
    "text": "Probability Curve\n\nWhat is the probability of default when the balance is 500? What about balance 2500?\n\n\n\n\n\n\n\n\n\n\n\n\n\n500 balance: Pr(default) = 0\n2000 balance: Pr(default) = 0.59\n2500 balance: Pr(default) = 0.96"
  },
  {
    "objectID": "slides/09-class-glm.html#maximum-likelihood-estimation",
    "href": "slides/09-class-glm.html#maximum-likelihood-estimation",
    "title": "Logistic Regression \n",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\n\nglm() uses maximum likelihood to estimate the parameters \\(\\boldsymbol \\beta\\).\n\nThe log-likelihood is given by\n\\[\\ell(\\boldsymbol \\beta) = \\sum_{i=1}^n \\log \\, p(y_i \\mid x_i, \\boldsymbol \\beta).\\]\nUsing Bernoulli probabilities, we have\n\\[\\begin{align}\n\\ell(\\boldsymbol \\beta) =& \\sum_{i=1}^n \\log \\left\\{ \\pi(\\mathbf{x}_i)^{y_i} [1-\\pi(\\mathbf{x}_i)]^{1-y_i} \\right\\}\\\\\n    =& \\sum_{i=1}^n y_i \\log \\frac{\\pi(\\mathbf{x}_i)}{1-\\pi(\\mathbf{x}_i)} + \\log [1-\\pi(\\mathbf{x}_i)] \\\\\n    =& \\sum_{i=1}^n y_i \\mathbf{x}_i' \\boldsymbol \\beta- \\log [ 1 + \\exp(\\mathbf{x}_i' \\boldsymbol \\beta)]\n\\end{align}\\]"
  },
  {
    "objectID": "slides/09-class-glm.html#maximum-likelihood-estimation-1",
    "href": "slides/09-class-glm.html#maximum-likelihood-estimation-1",
    "title": "Logistic Regression \n",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\nWe can use Newtonâs method that needs the gradient and Hessian matrix.\n\n\\[\\boldsymbol \\beta^{\\text{new}} = \\boldsymbol \\beta^{\\text{old}} - \\left[ \\frac{\\partial ^2 \\ell(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta\\partial \\boldsymbol \\beta'}\\right] ^{-1}\\frac{\\partial \\ell(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta}\\]\n\n\nSimply use optim()\n\n\n\nx &lt;- as.matrix(cbind(\"intercept\" = 1, Default$balance))\ny &lt;- as.matrix(as.numeric(Default$default) - 1)\nbeta &lt;- rep(0, ncol(x))  ## start with 0\n\noptim(beta, fn = my_loglik, gr = my_gradient, method = \"BFGS\", \n      x = x, y = y, \n      control = list(\"fnscale\" = -1))$par # \"fnscale\" = -1 for maximization\n\n[1] -10.6454   0.0055"
  },
  {
    "objectID": "slides/09-class-glm.html#evaluation",
    "href": "slides/09-class-glm.html#evaluation",
    "title": "Logistic Regression \n",
    "section": "Evaluation1\n",
    "text": "Evaluation1\n\n\nConfusion Matrix\n\n\n\n\n\n\n\n\n\nTrue 0\nTrue 1\n\n\n\nPredict 0\nTrue Negative (TN)\nFalse Negative (FN)\n\n\nPredict 1\nFalse Positive (FP)\nTrue Positive (TP)\n\n\n\n\nSensitivity (True Positive Rate) \\(= P( \\text{predict 1} \\mid \\text{true 1}) = \\frac{TP}{TP+FN}\\)\nSpecificity (True Negative Rate) \\(= P( \\text{predict 0} \\mid \\text{true 0}) = \\frac{TN}{FP+TN}\\)\nAccuracy \\(= \\frac{TP + TN}{TP+FN+FP+TN} = \\frac{1}{m}\\sum_{j=1}^mI(y_j = \\hat{y}_j)\\), where \\(y_j\\)s are true test labels and \\(\\hat{y}_j\\)s are their corresponding predicted label.\n\n\nA good classifier can be the one which the test accuracy rate is highest, or test error rate \\(\\frac{1}{m}\\sum_{j=1}^mI(y_j \\ne \\hat{y}_j)\\) is smallest.\n\n\nMore on Wiki page."
  },
  {
    "objectID": "slides/09-class-glm.html#confusion-matrix-2",
    "href": "slides/09-class-glm.html#confusion-matrix-2",
    "title": "Logistic Regression \n",
    "section": "Confusion Matrix 1\n",
    "text": "Confusion Matrix 1\n\n\npred_prob &lt;- predict(logit_fit, type = \"response\")\ntable(pred_prob &gt; 0.5, Default$default)\n\n       \n          No  Yes\n  FALSE 9625  233\n  TRUE    42  100\n\n\n\nPackages:\n\n\ncaret (Classification And REgression Training)\n\nyardstick of tidymodels\n\n\n\n\n\ncaret::confusionMatrix()\nyardstick::conf_mat()\n\nFor simplicity we use training set for demo. In real applications, we care about the classification result of the test set."
  },
  {
    "objectID": "slides/09-class-glm.html#receiver-operating-characteristic-roc-curve",
    "href": "slides/09-class-glm.html#receiver-operating-characteristic-roc-curve",
    "title": "Logistic Regression \n",
    "section": "Receiver Operating Characteristic (ROC) Curve",
    "text": "Receiver Operating Characteristic (ROC) Curve\n\nThe ROC curve plots True Positive Rate (Sensitivity) vs.Â False Positive Rate (1 - Specificity)\n\n\n\nROC Plot\nCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ROCR)\n# create an object of class prediction \npred &lt;- ROCR::prediction(\n    predictions = pred_prob, \n    labels = Default$default)\n# calculates the ROC curve\nroc &lt;- ROCR::performance(\n    prediction.obj = pred, \n    measure = \"tpr\",\n    x.measure = \"fpr\")\nplot(roc, colorize = TRUE, lwd = 3)\n\n\n\n\n\nPackages: ROCR, pROC, yardstick::roc_curve()"
  },
  {
    "objectID": "slides/09-class-glm.html#area-under-curve-auc",
    "href": "slides/09-class-glm.html#area-under-curve-auc",
    "title": "Logistic Regression \n",
    "section": "Area Under Curve (AUC)",
    "text": "Area Under Curve (AUC)\nFind the area under the curve:\n\n\n\n## object of class 'performance'\nauc &lt;- ROCR::performance(\n    prediction.obj = pred, \n    measure = \"auc\")\nauc@y.values\n\n[[1]]\n[1] 0.95\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrecision-recall curve"
  },
  {
    "objectID": "slides/09-class-glm.html#threshold-0.5-issue",
    "href": "slides/09-class-glm.html#threshold-0.5-issue",
    "title": "Logistic Regression \n",
    "section": "Threshold 0.5 Issue",
    "text": "Threshold 0.5 Issue\n\nThreshold 0.5 may be inappropriate if\n\nimbalanced data or skewed class distribution in training\nthe cost of one type of misclassification is more important than another.\n\n\n\n\n0.5 threshold may misclassify too many minority class samples. Using smaller threshold Example: If only 1% of the data is fraud, a model predicting âno fraudâ for everything would still achieve 99% accuracy, but it fails at detecting fraud.\nIf false positive is more costly (Marking a real email as spam (False Positive) can be frustrating), want to be more conservative, so use larger threshold.\nIf false negative is more costly (Medical Diagnosis: Missing cancer (False Negative) is much worse), want to be more aggressive, so use smaller threshold."
  },
  {
    "objectID": "slides/09-class-glm.html#optimal-threshold",
    "href": "slides/09-class-glm.html#optimal-threshold",
    "title": "Logistic Regression \n",
    "section": "Optimal Threshold",
    "text": "Optimal Threshold\nFrom the training set, choose the cut-off that maximizes\n\nG-Mean \\(= \\sqrt{\\text{TPR} * \\text{TNR}}\\) (geometric mean of TPR and TNR)\nYoudenâs J index \\(= \\text{TPR} + \\text{TNR} - 1\\) (the distance to the 45 degree identity line)\n\nor that minimizes\n\n\nD-optimal Threshold \\(= \\sqrt{(1 - \\text{TPR})^2 + (1 - \\text{TNR})^2}\\) (the distance to the optimal point)\n\nG-mean used When dealing with imbalanced datasets (rare events).\nYoudenâs J Index used when Equal importance of positives & negatives.\nD-optimal Threshold Does not assume class balance, unlike some other metrics. When minimizing both false positives and false negatives."
  },
  {
    "objectID": "slides/09-class-glm.html#optimal-threshold-1",
    "href": "slides/09-class-glm.html#optimal-threshold-1",
    "title": "Logistic Regression \n",
    "section": "Optimal Threshold",
    "text": "Optimal Threshold\nWith Youdenâs J index, the optimal threshold is \\(3.18\\%\\).\n\n\nThe predicted probabilities are not calibrated, e.g.Â those predicted by an SVM or decision tree.\nThe metric used to train the model is different from the metric used to evaluate a final model.\nThe class distribution is severely skewed.\nThe cost of one type of misclassification is more important than another type of misclassification.\nLearning from Imbalanced Data Sets 1st ed.Â 2018 Edition\nhttps://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/ The index is represented graphically as the height above the chance line, and it is also equivalent to the area under the curve subtended by a single operating point. https://web.expasy.org/pROC/files/pROC_1.7.2_R_manual.pdf"
  },
  {
    "objectID": "slides/09-class-glm.html#multinomial-logistic-regression-1",
    "href": "slides/09-class-glm.html#multinomial-logistic-regression-1",
    "title": "Logistic Regression \n",
    "section": "Multinomial Logistic Regression",
    "text": "Multinomial Logistic Regression\n\nWhen classifying \\(K &gt; 2\\) categories, we can consider multinomial logistic regression1.\nThe response should be nominal. If \\(Y_i\\) is ordinal, we should consider the ordinal regression.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSometimes it is known as multiclass logistic regression."
  },
  {
    "objectID": "slides/09-class-glm.html#multinomial-link",
    "href": "slides/09-class-glm.html#multinomial-link",
    "title": "Logistic Regression \n",
    "section": "Multinomial Link",
    "text": "Multinomial Link\n\nFirst select a single class to serve as the baseline, the \\(K\\)th class for example.1\n\n\n\nFor \\(k = 1, 2, \\dots, K-1,\\)\n\\[Pr(Y = k \\mid {\\bf x}) = \\dfrac{e^{\\beta_{k0}+\\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p}}{ \\sum_{l=1}^{K}e^{\\beta_{l0}+\\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}},\\] and \\[Pr(Y = K \\mid {\\bf x}) = \\dfrac{1}{ \\sum_{l=1}^{K}e^{\\beta_{l0}+\\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}}.\\]\n\n\n\nFor \\(k = 1, \\dots, K-1\\), \\[\\log\\left( \\frac{Pr(Y = k \\mid \\mathbf{x})}{Pr(Y = K \\mid \\mathbf{x})} \\right) = \\beta_{k0}+\\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p.\\]\n\n\nThe coefficient estimates will differ if a different baseline is used. But the fitted values, predictions, log odds between any pair of classes remain the same."
  },
  {
    "objectID": "slides/09-class-glm.html#multinomial-link-1",
    "href": "slides/09-class-glm.html#multinomial-link-1",
    "title": "Logistic Regression \n",
    "section": "Multinomial Link",
    "text": "Multinomial Link\n\nFirst select a single class to serve as the baseline, the \\(K\\)th class for example.1\n\\(\\pi_{ik} = Pr(Y_i = k \\mid {\\bf x}_i)\\): the probability that the \\(i\\)-th response falls in the \\(k\\)-th category.\n\\(\\sum_{k = 1}^K \\pi_{ik} = 1\\) for each observation \\(i\\).\n\n\n\nFor \\(k = 1, \\dots, K-1\\), the link \\(\\eta_{ik} = \\log\\left( \\frac{\\pi_{ik}}{\\pi_{iK}}\\right) = \\beta_{k0}+\\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p = {\\bf x}_i'\\boldsymbol \\beta_k\\), where \\(\\boldsymbol \\beta_k = (\\beta_{k0}, \\beta_{k1}, \\dots, \\beta_{kp})'\\)\n\n\n\nWhat is the value of \\(\\eta_{iK}\\)?\n\n\n\nFor \\(k = 1, \\dots, K\\),\n\\[\\pi_{ik} = \\dfrac{e^{\\eta_{ik}}}{\\sum_{l=1}^{K}e^{\\eta_{il}}};  \\left(\\pi_{iK} = \\dfrac{1}{\\sum_{l=1}^{K}e^{\\eta_{il}}} \\right)\\]\n\nThe coefficient estimates will differ if a different baseline is used. But the fitted values, predictions, log odds between any pair of classes remain the same."
  },
  {
    "objectID": "slides/09-class-glm.html#multinomial-data",
    "href": "slides/09-class-glm.html#multinomial-data",
    "title": "Logistic Regression \n",
    "section": "Multinomial Data",
    "text": "Multinomial Data\n\n\nRows: 200\nColumns: 3\n$ prog  &lt;fct&gt; vocation, general, vocation, vocation, vocation, general, vocatiâ¦\n$ ses   &lt;fct&gt; low, middle, high, low, middle, high, middle, middle, middle, miâ¦\n$ write &lt;dbl&gt; 35, 33, 39, 37, 31, 36, 36, 31, 41, 37, 44, 33, 31, 44, 35, 44, â¦\n\n\n\nResponse:\n\n\nprog (program type: general, academic, vocation)\n\n\nPredictors:\n\n\nses (social economic status: low, middle, high)\n\nwrite (writing score)\n\n\n\nEntering high school students make program choices among general program, vocational program and academic program. Their choice might be modeled using their writing score and their social economic status."
  },
  {
    "objectID": "slides/09-class-glm.html#multinomial-logit---variable-association",
    "href": "slides/09-class-glm.html#multinomial-logit---variable-association",
    "title": "Logistic Regression \n",
    "section": "Multinomial Logit - Variable Association",
    "text": "Multinomial Logit - Variable Association\nProgram type is affected by social economic status\n\n\n\n        prog\nses      general academic vocation\n  low         16       19       12\n  middle      20       44       31\n  high         9       42        7\n\n\n\n\nand writing score\n\n\n\n          M  SD\ngeneral  51 9.4\nacademic 56 7.9\nvocation 47 9.3\n\n\n\nThe level (coding order) of prog is\n\n\n\n[1] \"general\"  \"academic\" \"vocation\""
  },
  {
    "objectID": "slides/09-class-glm.html#multinomial-logit---setting-baseline",
    "href": "slides/09-class-glm.html#multinomial-logit---setting-baseline",
    "title": "Logistic Regression \n",
    "section": "Multinomial Logit - Setting Baseline",
    "text": "Multinomial Logit - Setting Baseline\n\nmultino_data$prog2 &lt;- relevel(multino_data$prog, ref = \"academic\")\nlevels(multino_data$prog2)\n\n[1] \"academic\" \"general\"  \"vocation\"\n\n\n\n\nacademic = 1 (baseline), general = 2, vocation = 3\nFor ses, low = 1 (baseline), middle = 2, high = 3\n\n\n\n\\[\\log \\left( \\frac{Pr(prog = general)}{Pr(prog = academic)}\\right) = b_{10} + b_{11}I(ses = 2) + b_{12}I(ses = 3) + b_{13}write\\]\n\\[\\log \\left( \\frac{Pr(prog = vocation)}{Pr(prog = academic)}\\right) = b_{20} + b_{21}I(ses = 2) + b_{22}I(ses = 3) + b_{23}write\\]\n\nAll others held constant,\n\n\\(b_{13}\\): the amount changed in the log odds of being in general vs.Â academic program as write increases one unit.\n\\(b_{21}\\) the amount changed in the log odds of being in vocation vs.Â academic program if moving from ses = \"low\" to ses = \"middle\".\n\nset âacademicâ as the baseline category factor(multino_data$prog, levels = c(âgeneralâ, âvocationâ, âacademicâ) b13 A one-unit increase in the variable write is associated with the decrease in the log odds of being in general program vs.Â academic program in the amount of .058 . b23 A one-unit increase in the variable write is associated with the decrease in the log odds of being in vocation program vs.Â academic program. in the amount of .1136 . b12 The log odds of being in general program vs.Â in academic program will decrease by 1.163 if moving from ses=âlowâ to ses=âhighâ. b11 The log odds of being in general program vs.Â in academic program will decrease by 0.533 if moving from ses=âlowâto ses=âmiddleâ, although this coefficient is not significant. b22 The log odds of being in vocation program vs.Â in academic program will decrease by 0.983 if moving from ses=âlowâ to ses=âhighâ. b21 The log odds of being in vocation program vs.Â in academic program will increase by 0.291 if moving from ses=âlowâ to ses=âmiddleâ, although this coefficient is not significant."
  },
  {
    "objectID": "slides/09-class-glm.html#multinomial-logit---nnetmultinom",
    "href": "slides/09-class-glm.html#multinomial-logit---nnetmultinom",
    "title": "Logistic Regression \n",
    "section": "Multinomial Logit - nnet::multinom()\n",
    "text": "Multinomial Logit - nnet::multinom()\n\n\nmultino_fit &lt;- nnet::multinom(prog2 ~ ses + write, data = multino_data, trace = FALSE)\nsumm &lt;- summary(multino_fit)\nsumm$coefficients\n\n         (Intercept) sesmiddle seshigh  write\ngeneral          2.9     -0.53   -1.16 -0.058\nvocation         5.2      0.29   -0.98 -0.114\n\n\n\n\n\nglmnet() uses the softmax coding that treats all \\(K\\) classes symmetrically, and coefficients have a different meaning. (ISL eq. (4.13), (4.14))\nThe fitted values, predictions, log odds between any pair of classes, and other key model outputs remain the same.\n\n\nglmnet(x = input_matrix, y = categorical_vector, family = \"multinom\", \n       family = \"multinomial\", lambda = 0)\n\n\nEstimate coefficients for all \\(K\\) classes.\nThe fitted values, predictions, log odds between any pair of classes, and other key model outputs remain the same."
  },
  {
    "objectID": "slides/09-class-glm.html#multinomial-logit---estimated-probability",
    "href": "slides/09-class-glm.html#multinomial-logit---estimated-probability",
    "title": "Logistic Regression \n",
    "section": "Multinomial Logit - Estimated Probability",
    "text": "Multinomial Logit - Estimated Probability\n\nhead(fitted(multino_fit))\n\n  academic general vocation\n1     0.15    0.34     0.51\n2     0.12    0.18     0.70\n3     0.42    0.24     0.34\n4     0.17    0.35     0.48\n5     0.10    0.17     0.73\n6     0.35    0.24     0.41\n\n\n\nHold write at its mean and examine the predicted probabilities for each level of ses.\n\n\ndses &lt;- data.frame(ses = c(\"low\", \"middle\", \"high\"), \n                   write = mean(multino_data$write))\npredict(multino_fit, newdata = dses, type = \"probs\")\n\n  academic general vocation\n1     0.44    0.36     0.20\n2     0.48    0.23     0.29\n3     0.70    0.18     0.12\n\n\nhttps://stackoverflow.com/questions/22905807/how-does-the-function-multinom-from-r-package-nnet-compute-the-multinomial-proba"
  },
  {
    "objectID": "slides/09-class-glm.html#multinomial-logit---probability-curve",
    "href": "slides/09-class-glm.html#multinomial-logit---probability-curve",
    "title": "Logistic Regression \n",
    "section": "Multinomial Logit - Probability Curve",
    "text": "Multinomial Logit - Probability Curve"
  },
  {
    "objectID": "slides/09-class-glm.html#generalized-linear-models-in-greater-generality",
    "href": "slides/09-class-glm.html#generalized-linear-models-in-greater-generality",
    "title": "Logistic Regression \n",
    "section": "\nGeneralized Linear Models in Greater Generality",
    "text": "Generalized Linear Models in Greater Generality\n\n\nConditional on \\(\\mathbf{X}\\), the response \\(Y\\) belongs to a certain family of distribution.1\n\nLinear regression: Gaussian\nLogistic regression: Bernoulli, binomial, multinomial\n\n\n\n\n\n\n\nModel the mean of \\(Y\\) as a function of the predictors.\n\nLinear regression: \\(E(Y \\mid \\mathbf{x}) = \\mathbf{x}'\\boldsymbol \\beta\\)\n\nLogistic regression: \\(E(Y \\mid \\mathbf{x}) = P(Y=1 \\mid \\mathbf{x}) = \\pi(\\mathbf{x}) = \\frac{e^{\\mathbf{x}'\\boldsymbol \\beta}}{1 + e^{\\mathbf{x}'\\boldsymbol \\beta}}\\)\n\n\n\n\n\n\n\n\nIn general, with \\(E(Y \\mid \\mathbf{x}) = \\mu\\), the link function \\(\\eta(\\mu) = {\\bf x}'\\boldsymbol \\beta\\) transforms \\(\\mu\\) so that the transformed mean is a linear function of predictors!\n\nLinear regression: \\(\\eta(\\mu) = \\mu\\)\n\nLogistic regression: \\(\\eta(\\mu) = \\log(\\mu/(1-\\mu))\\)\n\n\n\n\nAny regression approach that follows this general recipe is known as a generalized linear model (GLM).\n\n\nUsually the distribution of \\(Y\\) belongs to the exponential family."
  },
  {
    "objectID": "slides/09-class-glm.html#link-function",
    "href": "slides/09-class-glm.html#link-function",
    "title": "Logistic Regression \n",
    "section": "Link Function",
    "text": "Link Function\n\nLogistic regression uses the logit function \\(\\eta = g(\\pi) = \\ln\\left( \\dfrac{\\pi}{1-\\pi}\\right)\\) as the link function.\n\\(\\pi = \\dfrac{1}{1 + e^{-\\eta}} \\in (0, 1)\\) is the CDF of the logistic distribution, whose PDF is of the from \\[f(\\eta) = \\frac{e^{-\\eta}}{(1 + e^{-\\eta})^2}, \\quad \\eta \\in(-\\infty, \\infty)\\]\nCould use a different link function and hence CDF to describe the probability curve."
  },
  {
    "objectID": "slides/09-class-glm.html#probit-and-complementary-log-log-cloglog-link",
    "href": "slides/09-class-glm.html#probit-and-complementary-log-log-cloglog-link",
    "title": "Logistic Regression \n",
    "section": "Probit and Complementary log-log (cloglog) Link",
    "text": "Probit and Complementary log-log (cloglog) Link\n\nIf \\(\\pi = \\Phi(\\eta) \\in (0, 1)\\), \\(\\Phi(\\cdot)\\) is the CDF of standard normal distribution, the link function \\[\\eta = g(\\pi) = \\Phi^{-1}(\\pi)\\] is called probit, and the model is called probit model or probit regression.\n\n\n\nAnother popular link function is the complementary log-log link: \\[\\eta = g(\\pi) = \\log\\left( -\\log(1-\\pi)\\right)\\] where \\(\\pi = 1 - \\exp(-\\exp(\\eta))\\) is the CDF of the so called Gumbel distribution.1\n\n\n\nThe complementary log-log link is actually defined by a negative Gumbel random variable. No worries about why it is used at this moment."
  },
  {
    "objectID": "slides/09-class-glm.html#link-function-summary",
    "href": "slides/09-class-glm.html#link-function-summary",
    "title": "Logistic Regression \n",
    "section": "Link Function Summary",
    "text": "Link Function Summary\n\n\n\n\n\n\n\n\nDistribution\nLink Function \\(\\eta = g(\\pi)\\)\n\nCDF \\(\\pi = g^{-1}(\\eta)\\)\n\nPDF\n\n\n\nLogistic\nlogit: \\(\\ln\\left( \\dfrac{\\pi}{1-\\pi}\\right)\\)\n\n\\(\\dfrac{1}{1 + e^{-\\eta}}\\)\n\\(\\dfrac{e^{-\\eta}}{(1 + e^{-\\eta})^2}\\)\n\n\nNormal\nprobit: \\(\\Phi^{-1}(\\pi)\\)\n\n\\(\\Phi(\\eta)\\)\n\\(\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{\\eta^2}{2}}\\)\n\n\nGumbel\ncloglog: \\(\\log\\left( -\\log(1-\\pi)\\right)\\)\n\n\\(1 - \\exp(-\\exp(\\eta))\\)\n\\(e^{-(\\eta + e ^ {-\\eta})}\\)\n\n\n\n\nAny transformation that maps probabilities into the real line could be used to produce a GLM for binary classification, as long as the transformation is one-to-one continuous and differentiable.\n\\[\\pi = F(\\eta)\\] \\[\\eta = F^{-1}(\\pi)\\]"
  },
  {
    "objectID": "slides/09-class-glm.html#link-function-summary-1",
    "href": "slides/09-class-glm.html#link-function-summary-1",
    "title": "Logistic Regression \n",
    "section": "Link Function Summary",
    "text": "Link Function Summary\n\n\nName\nLink Function \\(\\eta = g(\\pi)\\)\n\n\n\n\nlogit\n\\(\\ln\\left( \\dfrac{\\pi}{1-\\pi}\\right)\\)\n\n\nprobit\n\\(\\Phi^{-1}(\\pi)\\)\n\n\ncloglog\n\\(\\log\\left( -\\log(1-\\pi)\\right)\\)\n\n\n\n\nLogit and probit links produce similar curve. For simple regression, both estimate \\(\\pi = 1/2\\) when \\(x = -\\beta_0/\\beta_1\\) and exhibit symmetric behavior.\nThe complementary log-log link is not symmetric."
  },
  {
    "objectID": "slides/09-class-glm.html#pdf",
    "href": "slides/09-class-glm.html#pdf",
    "title": "Logistic Regression \n",
    "section": "PDF",
    "text": "PDF\n\nNormal and logistic density are symmetric, while Gumbel is right skewed."
  },
  {
    "objectID": "slides/09-class-glm.html#probability-curve-1",
    "href": "slides/09-class-glm.html#probability-curve-1",
    "title": "Logistic Regression \n",
    "section": "Probability Curve",
    "text": "Probability Curve"
  },
  {
    "objectID": "slides/09-class-glm.html#probit-and-complementary-log-log-links",
    "href": "slides/09-class-glm.html#probit-and-complementary-log-log-links",
    "title": "Logistic Regression \n",
    "section": "Probit and Complementary log-log Links",
    "text": "Probit and Complementary log-log Links\n\nprobit_fit &lt;- glm(default ~ balance, data = Default, family = binomial(link = \"probit\"))\ncloglog_fit &lt;- glm(default ~ balance, data = Default, family = binomial(link = \"cloglog\"))"
  },
  {
    "objectID": "slides/09-class-glm.html#other-topics",
    "href": "slides/09-class-glm.html#other-topics",
    "title": "Logistic Regression \n",
    "section": "Other Topics",
    "text": "Other Topics\n\nRepeated measures and binomial logistic regression\nRegression diagnostics for binary data\nPenalized logistic regression\nExponential family\nPoisson regression\n\nhttps://grodri.github.io/glms/\nhttps://stats.oarc.ucla.edu/r/dae/multinomial-logistic-regression/\nhttps://glmnet.stanford.edu/articles/glmnet.html\n\n\n\n\n\n\nmssc6250-s25.github.io/website"
  },
  {
    "objectID": "slides/05-ridge-cv.html#when-ordinary-least-squares-ols-doesnt-work-well-collinearity",
    "href": "slides/05-ridge-cv.html#when-ordinary-least-squares-ols-doesnt-work-well-collinearity",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "When Ordinary Least Squares (OLS) Doesnât Work Well: Collinearity",
    "text": "When Ordinary Least Squares (OLS) Doesnât Work Well: Collinearity\n\nWhen predictors are highly correlated, \\(\\mathrm{Var}(b_j)\\) is much inflated.1\n\nA tiny change in the training set causes a large change in \\(b_j\\), leading to unreliable estimation and possibly prediction.\n\n\n\n\n\n\\({\\bf X'X} = \\begin{bmatrix} 1 & 0.99 \\\\ 0.99 & 1 \\end{bmatrix}\\) \\(\\quad ({\\bf X'X})^{-1} = \\begin{bmatrix}  50.3 & -49.7 \\\\ -49.7 & 50.3 \\end{bmatrix}\\)\n\n\\(\\mathrm{Var}(b_j) = 50.3\\sigma^2\\)\n\nAn increase in 50-fold over the ideal case when the two regressors are orthogonal.\n\n\n\n\nsolve(matrix(c(1, 0.99, 0.99, 1), 2, 2))\n\n      [,1]  [,2]\n[1,]  50.3 -49.7\n[2,] -49.7  50.3\n\n\nthe eigen vector direction with the smallest eigen value https://online.stat.psu.edu/stat857/node/155/ https://robjhyndman.com/hyndsight/loocv-linear-models/\n\n\nAlthough \\(\\mathrm{Var}(b_j)\\) is still the smallest among all linear unbiased estimators."
  },
  {
    "objectID": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-optimization-perspective",
    "href": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-optimization-perspective",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "When OLS Doesnât Work Well: Optimization Perspective",
    "text": "When OLS Doesnât Work Well: Optimization Perspective\n\n\\(\\beta_1 = \\beta_2 = 1\\) and \\(\\mathrm{Corr}(x_1, x_2) = 0.99\\)\nThe relatively âflatâ valley in the objective function walks along the eigen-vector of \\({\\bf X}'{\\bf X}\\) having the smallest eigen-value.\n\n\n\n\n\n\n\n\n\n\n\n\n\neigen(\n    matrix(c(1, 0.99, 0.99, 1), \n             2, 2)\n    )\n\neigen() decomposition\n$values\n[1] 1.99 0.01\n\n$vectors\n      [,1]   [,2]\n[1,] 0.707 -0.707\n[2,] 0.707  0.707\n\n\n\n\n\nFrom optimization point of view, the objective function (\\(\\ell_2\\) loss) is âflatâ along certain directions in the parameter domain.\na small eigen-value in XTX makes the corresponding eigen-value large in the inverse."
  },
  {
    "objectID": "slides/05-ridge-cv.html#ridge-and-valley",
    "href": "slides/05-ridge-cv.html#ridge-and-valley",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Ridge and Valley",
    "text": "Ridge and Valley\n\n\n\nStatisticians used to maximize the likelihood, leading to a âridgeâ.\n\n\n\n\n\n\n\n\n\nComputer scientists like to minimize the loss, leading to a âvalleyâ.\n\n\n\n\n\n\n\n\n\nhttps://stats.stackexchange.com/questions/151304/why-is-ridge-regression-called-ridge-why-is-it-needed-and-what-happens-when\nhttps://stats.stackexchange.com/questions/118712/why-does-ridge-estimate-become-better-than-ols-by-adding-a-constant-to-the-diago/119708#119708"
  },
  {
    "objectID": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-optimization-perspective-1",
    "href": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-optimization-perspective-1",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "When OLS Doesnât Work Well: Optimization Perspective",
    "text": "When OLS Doesnât Work Well: Optimization Perspective\n\nA little change in the training set perturbs the objective function. The LSEs lie on a valley centered around the truth."
  },
  {
    "objectID": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-high-variance",
    "href": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-high-variance",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "When OLS Doesnât Work Well: High Variance",
    "text": "When OLS Doesnât Work Well: High Variance\n\nThe optimizer could land anywhere along the valley, leading to large variance of the LSE.\nOver many simulation runs, the LSE lies around the line of \\(\\beta_1 + \\beta_2 = 2\\), the direction of the eigen-vector of the smallest eigen-value.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe shape of the loss function for independent predictors is shown in 04-lin-reg optimization"
  },
  {
    "objectID": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-large-p-small-n-high-dimensional-data",
    "href": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-large-p-small-n-high-dimensional-data",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "When OLS Doesnât Work Well: Large-\\(p\\)-small-\\(n\\) (High Dimensional Data)",
    "text": "When OLS Doesnât Work Well: Large-\\(p\\)-small-\\(n\\) (High Dimensional Data)\n\nOLS stays well in the world of âlarge-\\(n\\)-small-\\(p\\)â.\nWhen \\(p &gt; n\\), \\({\\bf X}'{\\bf X}\\) is not invertible.\nThere is no unique \\(\\boldsymbol \\beta\\) estimate.\n\n\nIntuition: Too many degrees of freedom (\\(p\\)) to specify a model, but not enough information (\\(n\\)) to decide which one is the one.\n\nToo flexible and ends up with overfitting\n\nToo much flexibility about how \\(f\\) can be shaped, but too little information to guide or determine which shape it should be."
  },
  {
    "objectID": "slides/05-ridge-cv.html#remedy-for-large-variance-and-large-p-small-n",
    "href": "slides/05-ridge-cv.html#remedy-for-large-variance-and-large-p-small-n",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Remedy for Large Variance and Large-\\(p\\)-small-\\(n\\)\n",
    "text": "Remedy for Large Variance and Large-\\(p\\)-small-\\(n\\)\n\n\nMake \\({\\bf X}'{\\bf X}\\) invertible when \\(p &gt; n\\) by regularizing coefficient behavior!\nA good estimator balances bias and variance well, or minimizes the mean square error \\[\\text{MSE}(\\hat{\\beta}) = E[(\\hat{\\beta} - \\beta)^2] = \\mathrm{Var}(\\hat{\\beta}) + \\text{Bias}(\\hat{\\beta})^2\\]\nA slightly biased estimator \\(\\hat{\\boldsymbol \\beta}\\) that has much smaller variance and MSE than the LSE \\({\\bf b}\\)."
  },
  {
    "objectID": "slides/05-ridge-cv.html#ridge-regression-1",
    "href": "slides/05-ridge-cv.html#ridge-regression-1",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nIdea: Add a ridge (diagonal matrix) to \\({\\bf X} ' {\\bf X}\\).1 \\[\\widehat{\\boldsymbol \\beta}^\\text{r} = (\\mathbf{X}' \\mathbf{X}+ n \\lambda \\mathbf{I})^{-1} \\mathbf{X}' \\mathbf{y},\\]\n\n\n\n\nTo regularize coefficients, add an \\(\\ell_2\\) penalty to the residual sum of squares, for some tuning parameter \\(\\lambda &gt; 0\\).\n\n\\[\n\\begin{align}\n\\widehat{\\boldsymbol \\beta}^\\text{r} =& \\, \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} \\underbrace{\\lVert \\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta\\rVert_2^2}_{SS_{res}} + n \\lambda \\lVert\\boldsymbol \\beta\\rVert_2^2\\\\\n=& \\, \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} \\underbrace{\\frac{1}{n} \\sum_{i=1}^n (y_i - x_i'\\boldsymbol \\beta)^2}_{\\text{MSE}_{\\texttt{Tr}}} + \\lambda \\sum_{j=1}^p \\beta_j^2\\\\\n=& \\, \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} \\color{green} - \\text{ goodness of fit } + \\text{ model complexity/flexibility}\n\\end{align}\n\\]\n\nThis is a special case of the Tikhonov regularization."
  },
  {
    "objectID": "slides/05-ridge-cv.html#properties-of-ridge-regression",
    "href": "slides/05-ridge-cv.html#properties-of-ridge-regression",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Properties of Ridge Regression",
    "text": "Properties of Ridge Regression\n\\[\n\\begin{align}\n\\widehat{\\boldsymbol \\beta}^\\text{r} =& \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i'\\boldsymbol \\beta)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2,\n\\end{align}\n\\]\nProperties of ridge regression:\n\nHas less degrees of freedom in the sense that the cost gets higher when larger coefficients are used.\n\n\n\nFavors small-magnitude coefficient estimates (Shrinkage) to avoid cost penalty.\n\n\n\n\nThe shrinkage parameter \\(\\lambda\\) controls the degree of penalty.\n\n\n\\(\\lambda \\rightarrow 0\\): No penalty, and \\(\\widehat{\\boldsymbol \\beta}^\\text{r} = \\bf b\\).\n\n\\(\\lambda \\rightarrow \\infty\\): Unbearable penalty, and \\(\\widehat{\\boldsymbol \\beta}^\\text{r} \\rightarrow \\mathbf{0}\\)"
  },
  {
    "objectID": "slides/05-ridge-cv.html#ridge-penalty",
    "href": "slides/05-ridge-cv.html#ridge-penalty",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Ridge Penalty",
    "text": "Ridge Penalty\n\n\n\\[\\lambda \\lVert \\boldsymbol \\beta\\rVert^2_2 = \\lambda \\boldsymbol \\beta' \\mathbf{I}\\boldsymbol \\beta\\]\n\nThe penalty contour is circle-shaped\nForce the objective function to be more convex\nA more stable or less varying solution"
  },
  {
    "objectID": "slides/05-ridge-cv.html#geometrical-meaning-of-ridge-regression",
    "href": "slides/05-ridge-cv.html#geometrical-meaning-of-ridge-regression",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Geometrical Meaning of Ridge Regression",
    "text": "Geometrical Meaning of Ridge Regression\n\n\n\n\n\nSource: https://online.stat.psu.edu/stat508/lessons/Lesson06\n\n\n\n\n\nPerform PCA of X\nProject y onto the PCs\nShrinks the projection\nReassemble the PCs using all the shrunken length"
  },
  {
    "objectID": "slides/05-ridge-cv.html#more-convex-loss-function",
    "href": "slides/05-ridge-cv.html#more-convex-loss-function",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "More Convex Loss Function",
    "text": "More Convex Loss Function\n\nAdding a ridge penalty forces the objective to be more convex due to the added eigenvalues.\n\n\neigen(matrix(c(1, 0.99, 0.99, 1), 2, 2) + diag(2))[1]\n\n$values\n[1] 2.99 1.01\n\n\n\nHence, by adding this to the OLS objective function, the solution is more stable, in the sense that each time we observe a new set of data, this contour looks pretty much the same. This may be interpreted in several different ways such as: 1) the objective function is more convex and less affected by the random samples; 2) the variance of the estimator is smaller because the eigenvalues of \\(\\mathbf{X}^\\text{T}\\mathbf{X}+ n \\lambda \\mathbf{I}\\) are large."
  },
  {
    "objectID": "slides/05-ridge-cv.html#the-bias-variance-tradeoff",
    "href": "slides/05-ridge-cv.html#the-bias-variance-tradeoff",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "The Bias-variance Tradeoff",
    "text": "The Bias-variance Tradeoff\n\nAs \\(\\lambda \\downarrow\\), bias \\(\\downarrow\\) and variance \\(\\uparrow\\)\n\nAs \\(\\lambda \\uparrow\\), bias \\(\\uparrow\\) and variance \\(\\downarrow\\)\n\n\n\nThis effect is gradually changing as we increase the penalty level. The following simulation shows how the variation of \\(\\boldsymbol \\beta\\) changes. We show this with two penalty values, and see how the estimated parameters are away from the truth.\nNow, we may ask the question: is it worth it? In fact, this bias and variance will be then carried to the predicted values \\(x^\\text{T}\\widehat{\\boldsymbol \\beta}^\\text{ridge}\\). Hence, we can judge if this is beneficial from the prediction accuracy. And we need some procedure to do this.\nRemark: The bias-variance trade-off will appear frequently in this course. And the way to evaluate the benefit of this is to see if it eventually reduces the prediction error (\\(\\text{Bias}^2 + \\text{Variance}\\) plus a term called irreducible error, which will be introduced in later chapter)."
  },
  {
    "objectID": "slides/05-ridge-cv.html#lower-test-mse",
    "href": "slides/05-ridge-cv.html#lower-test-mse",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Lower Test MSE",
    "text": "Lower Test MSE\n\n\n\nWhen \\(b_j\\) has large variance or \\(p &gt; n\\), ridge regression could have lower test MSE and better predictive performance.\n\\(\\text{MSE}_{\\texttt{Tr}}\\) (purple)\nSquared bias (black)\nVariance (green)\n\n\n\n\n\n\n\nSource: ISL Figure 6.5"
  },
  {
    "objectID": "slides/05-ridge-cv.html#masslm.ridge",
    "href": "slides/05-ridge-cv.html#masslm.ridge",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "MASS::lm.ridge()",
    "text": "MASS::lm.ridge()\n\nThe lambda parameter in MASS::lm.ridge() specifies the \\(n\\lambda\\) in our notation.\nOLS is scale equivalent: \\(X_jb_j\\) remains the same regardless of how \\(X_j\\) is scaled.\nRidge coefficient estimates can change substantially when multiplying a given predictor by a constant, i.e., \\(X_j\\hat{\\beta}^{r}_{j, \\lambda}\\) depends on \\(\\lambda\\), the scaling of \\(X_j\\), and even the scaling of other predictors.\nStandardize all predictors!1\n\n\n\nhead(mtcars, 3)\n\n               mpg cyl disp  hp drat   wt qsec vs am gear carb\nMazda RX4     21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\nMazda RX4 Wag 21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\nDatsun 710    22.8   4  108  93 3.85 2.32 18.6  1  1    4    1\n\n(fit &lt;- MASS::lm.ridge(mpg ~ ., data = mtcars, lambda = 1)) ## ridge fit\n\n              cyl     disp       hp     drat       wt     qsec       vs \n16.53766 -0.16240  0.00233 -0.01493  0.92463 -2.46115  0.49259  0.37465 \n      am     gear     carb \n 2.30838  0.68572 -0.57579 \n\n\n\n\nIn practice, if the intercept is not our interest, we also standardize the response."
  },
  {
    "objectID": "slides/05-ridge-cv.html#scaling-of-lm.ridge",
    "href": "slides/05-ridge-cv.html#scaling-of-lm.ridge",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Scaling of lm.ridge()\n",
    "text": "Scaling of lm.ridge()\n\n\ncoef(fit) transforms these back to the original scale for non-standardized \\(\\mathbf{X}\\).\nfit$coef shows the coefficients of the standardized predictors.\n\n\ncoef(fit)\n\n              cyl     disp       hp     drat       wt     qsec       vs \n16.53766 -0.16240  0.00233 -0.01493  0.92463 -2.46115  0.49259  0.37465 \n      am     gear     carb \n 2.30838  0.68572 -0.57579 \n\nfit$coef\n\n   cyl   disp     hp   drat     wt   qsec     vs     am   gear   carb \n-0.285  0.285 -1.008  0.487 -2.370  0.866  0.186  1.134  0.498 -0.915"
  },
  {
    "objectID": "slides/05-ridge-cv.html#ridge-trace",
    "href": "slides/05-ridge-cv.html#ridge-trace",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Ridge Trace",
    "text": "Ridge Trace\n\nridge_fit &lt;- lm.ridge(mpg ~ ., data = mtcars, lambda = 0:40)\nmatplot(coef(ridge_fit)[, -1], type = \"l\", xlab = \"Lambda\", ylab = \"Coefficients\")\ntext(rep(1, 10), coef(ridge_fit)[1,-1], colnames(mtcars)[2:11])\n\n\n\nSelect a value of \\(\\lambda\\) at which the ridge estimates are stable."
  },
  {
    "objectID": "slides/05-ridge-cv.html#methods-for-choosing-lambda",
    "href": "slides/05-ridge-cv.html#methods-for-choosing-lambda",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Methods for Choosing \\(\\lambda\\)\n",
    "text": "Methods for Choosing \\(\\lambda\\)\n\n\nMASS::select(ridge_fit)\n\nmodified HKB estimator is 2.59 \nmodified L-W estimator is 1.84 \nsmallest value of GCV  at 15 \n\n\n\nHoerl, Kennard, and Baldwin (1975): \\(\\lambda \\approx \\frac{p \\hat{\\sigma}^2}{{\\bf b}'{\\bf b}}\\)\nLawless and Wang (1976): \\(\\lambda \\approx \\frac{np \\hat{\\sigma}^2}{{\\bf b'X}'{\\bf Xb}}\\)\nGolub, Health, and Wahba (1979): Generalized Cross Validation"
  },
  {
    "objectID": "slides/05-ridge-cv.html#cross-validation",
    "href": "slides/05-ridge-cv.html#cross-validation",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Cross Validation",
    "text": "Cross Validation\n\nCross Validation (CV) is a resampling method.\nResampling methods refit a model of interest to data sampled from the training set.\n\nCV can be used to\n\nestimate the test error when there is no test data. (Model assessment)\n\nselect the tuning parameters that controls the model complexity/flexibility. (Model selection)\n\n\n\n\nHence, by adding this to the OLS objective function, the solution is more stable, in the sense that each time we observe a new set of data, this contour looks pretty much the same. This may be interpreted in several different ways such as: 1) the objective function is more convex and less affected by the random samples; 2) the variance of the estimator is smaller because the eigenvalues of \\(\\mathbf{X}^\\text{T}\\mathbf{X}+ n \\lambda \\mathbf{I}\\) are large."
  },
  {
    "objectID": "slides/05-ridge-cv.html#section-1",
    "href": "slides/05-ridge-cv.html#section-1",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "",
    "text": "\\(k\\)-Fold Cross Validation\n\nRandomly divide the training set into \\(k\\) folds, of approximately equal size.\nUse 1 fold for validation to compute MSE, and the remaining \\(k - 1\\) partitions for training.\nRepeat \\(k\\) times. Each time a different fold is treated as a validation set.\nAverage \\(k\\) metrics, \\(\\text{MSE}_{CV} = \\frac{1}{k}\\sum_{i=1}^k\\text{MSE}_i\\).\nUse the CV estimate \\(\\text{MSE}_{CV}\\) to select the âbestâ tuning parameter.\n\n\n\n\n\n\n\nFive-fold cross validation. Source: Data science in a box\n\n\n\n\nCan compute other performance measures."
  },
  {
    "objectID": "slides/05-ridge-cv.html#glmnet-package",
    "href": "slides/05-ridge-cv.html#glmnet-package",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "\nglmnet package \n",
    "text": "glmnet package \n\n\nThe parameter alpha controls the ridge (alpha = 0) and lasso (alpha = 1) penalties.\nSupply a decreasing sequence of lambda values.\nlm.ridge() use \\(SS_{res}\\) and \\(n\\lambda\\), while glmnet() use \\(\\text{MSE}_{\\texttt{Tr}}\\) and \\(\\lambda\\).\nArgument x should be a matrix.\n\n\n\n\nglmnet(x = data.matrix(mtcars[, -1]), \n       y = mtcars$mpg, \n       alpha = 0, \n       lambda = 5:0/nrow(mtcars))\n\n\n\nlm.ridge(formula = mpg ~ ., \n         data = mtcars, \n         lambda = 5:0)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nglmnet() only use coef() and return coefficients at the original scale.\nlm.ridge() and glmnet() coefficients do not match exactly, especially when transforming back to original scale.\nNo need to worry too much as we focus on predictive performance.\n\n\n\n\n\n\nhttps://stats.stackexchange.com/questions/74206/ridge-regression-results-different-in-using-lm-ridge-and-glmnet\nby default, glmnet standardize predictors and include intercept"
  },
  {
    "objectID": "slides/05-ridge-cv.html#k-fold-cross-validation-using-cv.glmnet",
    "href": "slides/05-ridge-cv.html#k-fold-cross-validation-using-cv.glmnet",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "\n\\(k\\)-Fold Cross Validation using cv.glmnet()1\n",
    "text": "\\(k\\)-Fold Cross Validation using cv.glmnet()1\n\n\nThe \\(\\lambda\\) values are automatically selected, on the \\(\\log_{e}\\) scale.\n\n\nridge_cv_fit &lt;- cv.glmnet(x = data.matrix(mtcars[, -1]), y = mtcars$mpg, alpha = 0,\n                          nfolds = 10, type.measure = \"mse\")\nplot(ridge_cv_fit$glmnet.fit, \"lambda\")\n\n\n\n\n\n\n\n\nWhy s and not lambda? In case we want to allow one to specify the model size in other ways in the future. s: Value(s) of the penalty parameter lambda at which predictions are required. Default is the entire sequence used to create the model.\nThere are other ways to do CV for ridge regression in R, for example, the caret (Classification And REgression Training) package and the rsample package in tidymodels ecosystem."
  },
  {
    "objectID": "slides/05-ridge-cv.html#determine-lambda",
    "href": "slides/05-ridge-cv.html#determine-lambda",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Determine \\(\\lambda\\)\n",
    "text": "Determine \\(\\lambda\\)\n\n\n\n\nplot(ridge_cv_fit)\n\n\n\n\n\n\n\n\n\nridge_cv_fit$lambda.min\n\n[1] 3.31\n\n# largest lambda s.t. error is within 1 s.e of the min\nridge_cv_fit$lambda.1se \n\n[1] 12.2\n\n\n\ncoef(ridge_cv_fit, s = \"lambda.min\")\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                  s1\n(Intercept) 20.97465\ncyl         -0.37636\ndisp        -0.00537\nhp          -0.01139\ndrat         1.05572\nwt          -1.17533\nqsec         0.15874\nvs           0.80136\nam           1.55883\ngear         0.53942\ncarb        -0.52151\n\n\n\n\ncoef(fit2, s = âlambda.1seâ) This plots the cross-validation curve (red dotted line) along with upper and lower standard deviation curves along the ð sequence (error bars). Two special values along the ð sequence are indicated by the vertical dotted lines. lambda.min is the value of ð that gives minimum mean cross-validated error, while lambda.1se is the value of ð that gives the most regularized model such that the cross-validated error is within one standard error of the minimum. https://stats.stackexchange.com/questions/253963/how-to-interpret-cv-glmnet-plot"
  },
  {
    "objectID": "slides/05-ridge-cv.html#generalized-cross-validation",
    "href": "slides/05-ridge-cv.html#generalized-cross-validation",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Generalized Cross-Validation",
    "text": "Generalized Cross-Validation\n\nThe generalized cross-validation (GCV) is a modified version of the leave-one-out CV (LOOCV) (\\(n\\)-fold CV).\nThe LOOCV for linear regression is\n\n\n\\[\\text{CV}_{(n)} = \\frac{1}{n}\\sum_{i=1}^n \\left[ \\frac{y_i - x_i' {\\mathbf{b}}} {1 - {\\bf H}_{ii}} \\right]^2 \\]\n\n\n\nThe GCV criterion is given by\n\n\n\\[\\text{GCV}(\\lambda) = \\frac{1}{n}\\sum_{i=1}^n \\left[ \\frac{y_i - x_i' \\widehat{\\boldsymbol \\beta}^\\text{r}_\\lambda}{1 - \\frac{\\text{Trace}(\\mathbf{S}_\\lambda)}{n}} \\right]^2\\]\n\nwhere \\(\\mathbf{S}_\\lambda\\) is the hat matrix corresponding to the ridge regression:\n\\[\\mathbf{S}_\\lambda = \\mathbf{X}(\\mathbf{X}' \\mathbf{X}+ n\\lambda \\mathbf{I})^{-1} \\mathbf{X}'\\]\nThe interesting fact about leave-one-out CV in the linear regression setting is that we do not need to explicitly fit all leave-one-out models.\n\nESL p.Â 244\nlm.ridge code"
  },
  {
    "objectID": "slides/05-ridge-cv.html#generalized-cross-validation-1",
    "href": "slides/05-ridge-cv.html#generalized-cross-validation-1",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Generalized Cross-Validation",
    "text": "Generalized Cross-Validation\nSelect the best \\(\\lambda\\) that produces the smallest GCV error.\n\nridge_fit &lt;- lm.ridge(mpg ~ ., data = mtcars, lambda = 0:40)\n\n\n\n\nplot(ridge_fit$lambda, \n     ridge_fit$GCV, \n     type = \"l\", col = \"darkgreen\", \n     ylab = \"GCV\", xlab = \"Lambda\", \n     lwd = 3)\n\n\n\n\n\n\n\n\n\nidx &lt;- which.min(ridge_fit$GCV)\nridge_fit$lambda[idx]\n\n[1] 15\n\nround(coef(ridge_fit)[idx, ], 2)\n\n        cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb \n21.13 -0.37 -0.01 -0.01  1.05 -1.23  0.16  0.77  1.62  0.54 -0.55 \n\n\n\n\nYou can clearly see that the GCV decreases initially, as \\(\\lambda\\) increases, this is because the reduced variance is more beneficial than the increased bias. However, as \\(\\lambda\\) increases further, the bias term will eventually dominate and causing the overall prediction error to increase. The fitted MSE under this model is\n\n\n\n\nmssc6250-s25.github.io/website"
  },
  {
    "objectID": "documents/project_slides/group4_win1.html",
    "href": "documents/project_slides/group4_win1.html",
    "title": "MSSC 6250 - Spring 2025",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n\naba = pd.read_csv(\"C:/Users/Rakes/OneDrive/Desktop/stats/p1/abalone_dataset.csv\")\n\n\naba.head(5)\n\n\n\n\n\n\n\n\n\nSex\nLength\nDiameter\nHeight\nWhole_weight\nShucked_weight\nViscera_weight\nShell_weight\nRings\n\n\n\n\n0\nM\n0.455\n0.365\n0.095\n0.5140\n0.2245\n0.1010\n0.150\n15\n\n\n1\nM\n0.350\n0.265\n0.090\n0.2255\n0.0995\n0.0485\n0.070\n7\n\n\n2\nF\n0.530\n0.420\n0.135\n0.6770\n0.2565\n0.1415\n0.210\n9\n\n\n3\nM\n0.440\n0.365\n0.125\n0.5160\n0.2155\n0.1140\n0.155\n10\n\n\n4\nI\n0.330\n0.255\n0.080\n0.2050\n0.0895\n0.0395\n0.055\n7\n\n\n\n\n\n\n\n\n\naba.isnull().sum()\n\nSex               0\nLength            0\nDiameter          0\nHeight            0\nWhole_weight      0\nShucked_weight    0\nViscera_weight    0\nShell_weight      0\nRings             0\ndtype: int64\n\n\n\naba.shape\n\n(4177, 9)\n\n\n\naba.info\n\n&lt;bound method DataFrame.info of      Sex  Length  Diameter  Height  Whole_weight  Shucked_weight  \\\n0      M   0.455     0.365   0.095        0.5140          0.2245   \n1      M   0.350     0.265   0.090        0.2255          0.0995   \n2      F   0.530     0.420   0.135        0.6770          0.2565   \n3      M   0.440     0.365   0.125        0.5160          0.2155   \n4      I   0.330     0.255   0.080        0.2050          0.0895   \n...   ..     ...       ...     ...           ...             ...   \n4172   F   0.565     0.450   0.165        0.8870          0.3700   \n4173   M   0.590     0.440   0.135        0.9660          0.4390   \n4174   M   0.600     0.475   0.205        1.1760          0.5255   \n4175   F   0.625     0.485   0.150        1.0945          0.5310   \n4176   M   0.710     0.555   0.195        1.9485          0.9455   \n\n      Viscera_weight  Shell_weight  Rings  \n0             0.1010        0.1500     15  \n1             0.0485        0.0700      7  \n2             0.1415        0.2100      9  \n3             0.1140        0.1550     10  \n4             0.0395        0.0550      7  \n...              ...           ...    ...  \n4172          0.2390        0.2490     11  \n4173          0.2145        0.2605     10  \n4174          0.2875        0.3080      9  \n4175          0.2610        0.2960     10  \n4176          0.3765        0.4950     12  \n\n[4177 rows x 9 columns]&gt;\n\n\n\naba.dtypes\n\nSex                object\nLength            float64\nDiameter          float64\nHeight            float64\nWhole_weight      float64\nShucked_weight    float64\nViscera_weight    float64\nShell_weight      float64\nRings               int64\ndtype: object\n\n\n\nplt.figure(figsize=(6,4))\nsns.countplot(x=aba[\"Sex\"])\nplt.title(\"Distribution of Abalone Sex\")\nplt.xlabel(\"Sex\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Box plots \ncols = [\"Sex\",\"Length\", \"Diameter\", \"Height\", \"Whole_weight\", \"Shucked_weight\", \"Viscera_weight\", \"Shell_weight\", \"Rings\"]\nplt.figure(figsize=(12,8))\nfor i, col in enumerate(cols, 1):\n    plt.subplot(3, 3, i)\n    sns.boxplot(y=aba[col])\n    plt.title(f\"Boxplot of {col}\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Relationships between features\nsns.pairplot(aba)\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(6,4))\nsns.histplot(aba[\"Height\"], bins=30, kde=True)\nplt.title(\"Distribution of Height\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Scatter plots to visualize growth\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nsns.scatterplot(x=aba[\"Length\"], y=aba[\"Rings\"], hue=aba[\"Sex\"].astype(str), alpha=0.5)\nplt.title(\"Length vs. Rings\")\nplt.xlabel(\"Length\")\nplt.ylabel(\"Rings\")\n\nText(0, 0.5, 'Rings')\n\n\n\n\n\n\n\n\n\n\n# Impute zero height values with mean height\naba.loc[aba[\"Height\"] == 0, \"Height\"] = aba[\"Height\"].mean()\n\n\nfrom scipy.stats.mstats import winsorize\n# Winsorization: Cap extreme values at 5th and 95th percentile\nfor col in [\"Length\", \"Diameter\", \"Height\", \"Whole_weight\", \"Shucked_weight\", \"Viscera_weight\", \"Shell_weight\", \"Rings\"]:\n    aba[col] = winsorize(aba[col], limits=[0.05, 0.05])\n\n\naba = pd.get_dummies(aba, columns=[\"Sex\"])\n\n\n# Compute correlation matrix\ncorr_matrix = aba.corr()\n\n# Plot correlation heatmap\nplt.figure(figsize=(10,6))\nsns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\nplt.title(\"Feature Correlation Heatmap\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Splitting data\nX = aba.drop(columns=[\"Rings\"])\ny = aba[\"Rings\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardizing numerical features\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X.columns, index=X_train.index)\nX_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X.columns, index=X_test.index)\n\nimport statsmodels.api as sm\n# Add constant for OLS\nX_train_ols = sm.add_constant(X_train_scaled)\nX_test_ols = sm.add_constant(X_test_scaled)\n# Fit OLS model\nols_model = sm.OLS(y_train, X_train_ols).fit()\nprint(ols_model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Rings   R-squared:                       0.554\nModel:                            OLS   Adj. R-squared:                  0.553\nMethod:                 Least Squares   F-statistic:                     460.5\nDate:                Fri, 07 Mar 2025   Prob (F-statistic):               0.00\nTime:                        14:00:14   Log-Likelihood:                -6730.3\nNo. Observations:                3341   AIC:                         1.348e+04\nDf Residuals:                    3331   BIC:                         1.354e+04\nDf Model:                           9                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nconst              9.8743      0.031    314.165      0.000       9.813       9.936\nLength            -0.1197      0.209     -0.574      0.566      -0.529       0.290\nDiameter           0.9141      0.210      4.347      0.000       0.502       1.326\nHeight             0.7526      0.085      8.863      0.000       0.586       0.919\nWhole_weight       3.7527      0.342     10.961      0.000       3.081       4.424\nShucked_weight    -4.0647      0.179    -22.722      0.000      -4.415      -3.714\nViscera_weight    -0.8375      0.138     -6.087      0.000      -1.107      -0.568\nShell_weight       0.9626      0.165      5.835      0.000       0.639       1.286\nSex_F              0.0906      0.023      3.899      0.000       0.045       0.136\nSex_I             -0.2235      0.026     -8.542      0.000      -0.275      -0.172\nSex_M              0.1296      0.021      6.047      0.000       0.088       0.172\n==============================================================================\nOmnibus:                      294.355   Durbin-Watson:                   1.970\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              415.293\nSkew:                           0.706   Prob(JB):                     6.61e-91\nKurtosis:                       3.993   Cond. No.                     2.45e+15\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 3.99e-27. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n# Prediction\ny_pred = ols_model.predict(X_test_ols)\n# Model evaluation metrics\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f\"\\nModel Evaluation Metrics:\")\nprint(f\"Mean Squared Error (MSE): {mse:.4f}\")\nprint(f\"Mean Absolute Error (MAE): {mae:.4f}\")\nprint(f\"R-squared (RÂ²): {r2:.4f}\")\n\n\nModel Evaluation Metrics:\nMean Squared Error (MSE): 3.3736\nMean Absolute Error (MAE): 1.4127\nR-squared (RÂ²): 0.5496\n\n\n\n# Scatter plot for OLS Model\nplt.figure(figsize=(6, 6))\nplt.scatter(y_test, y_pred, color='blue', alpha=0.5)\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2)\nplt.xlabel(\"Actual Rings\")\nplt.ylabel(\"Predicted Rings\")\nplt.title(\"Actual vs. Predicted Rings (OLS Model)\")\nplt.text(min(y_test), max(y_pred) - 2, f\"MSE: {mse:.4f}\\nMAE: {mae:.4f}\\nRÂ²: {r2:.4f}\", fontsize=10, verticalalignment='top')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Ridge Regression\nfrom sklearn.linear_model import Ridge\nridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)\ny_ridge_pred = ridge.predict(X_test)\n\nridge_mse = mean_squared_error(y_test, y_ridge_pred)\nridge_mae = mean_absolute_error(y_test, y_ridge_pred)\nridge_r2 = r2_score(y_test, y_ridge_pred)\n\nprint(f\"\\nRidge Regression Evaluation Metrics:\")\nprint(f\"MSE: {ridge_mse:.4f}\")\nprint(f\"MAE: {ridge_mae:.4f}\")\nprint(f\"RÂ²: {ridge_r2:.4f}\")\n\n\nRidge Regression Evaluation Metrics:\nMSE: 3.3697\nMAE: 1.4200\nRÂ²: 0.5502\n\n\n\n# Coefficients Summary for Ridge Regression\nridge_coefficients = pd.Series(ridge.coef_, index=X.columns)\nprint(\"\\nRidge Regression Coefficients:\")\nprint(ridge_coefficients)\n\n\nRidge Regression Coefficients:\nLength             2.134166\nDiameter           5.892062\nHeight             9.001296\nWhole_weight       6.008200\nShucked_weight   -16.912315\nViscera_weight    -4.183926\nShell_weight      10.411092\nSex_F              0.245876\nSex_I             -0.530194\nSex_M              0.284318\ndtype: float64\n\n\n\n# Scatter plot for Ridge Model\nplt.figure(figsize=(6, 6))\nplt.scatter(y_test, y_ridge_pred, color='blue', alpha=0.5)\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2)\nplt.xlabel(\"Actual Rings\")\nplt.ylabel(\"Predicted Rings\")\nplt.title(\"Actual vs. Predicted Rings (Ridge Model)\")\nplt.text(min(y_test), max(y_ridge_pred) - 2, f\"MSE: {ridge_mse:.4f}\\nMAE: {ridge_mae:.4f}\\nRÂ²: {ridge_r2:.4f}\", fontsize=10, verticalalignment='top')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Lasso Regression\nfrom sklearn.linear_model import Lasso\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_train, y_train)\ny_lasso_pred = lasso.predict(X_test)\n\nlasso_mse = mean_squared_error(y_test, y_lasso_pred)\nlasso_mae = mean_absolute_error(y_test, y_lasso_pred)\nlasso_r2 = r2_score(y_test, y_lasso_pred)\n\nprint(f\"\\nLasso Regression Evaluation Metrics:\")\nprint(f\"MSE: {lasso_mse:.4f}\")\nprint(f\"MAE: {lasso_mae:.4f}\")\nprint(f\"RÂ²: {lasso_r2:.4f}\")\n\n# Coefficients Summary for Lasso Regression\nlasso_coefficients = pd.Series(lasso.coef_, index=X.columns)\nprint(\"\\nLasso Regression Coefficients:\")\nprint(lasso_coefficients)\n\n\nLasso Regression Evaluation Metrics:\nMSE: 4.8442\nMAE: 1.7291\nRÂ²: 0.3533\n\nLasso Regression Coefficients:\nLength            0.000000\nDiameter          0.000000\nHeight            0.000000\nWhole_weight      2.429523\nShucked_weight   -0.000000\nViscera_weight    0.000000\nShell_weight      0.000000\nSex_F             0.000000\nSex_I            -0.785001\nSex_M             0.000000\ndtype: float64\n\n\n\n# Scatter plot for Lasso Model\nplt.figure(figsize=(6, 6))\nplt.scatter(y_test, y_lasso_pred, color='blue', alpha=0.5)\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2)\nplt.xlabel(\"Actual Rings\")\nplt.ylabel(\"Predicted Rings\")\nplt.title(\"Actual vs. Predicted Rings (Lasso Model)\")\n\nplt.text(min(y_test) + 0.5, max(y_lasso_pred) + 1,\n         f\"MSE: {lasso_mse:.4f}\\nMAE: {lasso_mae:.4f}\\nRÂ²: {lasso_r2:.4f}\", \n         fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom patsy import dmatrix\n\n# Polynomial Regression\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_train_poly = poly.fit_transform(X_train_scaled)\nX_test_poly = poly.transform(X_test_scaled)\n\npoly_model = sm.OLS(y_train, sm.add_constant(X_train_poly)).fit()\nprint(\"\\nPolynomial Regression Summary:\")\nprint(poly_model.summary())\n\n# Predictions\ny_poly_pred = poly_model.predict(sm.add_constant(X_test_poly))\npoly_mse = mean_squared_error(y_test, y_poly_pred)\npoly_mae = mean_absolute_error(y_test, y_poly_pred)\npoly_r2 = r2_score(y_test, y_poly_pred)\nprint(f\"\\nPolynomial Regression Evaluation Metrics:\")\nprint(f\"MSE: {poly_mse:.4f}\")\nprint(f\"MAE: {poly_mae:.4f}\")\nprint(f\"RÂ²: {poly_r2:.4f}\")\n\n\nPolynomial Regression Summary:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Rings   R-squared:                       0.595\nModel:                            OLS   Adj. R-squared:                  0.589\nMethod:                 Least Squares   F-statistic:                     94.80\nDate:                Fri, 07 Mar 2025   Prob (F-statistic):               0.00\nTime:                        14:00:15   Log-Likelihood:                -6570.2\nNo. Observations:                3341   AIC:                         1.324e+04\nDf Residuals:                    3289   BIC:                         1.356e+04\nDf Model:                          51                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.2016      0.017    130.561      0.000       2.169       2.235\nx1            -0.8063      0.227     -3.547      0.000      -1.252      -0.361\nx2             0.3630      0.221      1.645      0.100      -0.070       0.796\nx3             0.3648      0.096      3.796      0.000       0.176       0.553\nx4             5.2842      0.514     10.290      0.000       4.277       6.291\nx5            -4.6651      0.258    -18.089      0.000      -5.171      -4.159\nx6            -0.5059      0.206     -2.450      0.014      -0.911      -0.101\nx7             1.3842      0.220      6.295      0.000       0.953       1.815\nx8            -0.1508      0.013    -11.292      0.000      -0.177      -0.125\nx9            -0.0544      0.018     -3.095      0.002      -0.089      -0.020\nx10            0.1980      0.014     14.041      0.000       0.170       0.226\nx11           -0.1722      0.707     -0.244      0.808      -1.558       1.213\nx12           -0.2377      1.095     -0.217      0.828      -2.385       1.910\nx13           -0.1767      0.558     -0.317      0.752      -1.271       0.918\nx14            4.7773      2.267      2.108      0.035       0.333       9.221\nx15           -0.8998      1.150     -0.783      0.434      -3.154       1.354\nx16           -2.9639      0.892     -3.323      0.001      -4.713      -1.215\nx17           -0.8647      1.099     -0.787      0.431      -3.019       1.290\nx18            0.1726      0.146      1.180      0.238      -0.114       0.459\nx19           -0.4246      0.173     -2.450      0.014      -0.764      -0.085\nx20            0.2458      0.140      1.751      0.080      -0.029       0.521\nx21           -0.8939      0.569     -1.572      0.116      -2.009       0.221\nx22           -0.2281      0.540     -0.422      0.673      -1.287       0.831\nx23           -1.9052      2.110     -0.903      0.367      -6.043       2.233\nx24            1.0809      1.097      0.985      0.325      -1.071       3.233\nx25            1.7421      0.864      2.017      0.044       0.049       3.436\nx26            1.2800      1.016      1.260      0.208      -0.711       3.271\nx27            0.0947      0.148      0.639      0.523      -0.196       0.386\nx28            0.0083      0.175      0.047      0.962      -0.334       0.350\nx29           -0.0992      0.137     -0.723      0.469      -0.368       0.170\nx30            0.4671      0.121      3.871      0.000       0.230       0.704\nx31            1.5358      0.804      1.911      0.056      -0.040       3.111\nx32           -1.0549      0.429     -2.460      0.014      -1.896      -0.214\nx33           -0.4355      0.326     -1.336      0.182      -1.075       0.204\nx34           -0.5845      0.362     -1.617      0.106      -1.293       0.124\nx35           -0.0777      0.060     -1.295      0.196      -0.195       0.040\nx36            0.1129      0.074      1.523      0.128      -0.032       0.258\nx37           -0.0347      0.056     -0.620      0.535      -0.145       0.075\nx38           -1.7600      0.997     -1.765      0.078      -3.715       0.195\nx39           -2.5752      1.202     -2.143      0.032      -4.931      -0.219\nx40            1.3901      0.885      1.571      0.116      -0.345       3.125\nx41           -0.2140      0.962     -0.223      0.824      -2.100       1.672\nx42           -0.1731      0.268     -0.645      0.519      -0.699       0.353\nx43            0.6471      0.392      1.649      0.099      -0.122       1.416\nx44           -0.4611      0.257     -1.797      0.072      -0.964       0.042\nx45            2.0029      0.467      4.288      0.000       1.087       2.919\nx46            0.1708      0.502      0.340      0.734      -0.814       1.155\nx47            0.8347      0.619      1.349      0.178      -0.379       2.048\nx48           -0.1964      0.137     -1.438      0.150      -0.464       0.071\nx49            0.1796      0.197      0.911      0.363      -0.207       0.566\nx50            0.0149      0.132      0.113      0.910      -0.243       0.273\nx51           -0.0739      0.272     -0.272      0.786      -0.608       0.460\nx52           -0.2264      0.505     -0.448      0.654      -1.217       0.764\nx53            0.1532      0.104      1.474      0.141      -0.051       0.357\nx54           -0.1080      0.156     -0.693      0.488      -0.414       0.198\nx55           -0.0427      0.102     -0.418      0.676      -0.243       0.158\nx56           -0.2462      0.293     -0.839      0.401      -0.821       0.329\nx57           -0.1714      0.128     -1.339      0.181      -0.422       0.080\nx58           -0.0696      0.175     -0.397      0.692      -0.414       0.274\nx59            0.2325      0.117      1.982      0.048       0.002       0.463\nx60            2.0798      0.018    117.067      0.000       2.045       2.115\nx61           -0.8824      0.013    -70.215      0.000      -0.907      -0.858\nx62           -1.1463      0.014    -83.750      0.000      -1.173      -1.119\nx63            2.1601      0.023     92.571      0.000       2.114       2.206\nx64           -1.2459      0.015    -84.626      0.000      -1.275      -1.217\nx65            2.3122      0.019    124.315      0.000       2.276       2.349\n==============================================================================\nOmnibus:                      230.353   Durbin-Watson:                   1.964\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              331.696\nSkew:                           0.578   Prob(JB):                     9.40e-73\nKurtosis:                       4.024   Cond. No.                     2.87e+16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 2.43e-28. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\nPolynomial Regression Evaluation Metrics:\nMSE: 3.1953\nMAE: 1.3357\nRÂ²: 0.5734\n\n\n\n# Compute training predictions\ny_poly_train_pred = poly_model.predict(sm.add_constant(X_train_poly))\n\n# Training vs Test R2\ntrain_r2_poly = r2_score(y_train, y_poly_train_pred)\ntest_r2_poly = r2_score(y_test, y_poly_pred)\n\n# Training vs Test MSE\ntrain_mse_poly = mean_squared_error(y_train, y_poly_train_pred)\ntest_mse_poly = mean_squared_error(y_test, y_poly_pred)\n\nprint(f\"\\nPolynomial Regression Overfitting Check:\")\nprint(f\"Training R2: {train_r2_poly:.4f}\")\nprint(f\"Test R2: {test_r2_poly:.4f}\")\nprint(f\"Training MSE: {train_mse_poly:.4f}\")\nprint(f\"Test MSE: {test_mse_poly:.4f}\")\n\n\nPolynomial Regression Overfitting Check:\nTraining R2: 0.5952\nTest R2: 0.5734\nTraining MSE: 2.9898\nTest MSE: 3.1953\n\n\n\n# Scatter plot for Polynomial Regression\nplt.figure(figsize=(6, 6))\nplt.scatter(y_test, y_poly_pred, color='blue', alpha=0.5)\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2)\nplt.xlabel(\"Actual Rings\")\nplt.ylabel(\"Predicted Rings\")\nplt.title(\"Actual vs. Predicted Rings (Polynomial Model)\")\n\nplt.text(min(y_test), max(y_poly_pred) - 2, \n         f\"MSE: {poly_mse:.4f}\\nMAE: {poly_mae:.4f}\\nRÂ²: {poly_r2:.4f}\", \n         fontsize=10, verticalalignment='top')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\nimport statsmodels.api as sm\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\ndegrees = [1, 2, 3, 4, 5]\n\npoly_results = {}\n\nfor degree in degrees:\n    print(f\"\\n Polynomial Degree: {degree}\")\n\n    poly = PolynomialFeatures(degree=degree, include_bias=False)\n    X_train_poly = poly.fit_transform(X_train_scaled)\n    X_test_poly = poly.transform(X_test_scaled)\n\n    poly_model = sm.OLS(y_train, sm.add_constant(X_train_poly)).fit()\n\n    y_poly_pred = poly_model.predict(sm.add_constant(X_test_poly))\n\n    poly_mse = mean_squared_error(y_test, y_poly_pred)\n    poly_mae = mean_absolute_error(y_test, y_poly_pred)\n    poly_r2 = r2_score(y_test, y_poly_pred)\n\n    poly_results[degree] = {\n        \"MSE\": poly_mse,\n        \"MAE\": poly_mae,\n        \"R2\": poly_r2\n    }\n\n    print(f\" Degree {degree} - MSE: {poly_mse:.4f}, MAE: {poly_mae:.4f}, RÂ²: {poly_r2:.4f}\")\n\n\n Polynomial Degree: 1\n Degree 1 - MSE: 3.3736, MAE: 1.4127, RÂ²: 0.5496\n\n Polynomial Degree: 2\n Degree 2 - MSE: 3.1953, MAE: 1.3357, RÂ²: 0.5734\n\n Polynomial Degree: 3\n Degree 3 - MSE: 6.8021, MAE: 1.4638, RÂ²: 0.0919\n\n Polynomial Degree: 4\n Degree 4 - MSE: 175.1503, MAE: 2.5135, RÂ²: -22.3822\n\n Polynomial Degree: 5\n Degree 5 - MSE: 3170.7095, MAE: 8.2069, RÂ²: -422.2837\n\n\n\n# Plot MSE vs. Polynomial Degree\ndegrees = list(poly_results.keys())\nmse_values = [metrics[\"MSE\"] for metrics in poly_results.values()]\nplt.figure(figsize=(6, 4))\nplt.plot(degrees, mse_values, marker='o', linestyle='-', color='orange')\nplt.xlabel(\"Polynomial Degree\")\nplt.ylabel(\"Mean Squared Error (MSE)\")\nplt.title(\"MSE vs. Polynomial Degree\")\nplt.yscale(\"log\")\nplt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\nplt.show()"
  },
  {
    "objectID": "documents/project_slides/group5_Statistical ML Project 1.html",
    "href": "documents/project_slides/group5_Statistical ML Project 1.html",
    "title": "MSSC 6250 - Spring 2025",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import zscore\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n\ndata = pd.read_csv(\"C:\\\\Users\\\\anish\\\\Downloads\\\\train.csv\")\n\n\ndata.isna().sum().tolist()\n\n[0,\n 0,\n 0,\n 259,\n 0,\n 0,\n 1369,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 872,\n 8,\n 0,\n 0,\n 0,\n 37,\n 37,\n 38,\n 37,\n 0,\n 38,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 690,\n 81,\n 81,\n 81,\n 0,\n 0,\n 81,\n 81,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1453,\n 1179,\n 1406,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0]\n\n\n\ndata = pd.read_csv(\"C:\\\\Users\\\\anish\\\\Downloads\\\\train.csv\")\n\n# Drop columns with missing values\ncolumns_to_drop = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'MasVnrType', 'MasVnrArea', 'FireplaceQu']\ndata = data.drop(columns=columns_to_drop)\n\n# Replace missing values for garage-related columns\ngarage_cols = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\ndata[garage_cols] = data[garage_cols].fillna('NoGarage')\n\n# Replace missing values for basement-related columns\nbasement_cols = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\ndata[basement_cols] = data[basement_cols].fillna('NoBasement')\n\n# Calculate the mean of the 'LotFrontage' column (excluding missing values)\nmean_lotfrontage = data['LotFrontage'].mean()\n\n# Replace missing values with the mean\ndata['LotFrontage'].fillna(mean_lotfrontage, inplace=True)\n\n# Calculate the mode of the 'LotFrontage' column (excluding missing values)\nm_Electrical = data['Electrical'].mode()\n\n# Replace missing values with the mode\ndata['Electrical'].fillna(m_Electrical, inplace=True)\n\n\nimport numpy as np\nfrom scipy.stats import zscore\n\n# Select numerical columns, excluding 'GarageYrBlt' for the Z-score calculation\nnumerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n\n# Remove 'GarageYrBlt' from numerical columns for outlier detection\nnumerical_cols.remove('GarageYrBlt')\n\n# Calculate Z-scores for the remaining numerical columns\nz_scores = np.abs(zscore(data[numerical_cols]))\n\n# Set a threshold for Z-scores \nthreshold = 3\n\n# Keep only rows --&gt; where the Z-score is below the threshold for numerical columns\ndata = data[(z_scores &lt; threshold).all(axis=1)]\n\n# Print the number of rows before and after outlier removal\nprint(f\"After outlier removal: {data.shape[0]}\")\n\nAfter outlier removal: 1028\n\n\n\nnumerical_cols\n\n['Id',\n 'MSSubClass',\n 'LotFrontage',\n 'LotArea',\n 'OverallQual',\n 'OverallCond',\n 'YearBuilt',\n 'YearRemodAdd',\n 'BsmtFinSF1',\n 'BsmtFinSF2',\n 'BsmtUnfSF',\n 'TotalBsmtSF',\n '1stFlrSF',\n '2ndFlrSF',\n 'LowQualFinSF',\n 'GrLivArea',\n 'BsmtFullBath',\n 'BsmtHalfBath',\n 'FullBath',\n 'HalfBath',\n 'BedroomAbvGr',\n 'KitchenAbvGr',\n 'TotRmsAbvGrd',\n 'Fireplaces',\n 'GarageCars',\n 'GarageArea',\n 'WoodDeckSF',\n 'OpenPorchSF',\n 'EnclosedPorch',\n '3SsnPorch',\n 'ScreenPorch',\n 'PoolArea',\n 'MiscVal',\n 'MoSold',\n 'YrSold',\n 'SalePrice']\n\n\n\n# Creating New Features\ndata['TotalArea'] = data['GrLivArea'] + data['TotalBsmtSF']\ndata['HouseAge'] = data['YrSold'] - data['YearBuilt']\ndata['Remodel'] = (data['YearBuilt'] != data['YearRemodAdd']).astype(int)\ndata['TotalBathrooms'] = data['FullBath'] + (0.5 * data['HalfBath']) + data['BsmtFullBath'] + (0.5 * data['BsmtHalfBath'])\n\n\ndata['BsmtHalfBath'].dtype\ndata['KitchenAbvGr'].dtype\ndata['PoolArea'].dtype\n\ndtype('int64')\n\n\n\n# Select numerical columns\nnum = data.select_dtypes(include=['int64', 'float64'])  # Select only numerical columns\nnum = num.drop(columns=['BsmtHalfBath', 'KitchenAbvGr', 'PoolArea'])  # Drop unwanted columns\nprint(num)\n\n        Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  \\\n0        1          60         65.0     8450            7            5   \n2        3          60         68.0    11250            7            5   \n4        5          60         84.0    14260            8            5   \n6        7          20         75.0    10084            8            5   \n10      11          20         70.0    11200            5            5   \n...    ...         ...          ...      ...          ...          ...   \n1452  1453         180         35.0     3675            5            5   \n1453  1454          20         90.0    17217            5            5   \n1454  1455          20         62.0     7500            7            5   \n1455  1456          60         62.0     7917            6            5   \n1456  1457          20         85.0    13175            6            6   \n\n      YearBuilt  YearRemodAdd  BsmtFinSF1  BsmtFinSF2  ...  EnclosedPorch  \\\n0          2003          2003         706           0  ...              0   \n2          2001          2002         486           0  ...              0   \n4          2000          2000         655           0  ...              0   \n6          2004          2005        1369           0  ...              0   \n10         1965          1965         906           0  ...              0   \n...         ...           ...         ...         ...  ...            ...   \n1452       2005          2005         547           0  ...              0   \n1453       2006          2006           0           0  ...              0   \n1454       2004          2005         410           0  ...              0   \n1455       1999          2000           0           0  ...              0   \n1456       1978          1988         790         163  ...              0   \n\n      3SsnPorch  ScreenPorch  MiscVal  MoSold  YrSold  SalePrice  TotalArea  \\\n0             0            0        0       2    2008     208500       2566   \n2             0            0        0       9    2008     223500       2706   \n4             0            0        0      12    2008     250000       3343   \n6             0            0        0       8    2007     307000       3380   \n10            0            0        0       2    2008     129500       2080   \n...         ...          ...      ...     ...     ...        ...        ...   \n1452          0            0        0       5    2006     145000       1619   \n1453          0            0        0       7    2006      84500       2280   \n1454          0            0        0      10    2009     185000       2442   \n1455          0            0        0       8    2007     175000       2600   \n1456          0            0        0       2    2010     210000       3615   \n\n      HouseAge  TotalBathrooms  \n0            5             3.5  \n2            7             3.5  \n4            8             3.5  \n6            3             3.0  \n10          43             2.0  \n...        ...             ...  \n1452         1             2.0  \n1453         0             1.0  \n1454         5             3.0  \n1455         8             2.5  \n1456        32             3.0  \n\n[1028 rows x 37 columns]\n\n\n\n# Compute correlation matrix\ncorr = num.corr()\n\n# Plot the correlation matrix\nplt.figure(figsize=(20, 15))\nsns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, linecolor=\"black\")\nplt.title(\"Correlation Matrix\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n# StandardScaler\nscaler = StandardScaler()\n\n# Select numeric columns for scaling \nnumeric_cols = data.select_dtypes(include=['number']).columns\n\n# Apply StandardScaler\ndata[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n\n# Verify scaling\nprint(data.head())\n\n          Id  MSSubClass MSZoning  LotFrontage   LotArea Street LotShape  \\\n0  -1.735247    0.144020       RL    -0.142721 -0.194159   Pave      Reg   \n2  -1.730520    0.144020       RL     0.020674  0.576120   Pave      IR1   \n4  -1.725793    0.144020       RL     0.892110  1.404170   Pave      IR1   \n6  -1.721066   -0.846299       RL     0.401927  0.255354   Pave      Reg   \n10 -1.711611   -0.846299       RL     0.129603  0.562365   Pave      Reg   \n\n   LandContour Utilities LotConfig  ...   MiscVal    MoSold    YrSold  \\\n0          Lvl    AllPub    Inside  ... -0.144857 -1.592606  0.138594   \n2          Lvl    AllPub    Inside  ... -0.144857  1.026314  0.138594   \n4          Lvl    AllPub       FR2  ... -0.144857  2.148708  0.138594   \n6          Lvl    AllPub    Inside  ... -0.144857  0.652182 -0.607345   \n10         Lvl    AllPub    Inside  ... -0.144857 -1.592606  0.138594   \n\n   SaleType SaleCondition SalePrice  TotalArea  HouseAge   Remodel  \\\n0        WD        Normal  0.534157   0.180386 -0.945796 -0.885845   \n2        WD        Normal  0.766241   0.392149 -0.876887  1.128865   \n4        WD        Normal  1.176255   1.355673 -0.842432 -0.885845   \n6        WD        Normal  2.058171   1.411639 -1.014706  1.128865   \n10       WD        Normal -0.688148  -0.554736  0.363484 -0.885845   \n\n    TotalBathrooms  \n0         1.763476  \n2         1.763476  \n4         1.763476  \n6         1.109016  \n10       -0.199903  \n\n[5 rows x 78 columns]\n\n\n\n# List of nominal variables to one-hot encode\nnominal_vars = [\n    'MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', \n    'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', \n    'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterQual', 'ExterCond', \n    'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', \n    'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', \n    'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType', \n    'SaleCondition'\n]\n\n# List of ordinal variables to numeric encode\nordinal_vars = [\n    'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',\n    'HeatingQC', 'KitchenQual', 'Functional', 'GarageQual', 'GarageCond'\n]\n\n# Replace missing values for garage-related columns\ngarage_cols = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\ndata[garage_cols] = data[garage_cols].fillna('NoGarage')\n\n# Replace missing values for basement-related columns\nbasement_cols = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\ndata[basement_cols] = data[basement_cols].fillna('NoBasement')\n\n# Ordinal Mapping Dictionary\nordinal_mapping = {\n    'OverallQual': {1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10},\n    'OverallCond': {1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10},\n    'ExterQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NoBasement': 0},\n    'ExterCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1},\n    'BsmtQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NoBasement': 0},\n    'BsmtCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NoBasement': 0},\n    'HeatingQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1},\n    'KitchenQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1},\n    'Functional': {'Typ': 5, 'Min1': 4, 'Min2': 3, 'Mod': 2, 'Maj1': 1, 'Maj2': 0, 'Sev': -1, 'Sal': -2},\n    'GarageQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NoGarage': 0},\n    'GarageCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NoGarage': 0}\n}\n\n# Apply the ordinal mappings to the original columns before encoding\nfor var in ordinal_vars:\n    if var in data.columns:\n        data[var] = data[var].map(ordinal_mapping.get(var, {}))\n\n# Apply one-hot encoding for nominal variables\ndata = pd.get_dummies(data, columns=nominal_vars, drop_first=True)\n\n\n# Number of nominal variables\nnum_nominal = len(nominal_vars)\n\n# Number of ordinal variables\nnum_ordinal = len(ordinal_vars)\n\n# Print the count\nprint(f\"Number of nominal variables: {num_nominal}\")\nprint(f\"Number of ordinal variables: {num_ordinal}\")\n\nNumber of nominal variables: 37\nNumber of ordinal variables: 9\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1028 entries, 0 to 1456\nColumns: 220 entries, Id to SaleCondition_Partial\ndtypes: bool(179), float64(41)\nmemory usage: 517.0 KB\n\n\n\nmissing_values = data.isna().sum()\nmissing_columns = missing_values[missing_values &gt; 0]  # Select only columns with missing values\n\nprint(missing_columns)\n\nGarageYrBlt    46\ndtype: int64\n\n\n\ndata = data.drop(columns=['GarageYrBlt'])\n\n\n# Target and features\nX = data.drop(columns=[\"SalePrice\"])  # Take all features except SalePrice\ny = data[\"SalePrice\"]\n\n# Handle categorical variables by one-hot encoding (if necessary)\nX = pd.get_dummies(X, drop_first=True)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Standardize features for better performance\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\n# Ridge Regression\nridge = Ridge(alpha=100, max_iter=1000, random_state= 43)  # Adjust alpha for regularization strength\nridge.fit(X_train_scaled, y_train)\nridge_preds = ridge.predict(X_test_scaled)\nridge_mse = mean_squared_error(y_test, ridge_preds)\nprint(f\"Ridge Regression MSE: {ridge_mse:.4f}\")\n\n# Lasso Regression\nlasso = Lasso(alpha=0.1, max_iter=1000, random_state= 43)  # Adjust alpha for feature selection strength\nlasso.fit(X_train_scaled, y_train)\nlasso_preds = lasso.predict(X_test_scaled)\nlasso_mse = mean_squared_error(y_test, lasso_preds)\nprint(f\"Lasso Regression MSE: {lasso_mse:.4f}\")\n\nRidge Regression MSE: 0.0879\nLasso Regression MSE: 0.1505\n\n\n\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge, Lasso\nimport warnings\nfrom tabulate import tabulate\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# Target and features\nX = data.drop(columns=[\"SalePrice\"])  # Take all features except SalePrice\ny = data[\"SalePrice\"]\n\n# Handle categorical variables by one-hot encoding \nX = pd.get_dummies(X, drop_first=True)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize features for better performance \nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Ridge Regression\nridge = Ridge(alpha=100, max_iter=1000, random_state= 43)\nridge.fit(X_train_scaled, y_train)\nridge_preds = ridge.predict(X_test_scaled)\nridge_mse = mean_squared_error(y_test, ridge_preds)\nprint(f\"Ridge Regression MSE: {ridge_mse:.4f}\")\n\n# Lasso Regression\nlasso = Lasso(alpha=0.1, max_iter=1000, random_state= 43)\nlasso.fit(X_train_scaled, y_train)\nlasso_preds = lasso.predict(X_test_scaled)\nlasso_mse = mean_squared_error(y_test, lasso_preds)\nprint(f\"Lasso Regression MSE: {lasso_mse:.4f}\")\n\n# RÂ² for both models\nridge_r2 = r2_score(y_test, ridge_preds)\nlasso_r2 = r2_score(y_test, lasso_preds)\nprint(f\"Ridge RÂ²: {ridge_r2:.4f}\")\nprint(f\"Lasso RÂ²: {lasso_r2:.4f}\")\n\nRidge Regression MSE: 0.0879\nLasso Regression MSE: 0.1505\nRidge RÂ²: 0.9154\nLasso RÂ²: 0.8552\n\n\n\n# Function to calculate regression metrics\ndef regression_metrics(y_true, y_pred, X):\n    mse = mean_squared_error(y_true, y_pred)\n    # Root Mean Squared Error\n    rmse = np.sqrt(mse)  \n    # Mean Absolute Error\n    mae = mean_absolute_error(y_true, y_pred)  \n    # RÂ² Score\n    r2 = r2_score(y_true, y_pred)  \n    explained_var = explained_variance_score(y_true, y_pred)  \n    n, p = X.shape  # Number of samples (n) and predictors (p)\n\n    return {\"RMSE\": rmse, \"MAE\": mae}\n\n# Ridge Regression Metrics\nridge_metrics = regression_metrics(y_test, ridge_preds, X_test)\n\n# Lasso Regression Metrics\nlasso_metrics = regression_metrics(y_test, lasso_preds, X_test)\n\n# Create a DataFrame to display the results \nmetrics_df = pd.DataFrame([ridge_metrics, lasso_metrics], index=[\"Ridge\", \"Lasso\"])\n\n# Add MSE and RÂ² values to DataFrame\nmetrics_df[\"MSE\"] = [ridge_mse, lasso_mse]\nmetrics_df[\"RÂ²\"] = [ridge_r2, lasso_r2]\n\n# Reorder columns to the specified order\nmetrics_df = metrics_df[[\"MSE\", \"RMSE\", \"MAE\", \"RÂ²\"]]\n\n# Round values to 4 decimal places\nmetrics_df = metrics_df.applymap(lambda x: f\"{x:.4f}\")\n\n# Print the table with lines using tabulate\nprint(metrics_df)\n\n          MSE    RMSE     MAE      RÂ²\nRidge  0.0879  0.2964  0.2024  0.9154\nLasso  0.1505  0.3879  0.2759  0.8552\n\n\n\n# Display important features from Lasso\nlasso_coef = pd.Series(lasso.coef_, index=X.columns)\nprint(\"Lasso selected features: \\n\",lasso_coef[lasso_coef != 0])\n\nLasso selected features: \n LotArea             0.006018\nOverallQual         0.219580\nYearRemodAdd        0.020271\nGrLivArea           0.085451\nGarageCars          0.037253\nGarageArea          0.078726\nTotalArea           0.314569\nHouseAge           -0.026418\nTotalBathrooms      0.090354\nExterQual_3        -0.031297\nBsmtQual_5          0.068490\nBsmtFinType1_GLQ    0.010984\nKitchenQual_3      -0.003296\nKitchenQual_5       0.023717\ndtype: float64\n\n\n\nfrom sklearn.model_selection import GridSearchCV\nridge_cv = GridSearchCV(Lasso(), {'alpha': [0.01, 0.1, 1, 10, 100]}, cv=5)\nridge_cv.fit(X_train_scaled, y_train)\nprint(f\"Best Ridge Lasso: {ridge_cv.best_params_['alpha']}\")\n\nBest Ridge Lasso: 0.01\n\n\n\nfrom sklearn.model_selection import GridSearchCV\nridge_cv = GridSearchCV(Ridge(), {'alpha': [0.01, 0.1, 1, 10, 100]}, cv=5)\nridge_cv.fit(X_train_scaled, y_train)\nprint(f\"Best Ridge alpha: {ridge_cv.best_params_['alpha']}\")\n\nBest Ridge alpha: 100\n\n\n\ndata.shape\n\n(1028, 219)\n\n\n\n#OLS Regression\nimport statsmodels.api as sm\n\n# Add a constant term for the intercept\nX_train_ols = sm.add_constant(X_train_scaled)\nX_test_ols = sm.add_constant(X_test_scaled)\n\n# Fit OLS model\nols_model = sm.OLS(y_train, X_train_ols).fit()\n\n# Print summary\nprint(ols_model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              SalePrice   R-squared:                       0.954\nModel:                            OLS   Adj. R-squared:                  0.940\nMethod:                 Least Squares   F-statistic:                     66.42\nDate:                Thu, 06 Mar 2025   Prob (F-statistic):               0.00\nTime:                        15:19:50   Log-Likelihood:                 105.99\nNo. Observations:                 822   AIC:                             182.0\nDf Residuals:                     625   BIC:                             1110.\nDf Model:                         196                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0226      0.009     -2.662      0.008      -0.039      -0.006\nx1             0.0007      0.010      0.069      0.945      -0.018       0.020\nx2            -0.0853      0.086     -0.993      0.321      -0.254       0.083\nx3            -0.0177      0.015     -1.184      0.237      -0.047       0.012\nx4             0.0660      0.016      4.013      0.000       0.034       0.098\nx5             0.0975      0.021      4.683      0.000       0.057       0.138\nx6             0.0903      0.015      6.157      0.000       0.062       0.119\nx7             0.0721      0.020      3.633      0.000       0.033       0.111\nx8             0.0192      0.020      0.944      0.346      -0.021       0.059\nx9             0.0710      0.014      4.976      0.000       0.043       0.099\nx10            0.0125      0.020      0.625      0.532      -0.027       0.052\nx11           -0.0230      0.013     -1.790      0.074      -0.048       0.002\nx12            0.0547      0.021      2.660      0.008       0.014       0.095\nx13            0.0220      0.025      0.867      0.386      -0.028       0.072\nx14            0.1899      0.023      8.165      0.000       0.144       0.236\nx15           -0.0025      0.010     -0.263      0.793      -0.021       0.016\nx16            0.1932      0.018     10.859      0.000       0.158       0.228\nx17            0.0068      0.012      0.561      0.575      -0.017       0.031\nx18        -1.486e-15    6.6e-16     -2.251      0.025   -2.78e-15   -1.89e-16\nx19           -0.0012      0.014     -0.085      0.932      -0.029       0.026\nx20           -0.0105      0.016     -0.654      0.513      -0.042       0.021\nx21           -0.0382      0.017     -2.202      0.028      -0.072      -0.004\nx22        -2.905e-15   1.44e-15     -2.022      0.044   -5.73e-15   -8.35e-17\nx23           -0.0040      0.022     -0.184      0.854      -0.047       0.039\nx24            0.0231      0.013      1.848      0.065      -0.001       0.048\nx25            0.0597      0.025      2.421      0.016       0.011       0.108\nx26            0.0605      0.024      2.532      0.012       0.014       0.107\nx27            0.0230      0.011      2.116      0.035       0.002       0.044\nx28            0.0184      0.011      1.649      0.100      -0.004       0.040\nx29            0.0134      0.012      1.134      0.257      -0.010       0.037\nx30           -0.0058      0.009     -0.619      0.536      -0.024       0.013\nx31            0.0182      0.010      1.802      0.072      -0.002       0.038\nx32         9.506e-16   6.79e-16      1.399      0.162   -3.83e-16    2.28e-15\nx33            0.0178      0.010      1.755      0.080      -0.002       0.038\nx34            0.0057      0.010      0.570      0.569      -0.014       0.025\nx35           -0.0050      0.010     -0.476      0.634      -0.026       0.016\nx36            0.1594      0.011     14.562      0.000       0.138       0.181\nx37           -0.0721      0.020     -3.655      0.000      -0.111      -0.033\nx38            0.0118      0.013      0.917      0.359      -0.013       0.037\nx39            0.0002      0.010      0.024      0.981      -0.019       0.020\nx40            0.0912      0.047      1.938      0.053      -0.001       0.184\nx41            0.0611      0.018      3.381      0.001       0.026       0.097\nx42            0.1646      0.077      2.147      0.032       0.014       0.315\nx43            0.1325      0.063      2.120      0.034       0.010       0.255\nx44           -0.0212      0.015     -1.404      0.161      -0.051       0.008\nx45         4.089e-05      0.010      0.004      0.997      -0.020       0.020\nx46           -0.0068      0.013     -0.540      0.590      -0.032       0.018\nx47            0.0057      0.012      0.492      0.623      -0.017       0.029\nx48            0.0117      0.016      0.737      0.462      -0.020       0.043\nx49           -0.0315      0.013     -2.380      0.018      -0.057      -0.006\nx50           -0.0149      0.018     -0.814      0.416      -0.051       0.021\nx51            0.0056      0.011      0.488      0.625      -0.017       0.028\nx52           -0.0270      0.011     -2.569      0.010      -0.048      -0.006\nx53           -0.0137      0.011     -1.285      0.199      -0.035       0.007\nx54           -0.0189      0.012     -1.569      0.117      -0.042       0.005\nx55            0.0089      0.013      0.692      0.489      -0.016       0.034\nx56            0.0017      0.009      0.177      0.859      -0.017       0.020\nx57           -0.0108      0.010     -1.121      0.263      -0.030       0.008\nx58            0.0058      0.016      0.367      0.714      -0.025       0.037\nx59           -0.0336      0.026     -1.273      0.203      -0.085       0.018\nx60           -0.0311      0.014     -2.159      0.031      -0.059      -0.003\nx61           -0.0972      0.031     -3.149      0.002      -0.158      -0.037\nx62            0.0217      0.021      1.024      0.306      -0.020       0.063\nx63           -0.1061      0.028     -3.731      0.000      -0.162      -0.050\nx64           -0.0688      0.025     -2.728      0.007      -0.118      -0.019\nx65           -0.0468      0.024     -1.915      0.056      -0.095       0.001\nx66           -0.0288      0.020     -1.439      0.151      -0.068       0.011\nx67           -0.0747      0.020     -3.781      0.000      -0.113      -0.036\nx68           -0.1283      0.037     -3.427      0.001      -0.202      -0.055\nx69           -0.0015      0.016     -0.096      0.924      -0.032       0.029\nx70           -0.0791      0.022     -3.551      0.000      -0.123      -0.035\nx71            0.0153      0.019      0.808      0.419      -0.022       0.053\nx72            0.0193      0.022      0.880      0.379      -0.024       0.062\nx73           -0.0856      0.034     -2.543      0.011      -0.152      -0.019\nx74           -0.0344      0.016     -2.170      0.030      -0.066      -0.003\nx75           -0.0513      0.024     -2.130      0.034      -0.099      -0.004\nx76           -0.0375      0.019     -1.927      0.054      -0.076       0.001\nx77           -0.0040      0.030     -0.132      0.895      -0.064       0.056\nx78            0.0475      0.014      3.378      0.001       0.020       0.075\nx79           -0.0513      0.018     -2.825      0.005      -0.087      -0.016\nx80           -0.0219      0.012     -1.803      0.072      -0.046       0.002\nx81            0.0150      0.017      0.860      0.390      -0.019       0.049\nx82            0.0677      0.022      3.052      0.002       0.024       0.111\nx83           -0.0031      0.011     -0.293      0.770      -0.024       0.018\nx84            0.0294      0.013      2.312      0.021       0.004       0.054\nx85           -0.0075      0.012     -0.633      0.527      -0.031       0.016\nx86            0.0117      0.015      0.779      0.436      -0.018       0.041\nx87        -5.337e-16   1.89e-16     -2.818      0.005   -9.06e-16   -1.62e-16\nx88            0.0143      0.014      0.998      0.319      -0.014       0.042\nx89            0.0132      0.017      0.754      0.451      -0.021       0.047\nx90            0.0083      0.014      0.611      0.542      -0.018       0.035\nx91           -0.0125      0.013     -0.934      0.351      -0.039       0.014\nx92           -0.0174      0.011     -1.534      0.126      -0.040       0.005\nx93           -0.0292      0.042     -0.702      0.483      -0.111       0.052\nx94           -0.0226      0.063     -0.362      0.718      -0.145       0.100\nx95            0.0165      0.013      1.298      0.195      -0.008       0.041\nx96            0.0754      0.041      1.849      0.065      -0.005       0.156\nx97           -0.0092      0.010     -0.935      0.350      -0.028       0.010\nx98            0.0090      0.030      0.302      0.763      -0.050       0.068\nx99            0.0257      0.019      1.350      0.178      -0.012       0.063\nx100           0.0398      0.024      1.682      0.093      -0.007       0.086\nx101           0.0006      0.005      0.107      0.915      -0.010       0.011\nx102          -0.0094      0.011     -0.825      0.410      -0.032       0.013\nx103           0.0008      0.006      0.135      0.893      -0.011       0.012\nx104           0.0112      0.012      0.900      0.369      -0.013       0.036\nx105          -0.0083      0.012     -0.716      0.474      -0.031       0.014\nx106          -0.0139      0.013     -1.084      0.279      -0.039       0.011\nx107        -3.68e-16   1.37e-16     -2.678      0.008   -6.38e-16   -9.81e-17\nx108          -0.0012      0.021     -0.055      0.956      -0.043       0.040\nx109          -0.0099      0.038     -0.261      0.794      -0.085       0.065\nx110          -0.0020      0.007     -0.296      0.767      -0.015       0.011\nx111          -0.1526      0.069     -2.204      0.028      -0.288      -0.017\nx112          -0.1478      0.094     -1.576      0.115      -0.332       0.036\nx113          -0.0161      0.014     -1.181      0.238      -0.043       0.011\nx114          -0.1525      0.098     -1.559      0.120      -0.345       0.040\nx115          -0.0895      0.060     -1.481      0.139      -0.208       0.029\nx116          -0.0363      0.033     -1.102      0.271      -0.101       0.028\nx117          -0.2611      0.138     -1.886      0.060      -0.533       0.011\nx118          -0.1737      0.085     -2.032      0.043      -0.342      -0.006\nx119          -0.0419      0.040     -1.037      0.300      -0.121       0.037\nx120           0.0205      0.025      0.808      0.419      -0.029       0.070\nx121           0.0150      0.028      0.542      0.588      -0.039       0.069\nx122          -0.0020      0.007     -0.296      0.767      -0.015       0.011\nx123           0.1372      0.067      2.038      0.042       0.005       0.269\nx124           0.1009      0.087      1.159      0.247      -0.070       0.272\nx125           0.0115      0.023      0.492      0.623      -0.034       0.057\nx126           0.1548      0.094      1.646      0.100      -0.030       0.340\nx127           0.0074      0.013      0.560      0.576      -0.019       0.033\nx128           0.0672      0.066      1.026      0.305      -0.061       0.196\nx129          -0.0002      0.013     -0.017      0.986      -0.025       0.025\nx130           0.0394      0.035      1.123      0.262      -0.030       0.108\nx131           0.2421      0.133      1.822      0.069      -0.019       0.503\nx132           0.1518      0.081      1.868      0.062      -0.008       0.311\nx133           0.0474      0.043      1.097      0.273      -0.037       0.132\nx134          -0.0291      0.117     -0.248      0.804      -0.260       0.201\nx135          -0.0004      0.117     -0.003      0.997      -0.231       0.230\nx136           0.0156      0.032      0.486      0.627      -0.047       0.079\nx137           0.0285      0.031      0.905      0.366      -0.033       0.090\nx138           0.0079      0.031      0.253      0.800      -0.054       0.069\nx139           0.0303      0.026      1.170      0.242      -0.021       0.081\nx140           0.0371      0.028      1.339      0.181      -0.017       0.092\nx141          -0.0119      0.019     -0.641      0.522      -0.048       0.024\nx142           0.0089      0.011      0.789      0.430      -0.013       0.031\nx143          2.5e-17   3.54e-17      0.706      0.480   -4.45e-17    9.46e-17\nx144           0.0101      0.012      0.858      0.391      -0.013       0.033\nx145          -0.0066      0.012     -0.545      0.586      -0.030       0.017\nx146          -0.0269      0.011     -2.548      0.011      -0.048      -0.006\nx147           0.0582      0.013      4.626      0.000       0.034       0.083\nx148          -0.0133      0.010     -1.376      0.169      -0.032       0.006\nx149           0.0098      0.006      1.716      0.087      -0.001       0.021\nx150          -0.0069      0.008     -0.879      0.380      -0.022       0.009\nx151           0.0477      0.012      4.120      0.000       0.025       0.070\nx152          -0.0116      0.012     -0.997      0.319      -0.034       0.011\nx153          -0.0357      0.015     -2.402      0.017      -0.065      -0.007\nx154           0.0054      0.011      0.510      0.610      -0.015       0.026\nx155          -0.0153      0.012     -1.235      0.217      -0.040       0.009\nx156           0.0432      0.017      2.480      0.013       0.009       0.077\nx157          -0.0232      0.012     -1.998      0.046      -0.046      -0.000\nx158           0.0054      0.011      0.510      0.610      -0.015       0.026\nx159          -0.0007      0.013     -0.059      0.953      -0.025       0.024\nx160           0.0097      0.021      0.466      0.641      -0.031       0.051\nx161          -0.0094      0.028     -0.332      0.740      -0.065       0.046\nx162           0.0109      0.013      0.844      0.399      -0.014       0.036\nx163          -0.0020      0.032     -0.061      0.951      -0.066       0.062\nx164           0.0054      0.011      0.510      0.610      -0.015       0.026\nx165          -0.0080      0.030     -0.268      0.789      -0.067       0.051\nx166          -0.0052      0.056     -0.091      0.927      -0.116       0.105\nx167           0.0125      0.038      0.328      0.743      -0.062       0.087\nx168           0.0109      0.027      0.409      0.682      -0.042       0.063\nx169           0.0015      0.029      0.051      0.960      -0.056       0.059\nx170           0.0288      0.016      1.765      0.078      -0.003       0.061\nx171          -0.0323      0.031     -1.035      0.301      -0.093       0.029\nx172          -0.0302      0.027     -1.120      0.263      -0.083       0.023\nx173          -0.0285      0.036     -0.782      0.435      -0.100       0.043\nx174          -0.0013      0.015     -0.086      0.932      -0.031       0.029\nx175           0.0007      0.013      0.058      0.954      -0.024       0.026\nx176          -0.0020      0.013     -0.157      0.875      -0.027       0.023\nx177           0.0001      0.012      0.011      0.991      -0.024       0.024\nx178          -0.0064      0.042     -0.153      0.879      -0.088       0.075\nx179          -0.0127      0.045     -0.283      0.777      -0.101       0.075\nx180           0.0565      0.023      2.500      0.013       0.012       0.101\nx181           0.0273      0.026      1.031      0.303      -0.025       0.079\nx182          -0.0169      0.027     -0.628      0.530      -0.070       0.036\nx183           0.0525      0.046      1.154      0.249      -0.037       0.142\nx184           0.0356      0.043      0.824      0.410      -0.049       0.120\nx185           0.1038      0.069      1.505      0.133      -0.032       0.239\nx186           0.0072      0.011      0.650      0.516      -0.015       0.029\nx187          -0.0024      0.012     -0.195      0.846      -0.027       0.022\nx188           0.0103      0.014      0.760      0.447      -0.016       0.037\nx189          -0.0096      0.015     -0.630      0.529      -0.040       0.020\nx190           0.0095      0.006      1.726      0.085      -0.001       0.020\nx191           0.0095      0.006      1.726      0.085      -0.001       0.020\nx192           0.0172      0.014      1.242      0.215      -0.010       0.044\nx193           0.0173      0.019      0.911      0.362      -0.020       0.055\nx194          -0.0189      0.014     -1.336      0.182      -0.047       0.009\nx195          -0.0215      0.010     -2.233      0.026      -0.040      -0.003\nx196           0.0067      0.007      1.018      0.309      -0.006       0.020\nx197           0.0077      0.009      0.815      0.416      -0.011       0.026\nx198           0.0024      0.005      0.484      0.629      -0.007       0.012\nx199           0.0139      0.018      0.781      0.435      -0.021       0.049\nx200          -0.0103      0.010     -1.023      0.307      -0.030       0.009\nx201          -0.0022      0.007     -0.323      0.747      -0.015       0.011\nx202          -0.0051      0.009     -0.544      0.586      -0.024       0.013\nx203           0.0024      0.005      0.484      0.629      -0.007       0.012\nx204          -0.0178      0.012     -1.438      0.151      -0.042       0.007\nx205          -0.0148      0.015     -1.007      0.314      -0.044       0.014\nx206           0.0123      0.010      1.276      0.203      -0.007       0.031\nx207                0          0        nan        nan           0           0\nx208          -0.0029      0.011     -0.257      0.798      -0.025       0.020\nx209          -0.0177      0.010     -1.792      0.074      -0.037       0.002\nx210        8.218e-05      0.014      0.006      0.995      -0.028       0.028\nx211          -0.0631      0.062     -1.017      0.310      -0.185       0.059\nx212           0.0084      0.010      0.873      0.383      -0.011       0.027\nx213           0.0004      0.022      0.020      0.984      -0.043       0.044\nx214           0.0242      0.012      1.959      0.051   -5.65e-05       0.048\nx215          -0.0402      0.018     -2.210      0.028      -0.076      -0.004\nx216           0.0041      0.011      0.391      0.696      -0.017       0.025\nx217           0.0563      0.016      3.436      0.001       0.024       0.088\nx218           0.1447      0.060      2.410      0.016       0.027       0.263\n==============================================================================\nOmnibus:                      122.167   Durbin-Watson:                   1.976\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1183.919\nSkew:                           0.300   Prob(JB):                    8.23e-258\nKurtosis:                       8.849   Cond. No.                     6.03e+16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 4.24e-30. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular."
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Click here to download the syllabus.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#time-and-location",
    "href": "course-syllabus.html#time-and-location",
    "title": "Syllabus",
    "section": "Time and location",
    "text": "Time and location\n\n\n\n\nDay\nTime\nLocation\n\n\n\n\nLectures\nTu & Th\n3:30 - 4:45 PM\nCuday Hall 120\n\n\nLab\nNone\nNone\nNone",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#office-hours",
    "href": "course-syllabus.html#office-hours",
    "title": "Syllabus",
    "section": "Office Hours",
    "text": "Office Hours\n\nMy in-person office hours are TuTh 4:50 - 5:50 PM, and Wed 12 - 1 PM in Cudahy Hall room 353.\nYou are welcome to schedule an online meeting via Microsoft Teams if you need/prefer.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#prerequisites",
    "href": "course-syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nMATH 4720 (Intro to Statistics), MATH 3100 (Linear Algebra) and MATH 4780 (Regression Analysis)\nHaving taken MATH 4700 (Probability) and MATH 4710 (Statistical Inference) or more advanced ones is strongly recommended.\nThis course is supposed to be taken in the last semester for the applied statistics (APST) master students. Talk to me if you are not sure whether or not this is the right course for you.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#e-mail-policy",
    "href": "course-syllabus.html#e-mail-policy",
    "title": "Syllabus",
    "section": "E-mail Policy",
    "text": "E-mail Policy\n\nI will attempt to reply your email quickly, at least within 24 hours.\nExpect a reply on Monday if you send a question during weekends. If you do not receive a response from me within two days, re-send your question/comment in case there was a âmix-upâ with email communication (Hope this wonât happen!).\nPlease start your subject line with [mssc6250] followed by a clear description of your question. See an example below.\n\n\n\n\nEmail Subject Line Example\n\n\n\nEmail etiquette is important. Please read this article to learn more about email etiquette.\nI am more than happy to answer your questions about this course or data science/statistics in general. However, with tons of email messgaes everyday, I may choose NOT to respond to studentsâ e-mail if\n\nThe student could answer his/her own inquiry by reading the syllabus or information on the course website or D2L.\nThe student is asking for an extra credit opportunity. The answer is ânoâ.\nThe student is requesting an extension on homework. The answer is ânoâ.\nThe student is asking for a grade to be raised for no legitimate reason. The answer is ânoâ.\nThe student is sending an email with no etiquette.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#required-textbook",
    "href": "course-syllabus.html#required-textbook",
    "title": "Syllabus",
    "section": "Required Textbook",
    "text": "Required Textbook\n\n(ISL) An Introduction to Statistical Learning, by James et al.Â Publisher: Springer. (Undergraduate to master level, R and Python code)",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#optional-references",
    "href": "course-syllabus.html#optional-references",
    "title": "Syllabus",
    "section": "Optional References",
    "text": "Optional References\n\n(MML) Mathematics for Machine Learning, by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. Publisher: Cambridge University Press. (College level mathematics for machine learning)\n(PML) Probabilistic Machine Learning: An Introduction, by Kevin Murphy. Publisher: MIT Press. (Master to PhD level, lots of mathematics foundations, Python code)\n(PMLA) Probabilistic Machine Learning: Advanced Topics, by Kevin Murphy. Publisher: MIT Press. (PhD level, more probabilistic-based or Bayesian)\n(ESL) The Elements of Statistical Learning, 2nd edition, by Hastie et. al.Â Publisher: Springer. (PhD level, more frequentist-based)",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#grading-policy",
    "href": "course-syllabus.html#grading-policy",
    "title": "Syllabus",
    "section": "Grading Policy",
    "text": "Grading Policy\n\nYour final grade is earned out of 1000 total points distributed as follows:\n\nHomework: 500 pts\nMidterm project presentation: 300 pts\nFinal project: 200 pts\n\n\nYou will NOT be allowed any extra credit projects/homework/exam to compensate for a poor average. Everyone must be given the same opportunity to do well in this class. Individual exam will NOT be curved. \nThe final grade is based on your percentage of points earned out of 1000 points and the grade-percentage conversion Table. \\([x, y)\\) means greater than or equal to \\(x\\) and less than \\(y\\). For example, 94.1 is in \\([93, 100]\\) and the grade is A and 92.8 is in \\([90, 94)\\) and the grade is A-.\n\n\n\n\nGrade-Percentage Conversion\n\n\nGrade\nPercentage\n\n\n\n\nA\n[94, 100]\n\n\nA-\n[90, 94)\n\n\nB+\n[87, 90)\n\n\nB\n[83, 87)\n\n\nB-\n[80, 83)\n\n\nC+\n[77, 80)\n\n\nC\n[70, 77)\n\n\nF\n[0, 70)\n\n\n\n\n\n\n\nYou may use your preferred programming language to do your homework and/or your project.\n\n\nHomework\n\n\nHomework will be assigned through the course website in weekly modules.\nTo submit your homework, please go to D2L &gt; Assessments &gt; Dropbox and upload your homework in PDF format.\nNo late or make-up homework for any reason.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#midterm-project-presentation",
    "href": "course-syllabus.html#midterm-project-presentation",
    "title": "Syllabus",
    "section": "Midterm Project Presentation",
    "text": "Midterm Project Presentation\n\nThere will be 2 in-class mini project presentations\nStudents will learn from each other by presenting and discussing the assigned topics.\nMore details about the mini project presentation will be released later.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#final-project",
    "href": "course-syllabus.html#final-project",
    "title": "Syllabus",
    "section": "Final Project",
    "text": "Final Project\n\nThe final project is submitted as a paper and some relevant work.\nThe project submission deadline is Thursday, 5/8, 10 AM.\nMore details about the final project will be released in April.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#generative-ai-and-sharingreusing-code-policy",
    "href": "course-syllabus.html#generative-ai-and-sharingreusing-code-policy",
    "title": "Syllabus",
    "section": "Generative AI and Sharing/Reusing Code Policy",
    "text": "Generative AI and Sharing/Reusing Code Policy\n\nGenerative AI \n\nYou are responsible for the content of all work submitted for this course. You may use generative AI tools such as ChatGPT or DALL-E to generate a first draft of text for your assignments, provided that this use is appropriately documented and cited.\nRead the articles (MLA and APA) to learn how to cite and document the use of AI in your work. Learn more citing chatbots at https://libguides.marquette.edu/generative_technologies/citing\n\n\n\nSharing/Reusing Code\n\nUnless explicitly stated otherwise, you may make use of any online resources, but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solutions.\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#academic-integrity",
    "href": "course-syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\n\nThis course expects all students to follow University and College statements on academic integrity.\nHonor Pledge and Honor Code: I recognize the importance of personal integrity in all aspects of life and work. I commit myself to truthfulness, honor, and responsibility, by which I earn the respect of others. I support the development of good character, and commit myself to uphold the highest standards of academic integrity as an important aspect of personal integrity. My commitment obliges me to conduct myself according to the Marquette University Honor Code.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#accommodation",
    "href": "course-syllabus.html#accommodation",
    "title": "Syllabus",
    "section": "Accommodation",
    "text": "Accommodation\nIf you need to request accommodations, or modify existing accommodations that address disability-related needs, please contact Disability Service.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#important-dates",
    "href": "course-syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nJan 21: Last day to add/swap/drop\nMar 10-16: Spring break\nMar 11: Midterm grade submission\nApr 11: Withdrawal deadline\nApr 17 - Apr 20: Easter break\nMay 3: Last day of class\nMay 8: Final project submission\nMay 13: Final grade submission\n\nClick here for the full Marquette academic calendar.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  }
]
[
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Click here to download the syllabus.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#time-and-location",
    "href": "course-syllabus.html#time-and-location",
    "title": "Syllabus",
    "section": "Time and location",
    "text": "Time and location\n\n\n\n\nDay\nTime\nLocation\n\n\n\n\nLectures\nTu & Th\n3:30 - 4:45 PM\nCuday Hall 120\n\n\nLab\nNone\nNone\nNone",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#office-hours",
    "href": "course-syllabus.html#office-hours",
    "title": "Syllabus",
    "section": "Office Hours",
    "text": "Office Hours\n\nMy in-person office hours are TuTh 4:50 - 5:50 PM, and Wed 12 - 1 PM in Cudahy Hall room 353.\nYou are welcome to schedule an online meeting via Microsoft Teams if you need/prefer.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#prerequisites",
    "href": "course-syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nMATH 4720 (Intro to Statistics), MATH 3100 (Linear Algebra) and MATH 4780 (Regression Analysis)\nHaving taken MATH 4700 (Probability) and MATH 4710 (Statistical Inference) or more advanced ones is strongly recommended.\nThis course is supposed to be taken in the last semester for the applied statistics (APST) master students. Talk to me if you are not sure whether or not this is the right course for you.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#e-mail-policy",
    "href": "course-syllabus.html#e-mail-policy",
    "title": "Syllabus",
    "section": "E-mail Policy",
    "text": "E-mail Policy\n\nI will attempt to reply your email quickly, at least within 24 hours.\nExpect a reply on Monday if you send a question during weekends. If you do not receive a response from me within two days, re-send your question/comment in case there was a ‚Äúmix-up‚Äù with email communication (Hope this won‚Äôt happen!).\nPlease start your subject line with [mssc6250] followed by a clear description of your question. See an example below.\n\n\n\n\nEmail Subject Line Example\n\n\n\nEmail etiquette is important. Please read this article to learn more about email etiquette.\nI am more than happy to answer your questions about this course or data science/statistics in general. However, with tons of email messgaes everyday, I may choose NOT to respond to students‚Äô e-mail if\n\nThe student could answer his/her own inquiry by reading the syllabus or information on the course website or D2L.\nThe student is asking for an extra credit opportunity. The answer is ‚Äúno‚Äù.\nThe student is requesting an extension on homework. The answer is ‚Äúno‚Äù.\nThe student is asking for a grade to be raised for no legitimate reason. The answer is ‚Äúno‚Äù.\nThe student is sending an email with no etiquette.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#required-textbook",
    "href": "course-syllabus.html#required-textbook",
    "title": "Syllabus",
    "section": "Required Textbook",
    "text": "Required Textbook\n\n(ISL) An Introduction to Statistical Learning, by James et al.¬†Publisher: Springer. (Undergraduate to master level, R and Python code)",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#optional-references",
    "href": "course-syllabus.html#optional-references",
    "title": "Syllabus",
    "section": "Optional References",
    "text": "Optional References\n\n(MML) Mathematics for Machine Learning, by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. Publisher: Cambridge University Press. (College level mathematics for machine learning)\n(PML) Probabilistic Machine Learning: An Introduction, by Kevin Murphy. Publisher: MIT Press. (Master to PhD level, lots of mathematics foundations, Python code)\n(PMLA) Probabilistic Machine Learning: Advanced Topics, by Kevin Murphy. Publisher: MIT Press. (PhD level, more probabilistic-based or Bayesian)\n(ESL) The Elements of Statistical Learning, 2nd edition, by Hastie et. al.¬†Publisher: Springer. (PhD level, more frequentist-based)",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#grading-policy",
    "href": "course-syllabus.html#grading-policy",
    "title": "Syllabus",
    "section": "Grading Policy",
    "text": "Grading Policy\n\nYour final grade is earned out of 1000 total points distributed as follows:\n\nHomework: 500 pts\nMidterm project presentation: 300 pts\nFinal project: 200 pts\n\n\nYou will NOT be allowed any extra credit projects/homework/exam to compensate for a poor average. Everyone must be given the same opportunity to do well in this class. Individual exam will NOT be curved. \nThe final grade is based on your percentage of points earned out of 1000 points and the grade-percentage conversion Table. \\([x, y)\\) means greater than or equal to \\(x\\) and less than \\(y\\). For example, 94.1 is in \\([93, 100]\\) and the grade is A and 92.8 is in \\([90, 94)\\) and the grade is A-.\n\n\n\n\nGrade-Percentage Conversion\n\n\nGrade\nPercentage\n\n\n\n\nA\n[94, 100]\n\n\nA-\n[90, 94)\n\n\nB+\n[87, 90)\n\n\nB\n[83, 87)\n\n\nB-\n[80, 83)\n\n\nC+\n[77, 80)\n\n\nC\n[70, 77)\n\n\nF\n[0, 70)\n\n\n\n\n\n\n\nYou may use your preferred programming language to do your homework and/or your project.\n\n\nHomework\n\n\nHomework will be assigned through the course website in weekly modules.\nTo submit your homework, please go to D2L &gt; Assessments &gt; Dropbox and upload your homework in PDF format.\nNo late or make-up homework for any reason.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#midterm-project-presentation",
    "href": "course-syllabus.html#midterm-project-presentation",
    "title": "Syllabus",
    "section": "Midterm Project Presentation",
    "text": "Midterm Project Presentation\n\nThere will be 2 in-class mini project presentations\nStudents will learn from each other by presenting and discussing the assigned topics.\nMore details about the mini project presentation will be released later.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#final-project",
    "href": "course-syllabus.html#final-project",
    "title": "Syllabus",
    "section": "Final Project",
    "text": "Final Project\n\nThe final project is submitted as a paper and some relevant work.\nThe project submission deadline is Thursday, 5/8, 10 AM.\nMore details about the final project will be released in April.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#generative-ai-and-sharingreusing-code-policy",
    "href": "course-syllabus.html#generative-ai-and-sharingreusing-code-policy",
    "title": "Syllabus",
    "section": "Generative AI and Sharing/Reusing Code Policy",
    "text": "Generative AI and Sharing/Reusing Code Policy\n\nGenerative AI \n\nYou are responsible for the content of all work submitted for this course. You may use generative AI tools such as ChatGPT or DALL-E to generate a first draft of text for your assignments, provided that this use is appropriately documented and cited.\nRead the articles (MLA and APA) to learn how to cite and document the use of AI in your work. Learn more citing chatbots at https://libguides.marquette.edu/generative_technologies/citing\n\n\n\nSharing/Reusing Code\n\nUnless explicitly stated otherwise, you may make use of any online resources, but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solutions.\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#academic-integrity",
    "href": "course-syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\n\nThis course expects all students to follow University and College statements on academic integrity.\nHonor Pledge and Honor Code: I recognize the importance of personal integrity in all aspects of life and work. I commit myself to truthfulness, honor, and responsibility, by which I earn the respect of others. I support the development of good character, and commit myself to uphold the highest standards of academic integrity as an important aspect of personal integrity. My commitment obliges me to conduct myself according to the Marquette University Honor Code.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#accommodation",
    "href": "course-syllabus.html#accommodation",
    "title": "Syllabus",
    "section": "Accommodation",
    "text": "Accommodation\nIf you need to request accommodations, or modify existing accommodations that address disability-related needs, please contact Disability Service.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#important-dates",
    "href": "course-syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nJan 21: Last day to add/swap/drop\nMar 10-16: Spring break\nMar 11: Midterm grade submission\nApr 11: Withdrawal deadline\nApr 17 - Apr 20: Easter break\nMay 3: Last day of class\nMay 8: Final project submission\nMay 13: Final grade submission\n\nClick here for the full Marquette academic calendar.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "slides/05-ridge-cv.html#when-ordinary-least-squares-ols-doesnt-work-well-collinearity",
    "href": "slides/05-ridge-cv.html#when-ordinary-least-squares-ols-doesnt-work-well-collinearity",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "When Ordinary Least Squares (OLS) Doesn‚Äôt Work Well: Collinearity",
    "text": "When Ordinary Least Squares (OLS) Doesn‚Äôt Work Well: Collinearity\n\nWhen predictors are highly correlated, \\(\\mathrm{Var}(b_j)\\) is much inflated.1\n\nA tiny change in the training set causes a large change in \\(b_j\\), leading to unreliable estimation and possibly prediction.\n\n\n\n\n\n\\({\\bf X'X} = \\begin{bmatrix} 1 & 0.99 \\\\ 0.99 & 1 \\end{bmatrix}\\) \\(\\quad ({\\bf X'X})^{-1} = \\begin{bmatrix}  50.3 & -49.7 \\\\ -49.7 & 50.3 \\end{bmatrix}\\)\n\n\\(\\mathrm{Var}(b_j) = 50.3\\sigma^2\\)\n\nAn increase in 50-fold over the ideal case when the two regressors are orthogonal.\n\n\n\n\nsolve(matrix(c(1, 0.99, 0.99, 1), 2, 2))\n\n      [,1]  [,2]\n[1,]  50.3 -49.7\n[2,] -49.7  50.3\n\n\nthe eigen vector direction with the smallest eigen value https://online.stat.psu.edu/stat857/node/155/ https://robjhyndman.com/hyndsight/loocv-linear-models/\n\n\nAlthough \\(\\mathrm{Var}(b_j)\\) is still the smallest among all linear unbiased estimators."
  },
  {
    "objectID": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-optimization-perspective",
    "href": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-optimization-perspective",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "When OLS Doesn‚Äôt Work Well: Optimization Perspective",
    "text": "When OLS Doesn‚Äôt Work Well: Optimization Perspective\n\n\\(\\beta_1 = \\beta_2 = 1\\) and \\(\\mathrm{Corr}(x_1, x_2) = 0.99\\)\nThe relatively ‚Äúflat‚Äù valley in the objective function walks along the eigen-vector of \\({\\bf X}'{\\bf X}\\) having the smallest eigen-value.\n\n\n\n\n\n\n\n\n\n\n\n\n\neigen(\n    matrix(c(1, 0.99, 0.99, 1), \n             2, 2)\n    )\n\neigen() decomposition\n$values\n[1] 1.99 0.01\n\n$vectors\n      [,1]   [,2]\n[1,] 0.707 -0.707\n[2,] 0.707  0.707\n\n\n\n\n\nFrom optimization point of view, the objective function (\\(\\ell_2\\) loss) is ‚Äúflat‚Äù along certain directions in the parameter domain.\na small eigen-value in XTX makes the corresponding eigen-value large in the inverse."
  },
  {
    "objectID": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-optimization-perspective-1",
    "href": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-optimization-perspective-1",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "When OLS Doesn‚Äôt Work Well: Optimization Perspective",
    "text": "When OLS Doesn‚Äôt Work Well: Optimization Perspective\n\nA little change in the training set perturbs the objective function. The LSEs lie on a valley centered around the truth."
  },
  {
    "objectID": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-high-variance",
    "href": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-high-variance",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "When OLS Doesn‚Äôt Work Well: High Variance",
    "text": "When OLS Doesn‚Äôt Work Well: High Variance\n\nThe optimizer could land anywhere along the valley, leading to large variance of the LSE.\nOver many simulation runs, the LSE lies around the line of \\(\\beta_1 + \\beta_2 = 2\\), the direction of the eigen-vector of the smallest eigen-value."
  },
  {
    "objectID": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-large-p-small-n-high-dimensional-data",
    "href": "slides/05-ridge-cv.html#when-ols-doesnt-work-well-large-p-small-n-high-dimensional-data",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "When OLS Doesn‚Äôt Work Well: Large-\\(p\\)-small-\\(n\\) (High Dimensional Data)",
    "text": "When OLS Doesn‚Äôt Work Well: Large-\\(p\\)-small-\\(n\\) (High Dimensional Data)\n\nOLS stays well in the world of ‚Äúlarge-\\(n\\)-small-\\(p\\)‚Äù.\nWhen \\(p &gt; n\\), \\({\\bf X}'{\\bf X}\\) is not invertible.\nThere is no unique \\(\\boldsymbol \\beta\\) estimate.\n\n\nIntuition: Too many degrees of freedom (\\(p\\)) to specify a model, but not enough information (\\(n\\)) to decide which one is the one.\n\nToo flexible and ends up with overfitting"
  },
  {
    "objectID": "slides/05-ridge-cv.html#remedy-for-large-variance-and-large-p-small-n",
    "href": "slides/05-ridge-cv.html#remedy-for-large-variance-and-large-p-small-n",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Remedy for Large Variance and Large-\\(p\\)-small-\\(n\\)\n",
    "text": "Remedy for Large Variance and Large-\\(p\\)-small-\\(n\\)\n\n\nMake \\({\\bf X}'{\\bf X}\\) invertible when \\(p &gt; n\\) by regularizing coefficient behavior!\nA good estimator balances bias and variance well, or minimizes the mean square error \\[\\text{MSE}(\\hat{\\beta}) = E[(\\hat{\\beta} - \\beta)^2] = \\mathrm{Var}(\\hat{\\beta}) + \\text{Bias}(\\hat{\\beta})^2\\]\nA slightly biased estimator \\(\\hat{\\boldsymbol \\beta}\\) that has much smaller variance and MSE than the LSE \\({\\bf b}\\)."
  },
  {
    "objectID": "slides/05-ridge-cv.html#ridge-regression-1",
    "href": "slides/05-ridge-cv.html#ridge-regression-1",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nIdea: Add a ridge (diagonal matrix) to \\({\\bf X} ' {\\bf X}\\).1 \\[\\widehat{\\boldsymbol \\beta}^\\text{r} = (\\mathbf{X}' \\mathbf{X}+ n \\lambda \\mathbf{I})^{-1} \\mathbf{X}' \\mathbf{y},\\]\n\n\n\n\nTo regularize coefficients, add an \\(\\ell_2\\) penalty to the residual sum of squares, for some tuning parameter \\(\\lambda &gt; 0\\).\n\n\\[\n\\begin{align}\n\\widehat{\\boldsymbol \\beta}^\\text{r} =& \\, \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} \\underbrace{\\lVert \\mathbf{y}- \\mathbf{X}\\boldsymbol \\beta\\rVert_2^2}_{SS_{res}} + n \\lambda \\lVert\\boldsymbol \\beta\\rVert_2^2\\\\\n=& \\, \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} \\underbrace{\\frac{1}{n} \\sum_{i=1}^n (y_i - x_i'\\boldsymbol \\beta)^2}_{\\text{MSE}_{\\texttt{Tr}}} + \\lambda \\sum_{j=1}^p \\beta_j^2\\\\\n=& \\, \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} \\color{green} - \\text{ goodness of fit } + \\text{ model complexity/flexibility}\n\\end{align}\n\\]\n\nThis is a special case of the Tikhonov regularization."
  },
  {
    "objectID": "slides/05-ridge-cv.html#properties-of-ridge-regression",
    "href": "slides/05-ridge-cv.html#properties-of-ridge-regression",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Properties of Ridge Regression",
    "text": "Properties of Ridge Regression\n\\[\n\\begin{align}\n\\widehat{\\boldsymbol \\beta}^\\text{r} =& \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\beta} \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i'\\boldsymbol \\beta)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2,\n\\end{align}\n\\]\nProperties of ridge regression:\n\nHas less degrees of freedom in the sense that the cost gets higher when larger coefficients are used.\n\n\n\nFavors small-magnitude coefficient estimates (Shrinkage) to avoid cost penalty.\n\n\n\n\nThe shrinkage parameter \\(\\lambda\\) controls the degree of penalty.\n\n\n\\(\\lambda \\rightarrow 0\\): No penalty, and \\(\\widehat{\\boldsymbol \\beta}^\\text{r} = \\bf b\\).\n\n\\(\\lambda \\rightarrow \\infty\\): Unbearable penalty, and \\(\\widehat{\\boldsymbol \\beta}^\\text{r} \\rightarrow \\mathbf{0}\\)"
  },
  {
    "objectID": "slides/05-ridge-cv.html#ridge-penalty",
    "href": "slides/05-ridge-cv.html#ridge-penalty",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Ridge Penalty",
    "text": "Ridge Penalty\n\n\n\\[\\lambda \\lVert \\boldsymbol \\beta\\rVert^2_2 = \\lambda \\boldsymbol \\beta' \\mathbf{I}\\boldsymbol \\beta\\]\n\nThe penalty contour is circle-shaped\nForce the objective function to be more convex\nA more stable or less varying solution"
  },
  {
    "objectID": "slides/05-ridge-cv.html#geometrical-meaning-of-ridge-regression",
    "href": "slides/05-ridge-cv.html#geometrical-meaning-of-ridge-regression",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Geometrical Meaning of Ridge Regression",
    "text": "Geometrical Meaning of Ridge Regression\n\nSource: https://online.stat.psu.edu/stat508/lessons/Lesson06\nPerform PCA of X\nProject y onto the PCs\nShrinks the projection\nReassemble the PCs using all the shrunken length"
  },
  {
    "objectID": "slides/05-ridge-cv.html#more-convex-loss-function",
    "href": "slides/05-ridge-cv.html#more-convex-loss-function",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "More Convex Loss Function",
    "text": "More Convex Loss Function\n\nAdding a ridge penalty forces the objective to be more convex due to the added eigenvalues.\n\n\neigen(matrix(c(1, 0.99, 0.99, 1), 2, 2) + diag(2))[1]\n\n$values\n[1] 2.99 1.01\n\n\n\nHence, by adding this to the OLS objective function, the solution is more stable, in the sense that each time we observe a new set of data, this contour looks pretty much the same. This may be interpreted in several different ways such as: 1) the objective function is more convex and less affected by the random samples; 2) the variance of the estimator is smaller because the eigenvalues of \\(\\mathbf{X}^\\text{T}\\mathbf{X}+ n \\lambda \\mathbf{I}\\) are large."
  },
  {
    "objectID": "slides/05-ridge-cv.html#the-bias-variance-tradeoff",
    "href": "slides/05-ridge-cv.html#the-bias-variance-tradeoff",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "The Bias-variance Tradeoff",
    "text": "The Bias-variance Tradeoff\n\nAs \\(\\lambda \\downarrow\\), bias \\(\\downarrow\\) and variance \\(\\uparrow\\)\n\nAs \\(\\lambda \\uparrow\\), bias \\(\\uparrow\\) and variance \\(\\downarrow\\)\n\n\n\nThis effect is gradually changing as we increase the penalty level. The following simulation shows how the variation of \\(\\boldsymbol \\beta\\) changes. We show this with two penalty values, and see how the estimated parameters are away from the truth.\nNow, we may ask the question: is it worth it? In fact, this bias and variance will be then carried to the predicted values \\(x^\\text{T}\\widehat{\\boldsymbol \\beta}^\\text{ridge}\\). Hence, we can judge if this is beneficial from the prediction accuracy. And we need some procedure to do this.\nRemark: The bias-variance trade-off will appear frequently in this course. And the way to evaluate the benefit of this is to see if it eventually reduces the prediction error (\\(\\text{Bias}^2 + \\text{Variance}\\) plus a term called irreducible error, which will be introduced in later chapter)."
  },
  {
    "objectID": "slides/05-ridge-cv.html#lower-test-mse",
    "href": "slides/05-ridge-cv.html#lower-test-mse",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Lower Test MSE",
    "text": "Lower Test MSE\n\n\n\nWhen \\(b_j\\) has large variance or \\(p &gt; n\\), ridge regression could have lower test MSE and better predictive performance.\n\\(\\text{MSE}_{\\texttt{Tr}}\\) (purple)\nSquared bias (black)\nVariance (green)\n\n\n\n\n\n\n\nSource: ISL Figure 6.5"
  },
  {
    "objectID": "slides/05-ridge-cv.html#masslm.ridge",
    "href": "slides/05-ridge-cv.html#masslm.ridge",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "MASS::lm.ridge()",
    "text": "MASS::lm.ridge()\n\nThe lambda parameter in lm.ridge() specifies the \\(n\\lambda\\) in our notation.\nOLS is scale equivalent: \\(X_jb_j\\) remains the same regardless of how \\(X_j\\) is scaled.\nRidge coefficient estimates can change substantially when multiplying a given predictor by a constant, i.e., \\(X_j\\hat{\\beta}^{r}_{j, \\lambda}\\) depends on \\(\\lambda\\), the scaling of \\(X_j\\), and even the scaling of other predictors.\nStandardize all predictors!1\n\n\n\nhead(mtcars, 3)\n\n               mpg cyl disp  hp drat   wt qsec vs am gear carb\nMazda RX4     21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\nMazda RX4 Wag 21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\nDatsun 710    22.8   4  108  93 3.85 2.32 18.6  1  1    4    1\n\n(fit &lt;- MASS::lm.ridge(mpg ~ ., data = mtcars, lambda = 1)) ## ridge fit\n\n              cyl     disp       hp     drat       wt     qsec       vs \n16.53766 -0.16240  0.00233 -0.01493  0.92463 -2.46115  0.49259  0.37465 \n      am     gear     carb \n 2.30838  0.68572 -0.57579 \n\n\n\n\nIn practice, if the intercept is not our interest, we also standardize the response."
  },
  {
    "objectID": "slides/05-ridge-cv.html#scaling-issues-of-lm.ridge",
    "href": "slides/05-ridge-cv.html#scaling-issues-of-lm.ridge",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Scaling Issues of lm.ridge()\n",
    "text": "Scaling Issues of lm.ridge()\n\n\ncoef(fit) transforms these back to the original scale.\nfit$coef shows the coefficients of the standardized predictors.\n\n\ncoef(fit)\n\n              cyl     disp       hp     drat       wt     qsec       vs \n16.53766 -0.16240  0.00233 -0.01493  0.92463 -2.46115  0.49259  0.37465 \n      am     gear     carb \n 2.30838  0.68572 -0.57579 \n\nfit$coef\n\n   cyl   disp     hp   drat     wt   qsec     vs     am   gear   carb \n-0.285  0.285 -1.008  0.487 -2.370  0.866  0.186  1.134  0.498 -0.915"
  },
  {
    "objectID": "slides/05-ridge-cv.html#scaling-issues-of-lm.ridge-1",
    "href": "slides/05-ridge-cv.html#scaling-issues-of-lm.ridge-1",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Scaling Issues of lm.ridge()\n",
    "text": "Scaling Issues of lm.ridge()\n\n\nfit$coef\n\n   cyl   disp     hp   drat     wt   qsec     vs     am   gear   carb \n-0.285  0.285 -1.008  0.487 -2.370  0.866  0.186  1.134  0.498 -0.915 \n\n\n\n\nlm.ridge() uses \\(n\\) instead of \\(n-1\\) when calculating the standard deviation.\n\n\n# each column has mean 0 and var 1\nX &lt;- scale(data.matrix(mtcars[, -1]), center = TRUE, scale = TRUE)\n\n# center y but not scaling\ny &lt;- scale(mtcars$mpg, center = TRUE, scale = FALSE)\n\n\n\n# lambda = 1\nmy_ridge_coef &lt;- solve(t(X) %*% X + diag(ncol(X))) %*% t(X) %*% y\nt(my_ridge_coef)\n\n        cyl disp    hp  drat    wt qsec   vs   am  gear   carb\n[1,] -0.294 0.27 -1.02 0.495 -2.39 0.87 0.19 1.15 0.505 -0.937"
  },
  {
    "objectID": "slides/05-ridge-cv.html#scaling-issues-of-lm.ridge-2",
    "href": "slides/05-ridge-cv.html#scaling-issues-of-lm.ridge-2",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Scaling Issues of lm.ridge()\n",
    "text": "Scaling Issues of lm.ridge()\n\n\nfit$coef\n\n   cyl   disp     hp   drat     wt   qsec     vs     am   gear   carb \n-0.285  0.285 -1.008  0.487 -2.370  0.866  0.186  1.134  0.498 -0.915 \n\n\n\n\n# use n instead of (n-1) for standardization\nn &lt;- nrow(X); X &lt;- X * sqrt(n / (n - 1))\n\n\n\n        cyl  disp    hp  drat    wt  qsec    vs   am  gear   carb\n[1,] -0.285 0.285 -1.01 0.487 -2.37 0.866 0.186 1.13 0.498 -0.915\n\n\nMore discussion at https://stats.stackexchange.com/questions/288754/lm-ridge-returns-different-results-that-are-from-manual-calculation"
  },
  {
    "objectID": "slides/05-ridge-cv.html#ridge-trace",
    "href": "slides/05-ridge-cv.html#ridge-trace",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Ridge Trace",
    "text": "Ridge Trace\n\nridge_fit &lt;- lm.ridge(mpg ~ ., data = mtcars, lambda = 0:40)\nmatplot(coef(ridge_fit)[, -1], type = \"l\", xlab = \"Lambda\", ylab = \"Coefficients\")\ntext(rep(1, 10), coef(ridge_fit)[1,-1], colnames(mtcars)[2:11])\n\n\n\nSelect a value of \\(\\lambda\\) at which the ridge estimates are stable."
  },
  {
    "objectID": "slides/05-ridge-cv.html#methods-for-choosing-lambda",
    "href": "slides/05-ridge-cv.html#methods-for-choosing-lambda",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Methods for Choosing \\(\\lambda\\)\n",
    "text": "Methods for Choosing \\(\\lambda\\)\n\n\nMASS::select(ridge_fit)\n\nmodified HKB estimator is 2.59 \nmodified L-W estimator is 1.84 \nsmallest value of GCV  at 15 \n\n\n\nHoerl, Kennard, and Baldwin (1975): \\(\\lambda \\approx \\frac{p \\hat{\\sigma}^2}{{\\bf b}'{\\bf b}}\\)\nLawless and Wang (1976): \\(\\lambda \\approx \\frac{np \\hat{\\sigma}^2}{{\\bf b'X}'{\\bf Xb}}\\)\nGolub, Health, and Wahba (1979): Generalized Cross Validation"
  },
  {
    "objectID": "slides/05-ridge-cv.html#cross-validation",
    "href": "slides/05-ridge-cv.html#cross-validation",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Cross Validation",
    "text": "Cross Validation\n\nCross Validation (CV) is a resampling method.\nResampling methods refit a model of interest to data sampled from the training set.\n\nCV can be used to\n\nestimate the test error when there is no test data. (Model assessment)\n\nselect the tuning parameters that controls the model complexity/flexibility. (Model selection)\n\n\n\n\nHence, by adding this to the OLS objective function, the solution is more stable, in the sense that each time we observe a new set of data, this contour looks pretty much the same. This may be interpreted in several different ways such as: 1) the objective function is more convex and less affected by the random samples; 2) the variance of the estimator is smaller because the eigenvalues of \\(\\mathbf{X}^\\text{T}\\mathbf{X}+ n \\lambda \\mathbf{I}\\) are large."
  },
  {
    "objectID": "slides/05-ridge-cv.html#section-1",
    "href": "slides/05-ridge-cv.html#section-1",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "",
    "text": "\\(k\\)-Fold Cross Validation\n\nRandomly divide the training set into \\(k\\) folds, of approximately equal size.\nUse 1 fold for validation to compute MSE, and the remaining \\(k - 1\\) partitions for training.\nRepeat \\(k\\) times. Each time a different fold is treated as a validation set.\nAverage \\(k\\) metrics, \\(\\text{MSE}_{CV} = \\frac{1}{k}\\sum_{i=1}^k\\text{MSE}_i\\).\nUse the CV estimate \\(\\text{MSE}_{CV}\\) to select the ‚Äúbest‚Äù tuning parameter.\n\n\n\n\n\n\n\nFive-fold cross validation. Source: Data science in a box\n\n\n\n\nCan compute other performance measures."
  },
  {
    "objectID": "slides/05-ridge-cv.html#glmnet-package",
    "href": "slides/05-ridge-cv.html#glmnet-package",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "\nglmnet package \n",
    "text": "glmnet package \n\n\nThe parameter alpha controls the ridge (alpha = 0) and lasso (alpha = 1) penalties.\nSupply a decreasing sequence of lambda values.\nlm.ridge() use \\(SS_{res}\\) and \\(n\\lambda\\), while glmnet() use \\(\\text{MSE}_{\\texttt{Tr}}\\) and \\(\\lambda\\).\nArgument x should be a matrix.\n\n\n\n\nglmnet(x = data.matrix(mtcars[, -1]), \n       y = mtcars$mpg, \n       alpha = 0, \n       lambda = 5:0/nrow(mtcars))\n\n\n\nlm.ridge(formula = mpg ~ ., \n         data = mtcars, \n         lambda = 5:0/nrow(mtcars))\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nlm.ridge() and glmnet() coefficients do not match exactly, specially when transforming back to original scale.\nNo need to worry too much as we focus on predictive performance.\n\n\n\n\n\n\nhttps://stats.stackexchange.com/questions/74206/ridge-regression-results-different-in-using-lm-ridge-and-glmnet"
  },
  {
    "objectID": "slides/05-ridge-cv.html#k-fold-cross-validation-using-cv.glmnet",
    "href": "slides/05-ridge-cv.html#k-fold-cross-validation-using-cv.glmnet",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "\n\\(k\\)-Fold Cross Validation using cv.glmnet()1\n",
    "text": "\\(k\\)-Fold Cross Validation using cv.glmnet()1\n\n\nThe \\(\\lambda\\) values are automatically selected, on the \\(\\log_{e}\\) scale.\n\n\nridge_cv_fit &lt;- cv.glmnet(x = data.matrix(mtcars[, -1]), y = mtcars$mpg, alpha = 0,\n                          nfolds = 10, type.measure = \"mse\")\nplot(ridge_cv_fit$glmnet.fit, \"lambda\")\n\n\n\n\n\n\n\n\nWhy s and not lambda? In case we want to allow one to specify the model size in other ways in the future. s: Value(s) of the penalty parameter lambda at which predictions are required. Default is the entire sequence used to create the model.\nThere are other ways to do CV for ridge regression in R, for example, the caret (Classification And REgression Training) package and the rsample package in tidymodels ecosystem."
  },
  {
    "objectID": "slides/05-ridge-cv.html#determine-lambda",
    "href": "slides/05-ridge-cv.html#determine-lambda",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Determine \\(\\lambda\\)\n",
    "text": "Determine \\(\\lambda\\)\n\n\n\n\nplot(ridge_cv_fit)\n\n\n\n\n\n\n\n\n\nridge_cv_fit$lambda.min\n\n[1] 2.75\n\n# largest lambda s.t. error is within 1 s.e of the min\nridge_cv_fit$lambda.1se \n\n[1] 16.1\n\ncoef(ridge_cv_fit, s = \"lambda.min\")\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                  s1\n(Intercept) 21.11734\ncyl         -0.37134\ndisp        -0.00525\nhp          -0.01161\ndrat         1.05477\nwt          -1.23420\nqsec         0.16245\nvs           0.77196\nam           1.62381\ngear         0.54417\ncarb        -0.54742\n\n\n\n\ncoef(fit2, s = ‚Äúlambda.1se‚Äù) This plots the cross-validation curve (red dotted line) along with upper and lower standard deviation curves along the ùúÜ sequence (error bars). Two special values along the ùúÜ sequence are indicated by the vertical dotted lines. lambda.min is the value of ùúÜ that gives minimum mean cross-validated error, while lambda.1se is the value of ùúÜ that gives the most regularized model such that the cross-validated error is within one standard error of the minimum."
  },
  {
    "objectID": "slides/05-ridge-cv.html#generalized-cross-validation",
    "href": "slides/05-ridge-cv.html#generalized-cross-validation",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Generalized Cross-Validation",
    "text": "Generalized Cross-Validation\n\nThe generalized cross-validation (GCV) is a modified version of the leave-one-out CV (\\(n\\)-fold CV).\nThe GCV criterion is given by \\[\\text{GCV}(\\lambda) = \\frac{1}{n}\\sum_{i=1}^n \\left[ \\frac{y_i - x_i' \\widehat{\\boldsymbol \\beta}^\\text{r}_\\lambda}{1 - \\frac{\\text{Trace}(\\mathbf{S}_\\lambda)}{n}} \\right]^2\\]\n\nwhere \\(\\mathbf{S}_\\lambda\\) is the hat matrix corresponding to the ridge regression:\n\\[\\mathbf{S}_\\lambda = \\mathbf{X}(\\mathbf{X}' \\mathbf{X}+ n\\lambda \\mathbf{I})^{-1} \\mathbf{X}'\\]\nThe interesting fact about leave-one-out CV in the linear regression setting is that we do not need to explicitly fit all leave-one-out models.\n\nESL p.¬†244\nlm.ridge code"
  },
  {
    "objectID": "slides/05-ridge-cv.html#generalized-cross-validation-1",
    "href": "slides/05-ridge-cv.html#generalized-cross-validation-1",
    "title": "Ridge Regression and Cross Validation \n",
    "section": "Generalized Cross-Validation",
    "text": "Generalized Cross-Validation\n\n\n\nplot(ridge_fit$lambda, \n     ridge_fit$GCV, \n     type = \"l\", col = \"darkgreen\", \n     ylab = \"GCV\", xlab = \"Lambda\", \n     lwd = 3)\n\n\n\n\n\n\n\n\n\nidx &lt;- which.min(ridge_fit$GCV)\nridge_fit$lambda[idx]\n\n[1] 15\n\nround(coef(ridge_fit)[idx, ], 2)\n\n        cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb \n21.13 -0.37 -0.01 -0.01  1.05 -1.23  0.16  0.77  1.62  0.54 -0.55 \n\n\n\n\nSelect the best \\(\\lambda\\) that produces the smallest GCV.\nYou can clearly see that the GCV decreases initially, as \\(\\lambda\\) increases, this is because the reduced variance is more beneficial than the increased bias. However, as \\(\\lambda\\) increases further, the bias term will eventually dominate and causing the overall prediction error to increase. The fitted MSE under this model is\n\n\n\n\nmssc6250-s25.github.io/website"
  },
  {
    "objectID": "slides/01-syllabus.html#my-journey",
    "href": "slides/01-syllabus.html#my-journey",
    "title": "Welcome Aboard üôå",
    "section": "My Journey",
    "text": "My Journey\n\nAssistant Professor (2020/08 - )\n\n\n\n\n\n\n\n\n\n\nPostdoctoral Fellow\n\n\n\n\n\n\n\n\n\n\nPhD in Statistics and Applied Mathematics\n\n\n\n\n\n\n\n\n\n\nMA in Economics/PhD program in Statistics\n\n\n\n\n\n\n\n\n\nAfter college, working and doing military service for several years, I came to the US for my PhD degree. Originally I would like to study economics, but then I switched my major to statistics.\n\nI got my master degree in economics from Indiana University Bloomington, then I transferred to UC Santa Cruz to finish my PhD studies.\nThen I spent two years doing my postdoctoral research at Rice University in Houston, Texas.\nFinally, in fall 2020, I came to Marquette as an assistant professor.\nMidwest/Indiana-West/California-South/Texas-Midwest/Wisconsin\nBeen to any one of these universities/cities?\nThe most beautiful campus.\nWho are international students? I can totally understand how hard studying and living in another country. Feel free to share your stories or difficulties, and I am more than happy to help you if you have any questions.\nPoor listening and speaking skills. I was shy.\nOK so, this is my background. How about you introducing yourself as well. You can share anything, your major, hobbies, your favorite food, what do you want to do after graduation, anything,\nI have the class list. I‚Äôd like to learn your face and remember your name. You know, you all wear a mask. It‚Äôs hard to recognize you and connect your name and your face.\nWhen I call your name, you can say something about yourself. No need to be long, couple of seconds are good."
  },
  {
    "objectID": "slides/01-syllabus.html#my-research",
    "href": "slides/01-syllabus.html#my-research",
    "title": "Welcome Aboard üôå",
    "section": "My Research",
    "text": "My Research\n\nBayesian spatio-temporal modeling and computation in neuroimaging/epidemiology\nBayesian deep learning for image classification\nEfficient MCMC for high dimensional regression\nData science education\n\n\n\nfMRI\n\n\n\n\n\n\n\n\n\nEEG"
  },
  {
    "objectID": "slides/01-syllabus.html#how-to-reach-me",
    "href": "slides/01-syllabus.html#how-to-reach-me",
    "title": "Welcome Aboard üôå",
    "section": "How to Reach Me",
    "text": "How to Reach Me\n\nOffice hours TuTh 4:50 - 5:50 PM and Wed 12 - 1 PM in Cudahy Hall 353.\nüìß cheng-han.yu@marquette.edu\n\nAnswer your question within 24 hours.\nExpect a reply on Monday if shoot me a message on weekends.\nStart your subject line with [mssc6250] followed by a clear description of your question.\n\n\n\n\n\nI will NOT reply your e-mail if ‚Ä¶ Check the email policy in the syllabus!"
  },
  {
    "objectID": "slides/01-syllabus.html#prerequisites",
    "href": "slides/01-syllabus.html#prerequisites",
    "title": "Welcome Aboard üôå",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nOn bulletin: MATH 4720 (Intro to Statistics), MATH 3100 (Linear Algebra) and/or MSSC 5780 (Regression Analysis)\nProgramming experience (Who does machine learning without coding?)\n\n\n\nHaving taken MSSC 5700 (Probability) and MSSC 5710 (Stats Inference) or other math and statistics courses (Stats Computing, etc) is recommended."
  },
  {
    "objectID": "slides/01-syllabus.html#textbook",
    "href": "slides/01-syllabus.html#textbook",
    "title": "Welcome Aboard üôå",
    "section": "Textbook",
    "text": "Textbook\n\n\n\n\n(ISL) An Introduction to Statistical Learning, by James et al.¬†Publisher: Springer. \n\nDiscuss all chapters except Chapter 11 (survival analysis) and 13 (multiple testing).\nR and Python code\n\nIn the Preface,\n\n‚Ä¶ for advanced undergraduates or master‚Äôs students in Statistics or related quantitative fields,\n\n\n‚Ä¶ concentrate more on the applications of the methods and less on the mathematical details."
  },
  {
    "objectID": "slides/01-syllabus.html#optional-references-1",
    "href": "slides/01-syllabus.html#optional-references-1",
    "title": "Welcome Aboard üôå",
    "section": "Optional References",
    "text": "Optional References\n\n\n\n(MML) Mathematics for Machine Learning, by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. Publisher: Cambridge University Press.\nCollege level mathematics for machine learning\nMathematical concepts behind models and algorithms"
  },
  {
    "objectID": "slides/01-syllabus.html#optional-references-2",
    "href": "slides/01-syllabus.html#optional-references-2",
    "title": "Welcome Aboard üôå",
    "section": "Optional References",
    "text": "Optional References\n\n\n\n(PMLI) Probabilistic Machine Learning: An Introduction, by Kevin Murphy. Publisher: MIT Press.\nSelf-contained with lots of mathematics foundations\nPython code"
  },
  {
    "objectID": "slides/01-syllabus.html#optional-references-3",
    "href": "slides/01-syllabus.html#optional-references-3",
    "title": "Welcome Aboard üôå",
    "section": "Optional References",
    "text": "Optional References\n\n\n\n(PMLA) Probabilistic Machine Learning: Advanced Topics, by Kevin Murphy. Publisher: MIT Press.\nPhD level\nProbabilistic or distributional-based"
  },
  {
    "objectID": "slides/01-syllabus.html#optional-references-4",
    "href": "slides/01-syllabus.html#optional-references-4",
    "title": "Welcome Aboard üôå",
    "section": "Optional References",
    "text": "Optional References\n\n\n\n(ESL) The Elements of Statistical Learning, 2nd edition, by Hastie et. al.¬†Publisher: Springer.\nFor PhD students or researchers in mathematical sciences\nFrequentist-based"
  },
  {
    "objectID": "slides/01-syllabus.html#course-website---httpsmssc6250-s25.github.iowebsite",
    "href": "slides/01-syllabus.html#course-website---httpsmssc6250-s25.github.iowebsite",
    "title": "Welcome Aboard üôå",
    "section": "Course Website - https://mssc6250-s25.github.io/website/\n",
    "text": "Course Website - https://mssc6250-s25.github.io/website/"
  },
  {
    "objectID": "slides/01-syllabus.html#learning-management-system-d2l",
    "href": "slides/01-syllabus.html#learning-management-system-d2l",
    "title": "Welcome Aboard üôå",
    "section": "Learning Management System (D2L)",
    "text": "Learning Management System (D2L)\n\n\n\n\n\n\nSubmit your homework Assessments &gt; Dropbox.\nCheck your grade Assessments &gt; Grades."
  },
  {
    "objectID": "slides/01-syllabus.html#grading-policy",
    "href": "slides/01-syllabus.html#grading-policy",
    "title": "Welcome Aboard üôå",
    "section": "Grading Policy ‚ú®",
    "text": "Grading Policy ‚ú®\n\nThe grade is earned out of 1000 total points distributed as follows:\n\nHomework: 500 pts\nMidterm mini-project presentations: 300 pts\nFinal project: 200 pts\n\n\n\n‚ùå No extra credit projects/homework/exam to compensate for a poor grade."
  },
  {
    "objectID": "slides/01-syllabus.html#grade-percentage-conversion",
    "href": "slides/01-syllabus.html#grade-percentage-conversion",
    "title": "Welcome Aboard üôå",
    "section": "Grade-Percentage Conversion",
    "text": "Grade-Percentage Conversion\n\nYour final grade is based on your percentage of pts earned out of 1000 pts.\n\n[x, y) means greater than or equal to x and less than y.\n\n\n\n\n\n\nGrade\nPercentage\n\n\n\nA\n[94, 100]\n\n\nA-\n[90, 94)\n\n\nB+\n[87, 90)\n\n\nB\n[83, 87)\n\n\nB-\n[80, 83)\n\n\nC+\n[77, 80)\n\n\nC\n[70, 77)\n\n\nF\n[0, 70)"
  },
  {
    "objectID": "slides/01-syllabus.html#homework-500-pts",
    "href": "slides/01-syllabus.html#homework-500-pts",
    "title": "Welcome Aboard üôå",
    "section": "Homework (500 pts)",
    "text": "Homework (500 pts)\n\n\nAssessments &gt; Dropbox and upload your homework in PDF format.\n‚ùå No make-up homework.\nDue Friday 11:59 PM  (Hard deadline and no late submission).\nYou have at least one week to finish your homework."
  },
  {
    "objectID": "slides/01-syllabus.html#mini-project-presentation-300-pts",
    "href": "slides/01-syllabus.html#mini-project-presentation-300-pts",
    "title": "Welcome Aboard üôå",
    "section": "Mini-Project Presentation (300 pts)",
    "text": "Mini-Project Presentation (300 pts)\n\nThere will be 2 in-class mini-project presentations (150 pts each).\nLearn from each other by presenting and discussing the assigned topics.\nMore details about the activity will be released later."
  },
  {
    "objectID": "slides/01-syllabus.html#final-project-200-pts",
    "href": "slides/01-syllabus.html#final-project-200-pts",
    "title": "Welcome Aboard üôå",
    "section": "Final Project (200 pts)",
    "text": "Final Project (200 pts)\n\nThe final project is submitted as a paper (and some relevant work?)\nThe project submission deadline is Thursday, 5/8, 10 AM.\nMore details about the project will be released later."
  },
  {
    "objectID": "slides/01-syllabus.html#which-programming-language",
    "href": "slides/01-syllabus.html#which-programming-language",
    "title": "Welcome Aboard üôå",
    "section": "Which Programming Language?",
    "text": "Which Programming Language?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse any language you prefer!"
  },
  {
    "objectID": "slides/01-syllabus.html#generative-ai-and-sharingreusing-code-policy",
    "href": "slides/01-syllabus.html#generative-ai-and-sharingreusing-code-policy",
    "title": "Welcome Aboard üôå",
    "section": "Generative AI and Sharing/Reusing Code Policy",
    "text": "Generative AI and Sharing/Reusing Code Policy\n\n\n\nYou are responsible for the content of all work submitted for this course.\nYou may use generative AI tools such as ChatGPT or DALL-E to generate a first draft of text for your assignments, provided that this use is appropriately documented and cited.\nLearn how to cite the use of AI in MLA and APA format, and more\n\n\n\n\n\n\n\n\n\n\n\n\n\nSharing/Reusing Code\n\nUnless explicitly stated otherwise, you may make use of any online resources, but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solutions.\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source.\n\n\n\n\n\nmssc6250-s25.github.io/website"
  },
  {
    "objectID": "slides/04-lin-reg.html#linear-regression",
    "href": "slides/04-lin-reg.html#linear-regression",
    "title": "Linear Regression üìà",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nLinear regression is an approach to supervised learning.\nIt assumes that the dependence of \\(Y\\) on \\(X_1, X_2, \\dots, X_k\\)1 is linear.\nTrue regression functions \\(f(x)\\) are never linear!\nAlthough it is overly simplistic, the linear model has distinct advantages in terms of its interpretability and often shows good predictive performance.\n\nlinear regression is extremely useful both conceptually and practically\nThe textbook ISL uses \\(p\\) instead of \\(k\\) to denote the number of predictors in the model. We use \\(k\\) here, but switch to \\(p\\) starting next week."
  },
  {
    "objectID": "slides/04-lin-reg.html#why-multiple-linear-regression-mlr-model",
    "href": "slides/04-lin-reg.html#why-multiple-linear-regression-mlr-model",
    "title": "Linear Regression üìà",
    "section": "Why Multiple Linear Regression (MLR) Model",
    "text": "Why Multiple Linear Regression (MLR) Model\n\nOur target response may be affected by several factors.\nTotal sales \\((Y)\\) and amount of money spent on advertising on YouTube (YT) \\((X_1)\\), Facebook (FB) \\((X_2)\\), Instagram (IG) \\((X_3)\\).1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredict sales based on the three advertising expenditures and see which medium is more effective.\nIn ISL, the predictors are TV, radio and newspaper."
  },
  {
    "objectID": "slides/04-lin-reg.html#fit-separate-simple-linear-regression-slr-models",
    "href": "slides/04-lin-reg.html#fit-separate-simple-linear-regression-slr-models",
    "title": "Linear Regression üìà",
    "section": "Fit Separate Simple Linear Regression (SLR) Models",
    "text": "Fit Separate Simple Linear Regression (SLR) Models\n\n\n\n‚ùå Fitting a separate SLR model for each predictor is not satisfactory."
  },
  {
    "objectID": "slides/04-lin-reg.html#dont-fit-a-separate-simple-linear-regression",
    "href": "slides/04-lin-reg.html#dont-fit-a-separate-simple-linear-regression",
    "title": "Linear Regression üìà",
    "section": "Don‚Äôt Fit a Separate Simple Linear Regression",
    "text": "Don‚Äôt Fit a Separate Simple Linear Regression\n\nüëâ How to make a single prediction of sales given levels of the 3 advertising media budgets?\n\n How to predict the sales when the amount spent on YT is 50, 100 on FB and 30 on IG? \n\n\n\n\n\nüëâ Each regression equation ignores the other 2 media in forming coefficient estimates.\n\n The effect of FB advertising on sales may be increased or decreased when YT and IG advertising are in the model. \n IG advertising may have no impact on sales when YT and FB advertising are in the model. \n\n\n\n\n\n\nüëçüëç Better approach: extend the SLR model so that it can directly accommodate multiple predictors."
  },
  {
    "objectID": "slides/04-lin-reg.html#multiple-linear-regression-model",
    "href": "slides/04-lin-reg.html#multiple-linear-regression-model",
    "title": "Linear Regression üìà",
    "section": "Multiple Linear Regression Model",
    "text": "Multiple Linear Regression Model\n\nThe population MLR model: \\[Y_i= \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\dots + \\beta_kX_{ik} + \\epsilon_i\\]  \n\nIn the advertising example, \\(k = 3\\) and \\[\\texttt{sales} = \\beta_0 + \\beta_1 \\times \\texttt{YouTube} + \\beta_2 \\times  \\texttt{Facebook} + \\beta_3 \\times \\texttt{Instagram} + \\epsilon\\]\n\nModel assumptions:\n\n\\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\)\n\n\n\n\n\nHow many parameters are there in the model?"
  },
  {
    "objectID": "slides/04-lin-reg.html#sample-mlr-model",
    "href": "slides/04-lin-reg.html#sample-mlr-model",
    "title": "Linear Regression üìà",
    "section": "Sample MLR Model",
    "text": "Sample MLR Model\n\nGiven the training sample \\((x_{11}, \\dots, x_{1k}, y_1), (x_{21}, \\dots, x_{2k}, y_2), \\dots, (x_{n1}, \\dots, x_{nk}, y_n),\\)\n\nThe sample MLR model to be trained: \\[\\begin{align*}\ny_i &= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_k x_{ik} + \\epsilon_i \\\\\n&= \\beta_0 + \\sum_{j=1}^k\\beta_j x_{ij} + \\epsilon_i, \\quad i = 1, 2, \\dots, n\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/04-lin-reg.html#regression-hyperplane",
    "href": "slides/04-lin-reg.html#regression-hyperplane",
    "title": "Linear Regression üìà",
    "section": "Regression Hyperplane",
    "text": "Regression Hyperplane\n\n\nSLR: regression line\nMLR: regression hyperplane or response surface\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_{2}x_2 + \\epsilon\\)\n\\(E(y \\mid x_1, x_2) = 50 + 10x_1 + 7x_2\\)"
  },
  {
    "objectID": "slides/04-lin-reg.html#response-surface",
    "href": "slides/04-lin-reg.html#response-surface",
    "title": "Linear Regression üìà",
    "section": "Response Surface",
    "text": "Response Surface\n\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_{2}x_2 + \\beta_{11}x_1^2 + \\beta_{22}x_2^2 + \\beta_{12}x_1x_2 + \\epsilon\\)\n\\(E(y \\mid x_1, x_2) = 800+10x_1+7x_2 -8.5x_1^2-5x_2^2- 4x_1x_2\\)\nüòé ü§ì A linear regression model can describe a complex nonlinear relationship between the response and predictors!"
  },
  {
    "objectID": "slides/04-lin-reg.html#ordinary-least-squares-estimation-of-the-coefficients",
    "href": "slides/04-lin-reg.html#ordinary-least-squares-estimation-of-the-coefficients",
    "title": "Linear Regression üìà",
    "section": "Ordinary Least Squares Estimation of the Coefficients",
    "text": "Ordinary Least Squares Estimation of the Coefficients\n\\[\\begin{align*}\ny_i &= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_k x_{ik} + \\epsilon_i \\\\\n&= \\beta_0 + \\sum_{j=1}^k\\beta_j x_{ij} + \\epsilon_i, \\quad i = 1, 2, \\dots, n\n\\end{align*}\\]\n\nThe least-squares function Sum of Squared Residuals (\\(SS_{res}\\))1 is \\[SS_{res}(\\alpha_0, \\alpha_1, \\dots, \\alpha_k) = \\sum_{i=1}^n\\left(y_i - \\alpha_0 - \\sum_{j=1}^k\\alpha_j x_{ij}\\right)^2\\]\n\n\n\\(SS_{res}\\) must be minimized with respect to the coefficients, i.e., \\[(b_0, b_1, \\dots, b_k) = \\underset{{\\alpha_0, \\alpha_1, \\dots, \\alpha_k}}{\\mathrm{arg \\, min}}  SS_{res}(\\alpha_0, \\alpha_1, \\dots, \\alpha_k)\\]\n\nIn ISL, RSS is used for Residual Sum of Squares."
  },
  {
    "objectID": "slides/04-lin-reg.html#geometry-of-least-squares-estimation",
    "href": "slides/04-lin-reg.html#geometry-of-least-squares-estimation",
    "title": "Linear Regression üìà",
    "section": "Geometry of Least Squares Estimation",
    "text": "Geometry of Least Squares Estimation"
  },
  {
    "objectID": "slides/04-lin-reg.html#least-squares-normal-equations",
    "href": "slides/04-lin-reg.html#least-squares-normal-equations",
    "title": "Linear Regression üìà",
    "section": "Least-squares Normal Equations",
    "text": "Least-squares Normal Equations\n\\[\\begin{align*}\n\\left.\\frac{\\partial SS_{res}}{\\partial\\alpha_0}\\right\\vert_{b_0, b_1, \\dots, b_k} &= -2 \\sum_{i=1}^n\\left(y_i - \\alpha_0 - \\sum_{j=1}^k \\alpha_j x_{ij}\\right) = 0\\\\\n\\left.\\frac{\\partial SS_{res}}{\\partial\\alpha_j}\\right\\vert_{b_0, b_1, \\dots, b_k} &= -2 \\sum_{i=1}^n\\left(y_i - \\alpha_0 - \\sum_{j=1}^k \\alpha_j x_{ij}\\right)x_{ij} = 0, \\quad j = 1, 2, \\dots, k\n\\end{align*}\\]\n\n\\(k + 1\\) equations with \\(k + 1\\) unknown parameters.\nThe ordinary least squares estimators are the solutions to the normal equations.\n\n\n\nüçπ üç∫ üç∏ ü•Ç I buy you a drink if you solve the equations by hand without using matrix notations or operations!"
  },
  {
    "objectID": "slides/04-lin-reg.html#interpreting-coefficients",
    "href": "slides/04-lin-reg.html#interpreting-coefficients",
    "title": "Linear Regression üìà",
    "section": "Interpreting Coefficients",
    "text": "Interpreting Coefficients\n\n\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    2.939      0.312   9.422     0.00\nyoutube        0.046      0.001  32.809     0.00\nfacebook       0.189      0.009  21.893     0.00\ninstagram     -0.001      0.006  -0.177     0.86\n\n\n\n\\[\\hat{y} = b_0 + b_1 x_1 + \\cdots + b_kx_k\\]\n\\[\\widehat{\\texttt{sales}} = 2.939 + 0.046 \\times \\texttt{YouTube} + 0.189 \\times  \\texttt{Facebook} - 0.001 \\times \\texttt{Instagram}\\]\n\n\n\\(b_1\\): Holding all other predictors fixed, for one unit increase of Youtube, the sales is expected to be increased, on average, by 0.046 units.\n\n\\(b_2\\): All else held constant, one unit increase of Facebook leads to, on average, 0.189 unit increase of sales.\n\n\\(b_0\\): The sales with no expenditures on Youtube, Facebook, and Instagram is expected to be 2.939. (Make sense?!)"
  },
  {
    "objectID": "slides/04-lin-reg.html#inference-on-coefficients",
    "href": "slides/04-lin-reg.html#inference-on-coefficients",
    "title": "Linear Regression üìà",
    "section": "Inference on Coefficients",
    "text": "Inference on Coefficients\n\n\n\n\n\nThe \\((1-\\alpha)100\\%\\) Wald confidence interval (CI) for \\(\\beta_j\\), \\(j = 0, 1, \\dots, k\\) is\n\n\\[\\left(b_j- t_{\\alpha/2, n-p}~se(b_j), \\quad b_j + t_{\\alpha/2, n-p}~ se(b_j)\\right)\\]\n\n\n\n            2.5 % 97.5 %\n(Intercept)  2.32   3.55\nyoutube      0.04   0.05\nfacebook     0.17   0.21\ninstagram   -0.01   0.01\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nThese are marginal CIs seperately for each \\(b_j\\).\nIndividual CI ignores the correlation between \\(b_j\\)s.\nBetter to consider elliptically-shaped regions."
  },
  {
    "objectID": "slides/04-lin-reg.html#collinearity",
    "href": "slides/04-lin-reg.html#collinearity",
    "title": "Linear Regression üìà",
    "section": "Collinearity",
    "text": "Collinearity\n\nInterpretation makes sense when predictors are not highly correlated.\n\nCorrelations amongst predictors cause problems (Collinearity).\n\n\nLarge variance of coefficients.\n\nLarge magnitude of coefficients.\n\nInstable and wrong signed coefficients.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThink about the standard error size. Do we tend to conclude \\(\\beta_j = 0\\) or not?"
  },
  {
    "objectID": "slides/04-lin-reg.html#extrapolation",
    "href": "slides/04-lin-reg.html#extrapolation",
    "title": "Linear Regression üìà",
    "section": "Extrapolation",
    "text": "Extrapolation\n\nRegression models are intended as interpolation equations over the range of the regressors used to fit the model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDo not use regression for time series forecasting!"
  },
  {
    "objectID": "slides/04-lin-reg.html#regression-is-not-for-causal-inference",
    "href": "slides/04-lin-reg.html#regression-is-not-for-causal-inference",
    "title": "Linear Regression üìà",
    "section": "Regression is Not for Causal Inference",
    "text": "Regression is Not for Causal Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClaims of causality should be avoided for observational data."
  },
  {
    "objectID": "slides/04-lin-reg.html#practical-significance-vs.-statisical-significance",
    "href": "slides/04-lin-reg.html#practical-significance-vs.-statisical-significance",
    "title": "Linear Regression üìà",
    "section": "Practical Significance vs.¬†Statisical Significance",
    "text": "Practical Significance vs.¬†Statisical Significance\n\n\n\n\n\n\nImportant\n\n\n\n\n\n\\(H_0:\\beta_j = 0\\) will always be rejected as long as the sample size is large enough, even \\(x_j\\) has a very small effect on \\(y\\).\n\nConsider the practical significance of the result, not just the statistical significance.\nUse confidence intervals to draw conclusions instead of relying only on p-values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nIf the sample size is small, there may not be enough evidence to reject \\(H_0:\\beta_j = 0\\).\n\n\nDo Not immediately conclude that the variable has no association with the response.\nThere may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association.\n\n\n\n\n\n\n\np-value is a dichotomous approach"
  },
  {
    "objectID": "slides/04-lin-reg.html#regression-model-in-matrix-form",
    "href": "slides/04-lin-reg.html#regression-model-in-matrix-form",
    "title": "Linear Regression üìà",
    "section": "Regression Model in Matrix Form",
    "text": "Regression Model in Matrix Form\n\\[y_i= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_kx_{ik} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2), \\quad i = 1, 2, \\dots, n\\]\n\n\n\\[{\\bf y} = {\\bf X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\] where\n\\[\\begin{align}\n\\bf y = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n \\end{bmatrix},\\quad\n\\bf X = \\begin{bmatrix}\n  1 & x_{11} & x_{12} & \\cdots & x_{1k} \\\\\n  1 & x_{21} & x_{22} & \\cdots & x_{2k} \\\\\n  \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  1 & x_{n1} & x_{n2} & \\cdots & x_{nk} \\end{bmatrix}, \\quad\n\\boldsymbol{\\beta} = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_k \\end{bmatrix} , \\quad\n\\boldsymbol{\\epsilon} = \\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\vdots \\\\\n\\epsilon_n\\end{bmatrix}\n\\end{align}\\]\n\n\n\\({\\bf X}_{n \\times p}\\): Design matrix\n\n\\(\\boldsymbol{\\epsilon} \\sim MVN_n({\\bf 0}, \\sigma^2 {\\bf I}_n)\\)1\n\n\n\nFor simplicity and convenience, \\(N({\\bf a}, {\\bf B})\\) represents a multivariate normal distribution with mean vector \\({\\bf a}\\) and covariance matrix \\({\\bf B}\\)."
  },
  {
    "objectID": "slides/04-lin-reg.html#least-squares-estimation-in-matrix-form",
    "href": "slides/04-lin-reg.html#least-squares-estimation-in-matrix-form",
    "title": "Linear Regression üìà",
    "section": "Least Squares Estimation in Matrix Form",
    "text": "Least Squares Estimation in Matrix Form\n\n\\[\\begin{align}\nS(\\boldsymbol{\\beta}) = \\sum_{i=1}^n\\epsilon_i^2 = \\boldsymbol{\\epsilon}'\\boldsymbol{\\epsilon} &= ({\\bf y} - {\\bf X} \\boldsymbol{\\beta})'({\\bf y} - {\\bf X} \\boldsymbol{\\beta}) \\\\\n&={\\bf y}'{\\bf y} - \\boldsymbol{\\beta}'{\\bf X}'{\\bf y} - {\\bf y}'{\\bf X} \\boldsymbol{\\beta} + \\boldsymbol{\\beta}' {\\bf X}' {\\bf X} \\boldsymbol{\\beta} \\\\\n&={\\bf y}'{\\bf y} - 2\\boldsymbol{\\beta}'{\\bf X}'{\\bf y} + \\boldsymbol{\\beta}' {\\bf X}' {\\bf X} \\boldsymbol{\\beta}\n\\end{align}\\]\n\n\nLet \\({\\bf t}\\) and \\({\\bf a}\\) be \\(n \\times 1\\) column vectors, and \\({\\bf A}_{n \\times n}\\) is a symmetric matrix.\n\n\\(\\frac{\\partial {\\bf t}'{\\bf a} }{\\partial {\\bf t}} = \\frac{\\partial {\\bf a}'{\\bf t} }{\\partial {\\bf t}} = {\\bf a}\\)\n\\(\\frac{\\partial {\\bf t}'{\\bf A} {\\bf t}}{\\partial {\\bf t}} = 2{\\bf A} {\\bf t}\\)\n\n\n\nNormal equations: \\(\\begin{align} \\left.\\frac{\\partial S}{\\partial \\boldsymbol{\\beta}}\\right \\vert_{\\bf b} = -2 {\\bf X}' {\\bf y} + 2 {\\bf X}' {\\bf X} {\\bf b} = \\boldsymbol{0} \\end{align}\\)\n\n\n\n\nLSE for \\(\\boldsymbol{\\beta}\\):  \\(\\begin{align} \\boxed{{\\bf b} = \\arg \\min _{\\boldsymbol{\\beta}} ({\\bf y} - {\\bf X} \\boldsymbol{\\beta})'({\\bf y} - {\\bf X} \\boldsymbol{\\beta}) = ({\\bf X}' {\\bf X})^{-1} {\\bf X}' \\bf y} \\end{align}\\)  provided that \\({\\bf X}' {\\bf X}\\) is full rank."
  },
  {
    "objectID": "slides/04-lin-reg.html#normal-equations",
    "href": "slides/04-lin-reg.html#normal-equations",
    "title": "Linear Regression üìà",
    "section": "Normal Equations",
    "text": "Normal Equations\n\\(({\\bf X}' {\\bf X}) {\\bf b} = {\\bf X}' {\\bf y}\\)"
  },
  {
    "objectID": "slides/04-lin-reg.html#hat-matrix",
    "href": "slides/04-lin-reg.html#hat-matrix",
    "title": "Linear Regression üìà",
    "section": "Hat Matrix",
    "text": "Hat Matrix\n\\[\\hat{\\bf y} = {\\bf X} {\\bf b} = {\\bf X} ({\\bf X}' {\\bf X}) ^{-1} {\\bf X}' {\\bf y} = {\\bf H} {\\bf y}\\]\n\nThe hat matrix \\({\\bf H}_{n \\times n} = {\\bf X} ({\\bf X}' {\\bf X}) ^{-1} {\\bf X}'\\)\nThe vector of residuals \\(e_i = y_i - \\hat{y}_i\\) is \\[{\\bf e} = {\\bf y} - \\hat{\\bf y} = {\\bf y} - {\\bf X} {\\bf b} = {\\bf y} - {\\bf H} {\\bf y} = ({\\bf I} - {\\bf H}) {\\bf y}\\]\n\n\n\n\nBoth \\({\\bf H}\\) and \\({\\bf I} - {\\bf H}\\) are symmetric and idempotent. They are projection matrices.\n\\(\\bf H\\) projects \\(\\bf y\\) to \\(\\hat{\\bf y}\\) on the \\((k+1)\\)-dimensional space spanned by columns of \\(\\bf X\\), or the column space of \\(\\bf X\\), \\(Col({\\bf X})\\).\n\\({\\bf I} - {\\bf H}\\) projects \\(\\bf y\\) to \\(\\bf e\\) on the space perpendicular to \\(Col({\\bf X})\\), or \\(Col({\\bf X})^{\\bot}\\)."
  },
  {
    "objectID": "slides/04-lin-reg.html#geometrical-interpretation-of-least-squares",
    "href": "slides/04-lin-reg.html#geometrical-interpretation-of-least-squares",
    "title": "Linear Regression üìà",
    "section": "Geometrical Interpretation of Least Squares",
    "text": "Geometrical Interpretation of Least Squares\n\n\n\n\\(Col({\\bf X}) = \\{ {\\bf Xb}: {\\bf b} \\in {\\bf R}^{k+1} \\}\\)\n\\({\\bf y} \\notin Col({\\bf X})\\)\n\\(\\hat{{\\bf y}} = {\\bf Xb} = {\\bf H} {\\bf y} \\in Col({\\bf X})\\)\n\\(\\small {\\bf e} = ({\\bf y} - \\hat{{\\bf y}}) = ({\\bf y} - {\\bf X b}) = ({\\bf I} - {\\bf H}) {\\bf y} \\perp Col({\\bf X})\\)\n\\({\\bf X}'{\\bf e} = 0\\)\n\n\nSearching for \\(\\bf b\\) that minimizes \\(SS_{res}\\) is equivalent to locating the point \\({\\bf Xb} \\in Col({\\bf X})\\) (C) that is as close to \\(\\bf y\\) (A) as possible!\n\n\n\n\n\n\nhttps://commons.wikimedia.org/wiki/File:OLS_geometric_interpretation.svg\n\n\n\n\n\n\nMinimize the distance of \\(\\color{red}{A}\\) to \\(Col(\\bf X)\\): Find the point in \\(Col(\\bf X)\\) that is closest to \\(A\\). The distance is minimized when the point in the space is the foot of the line from \\(A\\) normal to the space. This is point \\(C\\)."
  },
  {
    "objectID": "slides/04-lin-reg.html#multivariate-gaussiannormal-distribution",
    "href": "slides/04-lin-reg.html#multivariate-gaussiannormal-distribution",
    "title": "Linear Regression üìà",
    "section": "Multivariate Gaussian/Normal Distribution",
    "text": "Multivariate Gaussian/Normal Distribution\n\n\n\\({\\bf y} \\sim N_n(\\boldsymbol \\mu, {\\bf \\Sigma})\\)\n\n\n\\(\\boldsymbol \\mu\\): \\(n \\times 1\\) mean vector\n\n\\({\\bf \\Sigma}\\): \\(n \\times n\\) covariance matrix\n\n\n\n\\(\\boldsymbol \\mu= (2, 1)'\\); \\({\\bf \\Sigma} = \\begin{pmatrix} 2 & -1 \\\\ -1 & 2\\end{pmatrix}\\)\n\n\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "slides/04-lin-reg.html#sampling-distribution-of-bf-b",
    "href": "slides/04-lin-reg.html#sampling-distribution-of-bf-b",
    "title": "Linear Regression üìà",
    "section": "Sampling Distribution of \\({\\bf b}\\)\n",
    "text": "Sampling Distribution of \\({\\bf b}\\)\n\n\n\\({\\bf y} \\sim N(\\boldsymbol \\mu, {\\bf \\Sigma})\\), and \\({\\bf Z} = {\\bf By} + {\\bf c}\\) with a constant matrix \\({\\bf B}\\) and vector \\(\\bf c\\), then \\[{\\bf Z} \\sim N({\\bf B\\boldsymbol \\mu}, {\\bf B \\Sigma B}')\\]\n\n\\({\\bf b} = ({\\bf X}' {\\bf X}) ^{-1} {\\bf X}' \\bf y\\)\n\\[\\textbf{b} \\sim N \\left( \\boldsymbol \\beta, \\sigma^2 ( {\\bf X}' {\\bf X})^{-1}  \\right)\\] \\[E(\\textbf{b}) = E\\left[ ({\\bf X}' {\\bf X})^{-1} {\\bf X}' {\\bf y}\\right] = \\boldsymbol \\beta\\] \\[\\mathrm{Var}(\\textbf{b}) = \\mathrm{Var}\\left[({\\bf X}' {\\bf X})^{-1} {\\bf X}' {\\bf y} \\right] = \\sigma^2 ({\\bf X}' {\\bf X})^{-1}\\]\n\nThe unknown \\(\\sigma^2\\) is estimated by \\(\\hat{\\sigma}^2 = MS_{res} = \\frac{SS_{res}}{n - k - 1}\\).\n\\(\\textbf{b}\\) is Best Linear Unbiased Estimator (BLUE) (Gauss-Markov Theorem).\n\nThe standard error of \\(b_j\\) is \\({\\sqrt{s^2C_{jj}}}\\), where \\(C_{jj}\\) is the diagonal element of \\(({\\bf X'X})^{-1}\\) corresponding to \\(b_j\\)."
  },
  {
    "objectID": "slides/04-lin-reg.html#what-this-review-is-not-covered",
    "href": "slides/04-lin-reg.html#what-this-review-is-not-covered",
    "title": "Linear Regression üìà",
    "section": "What This Review is Not Covered",
    "text": "What This Review is Not Covered\n\nDetailed statistical inference\nRegression Diagnostics (Usual Data: Outliers, leverage points, influential points; Non-normality; Non-constant variance; Non-linearity)\nCategorical variables\nModel/Variable Selection\nCode for doing regression (lm() in R and linear_model.LinearRegression() in sklearn of Python)\nMaximum likelihood estimation\n\n\nWhere to learn these stuff?\n\nDr.¬†Yu‚Äôs MSSC 5780 slides https://math4780-f23.github.io/website/\nISL Ch 3, 6.1."
  },
  {
    "objectID": "slides/04-lin-reg.html#generalizations-of-the-linear-model",
    "href": "slides/04-lin-reg.html#generalizations-of-the-linear-model",
    "title": "Linear Regression üìà",
    "section": "Generalizations of the Linear Model",
    "text": "Generalizations of the Linear Model\n\nClassification: logistic regression, support vector machines\nNon-linearity: kernel smoothing, splines and generalized additive models, nearest neighbor methods.\nInteractions: Tree-based methods, bagging, random forests and boosting\nShrinkage and Regularization: Ridge regression and LASSO"
  },
  {
    "objectID": "slides/04-lin-reg.html#basic-concepts",
    "href": "slides/04-lin-reg.html#basic-concepts",
    "title": "Linear Regression üìà",
    "section": "Basic Concepts",
    "text": "Basic Concepts\n\nAlthough we have the closed form for \\({\\bf b} = ({\\bf X}' {\\bf X}) ^ {-1} {\\bf X}' {\\bf y}\\), we can solve for \\(\\bf b\\) numerically.\n\n\\[\\begin{align}\n\\underset{\\boldsymbol \\beta}{\\text{min}} \\quad \\ell(\\boldsymbol \\beta) = \\frac{1}{n} \\sum_i (y_i - \\beta_0 - x_i\\beta_1)^2 \\\\\n\\end{align}\\]\n\n\n\\(\\ell(\\boldsymbol \\beta)\\), the \\(\\text{MSE}_{\\texttt{Tr}}\\), is called the (squared) loss function.\n\n\n# generate data from a simple linear regression beta0 = 0.5, beta1 = 1\nset.seed(2025)\nn &lt;- 1000\nx &lt;- rnorm(n)\ny &lt;- 0.5 + x + rnorm(n)\n\n\nStart with an initial value of \\(\\boldsymbol \\beta\\), say \\(\\widehat{\\boldsymbol \\beta} = (0.3, 1.5)\\), we compute the \\(\\text{MSE}_{\\texttt{Tr}}\\)\n\n\n\\[\\begin{align}\n\\frac{1}{n}\\sum_i \\left( y_i - 0.3 - 1.5 x_i \\right)^2 \\\\\n\\end{align}\\]\nLet‚Äôs first generate a set of data. We have two parameters, an intercept \\(\\beta_= 0.5\\) and a slope \\(\\beta_1 = 1\\)."
  },
  {
    "objectID": "slides/04-lin-reg.html#basic-concepts-1",
    "href": "slides/04-lin-reg.html#basic-concepts-1",
    "title": "Linear Regression üìà",
    "section": "Basic Concepts",
    "text": "Basic Concepts\n\n# calculate the residual sum of squares for a grid of beta values\nmse &lt;- function(b, trainx, trainy) mean((trainy - b[1] - trainx * b[2]) ^ 2)\nmse(b = c(0.3, 1.5), trainx = x, trainy = y)\n\n[1] 1.354548\n\n\n\nThe initial point \\((0.3, 1.5)\\) (red) is not at the bottom of the surface.\n\n\n\n\n\n\n\nDoing this on all such Œ≤ values would allow us to create a surface of the RSS, as a function of the parameters. - Our goal is to minimize the \\(RR_{res}\\), knowing the corresponding \\(\\boldsymbol \\beta\\) values. Numerical optimization is a research field that investigates such problems and their properties."
  },
  {
    "objectID": "slides/04-lin-reg.html#optim",
    "href": "slides/04-lin-reg.html#optim",
    "title": "Linear Regression üìà",
    "section": "\noptim()1\n",
    "text": "optim()1\n\n\n(lm_optim &lt;- optim(par = c(0.3, 1.5), fn = mse, trainx = x, trainy = y))\n\n$par\n[1] 0.4571491 0.9723691\n\n$value\n[1] 1.055419\n\n$counts\nfunction gradient \n      53       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\n\nlm(y ~ x)$coef\n\n(Intercept)           x \n  0.4572794   0.9723262 \n\n\n\nThe par argument specifies an initial value. In this case, it is \\(\\beta_0 = \\beta_1 = 2\\).\nThe fn argument specifies the name of a function (mse in this case) that can calculate the objective function. This function may have multiple arguments. However, the first argument has to be the parameter(s) that is being optimized. In addition, the parameters need to be supplied to the function as a vector, but not matrix, list or other formats.\n\nThe arguments trainx = x, trainy = y specifies any additional arguments that the objective function fn (mse) needs. It behaves the same as if you are supplying this to the function mse it self.\n\n\n\n\nscipy.optimize for Python."
  },
  {
    "objectID": "slides/04-lin-reg.html#basic-principles",
    "href": "slides/04-lin-reg.html#basic-principles",
    "title": "Linear Regression üìà",
    "section": "Basic Principles",
    "text": "Basic Principles\nFor a general function \\(f(\\mathbf{x})\\) to be minimized with respect to (w.r.t.) \\(\\mathbf{x}\\in \\mathbf{R}^{p}\\), we have\n\n\nFirst-Order Necessary Condition:\n\n\nIf \\(f\\) is continuously differentiable in an open neighborhood of local minimum \\(\\mathbf{x}^\\ast\\), then \\(\\nabla f(\\mathbf{x}^\\ast) = \\mathbf{0}\\)."
  },
  {
    "objectID": "slides/04-lin-reg.html#taylor-expansion",
    "href": "slides/04-lin-reg.html#taylor-expansion",
    "title": "Linear Regression üìà",
    "section": "Taylor Expansion",
    "text": "Taylor Expansion\n\\[f(\\mathbf{x}^\\text{new}) \\approx f(\\mathbf{x}) + \\nabla f(\\mathbf{x})' (\\mathbf{x}^\\text{new} - \\mathbf{x})\\]\n\nWhether \\(\\nabla f(\\mathbf{x})\\) is positive or negative, we can always find a new point \\(\\mathbf{x}^\\text{new}\\) that makes \\(\\nabla f(\\mathbf{x})' (\\mathbf{x}^\\text{new} - \\mathbf{x})\\) less than 0, so that \\(f(\\mathbf{x}^\\text{new}) &lt; f(\\mathbf{x})\\).\n\n\n\n\n\\(\\nabla f(\\mathbf{x}^\\ast) = \\mathbf{0}\\) is only a necessary condition, not a sufficient condition.\n\\(x = 1\\) is a local minimizer, not a global one.\n\nOn the left hand side, we have a convex function, which looks like a bowl. The intuition is that, if \\(f(\\mathbf{x})\\) is a function that is smooth enough, and \\(\\mathbf{x}\\) is a point with \\(\\nabla f(\\mathbf{x}^\\ast) \\neq 0\\), then by the Taylor expansion, we have, for any new point \\(\\mathbf{x}^{new}\\) in the neighborhood of \\(\\mathbf{x}\\), we can approximate its function value - Since we only checked if the slope if ‚Äúflat‚Äù but didn‚Äôt care if its facing upward or downward, our condition cannot tell the difference."
  },
  {
    "objectID": "slides/04-lin-reg.html#second-order-property",
    "href": "slides/04-lin-reg.html#second-order-property",
    "title": "Linear Regression üìà",
    "section": "Second-order Property",
    "text": "Second-order Property\nSecond-order Sufficient Condition:\n\n\\(f\\) is twice continuously differentiable in an open neighborhood of \\(\\mathbf{x}^\\ast\\). If \\(\\nabla f(\\mathbf{x}^\\ast) = \\mathbf{0}\\) and \\(\\nabla^2 f(\\mathbf{x}^\\ast),\\) the Hessian matrix, is positive definite, i.e., \\[\n\\nabla^2 f(\\mathbf{x}) = \\left(\\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_i \\partial x_j}\\right) = \\mathbf{H}(\\mathbf{x}) \\succeq 0,\n\\] then \\(\\mathbf{x}^\\ast\\) is a strict local minimizer of \\(f\\).\n\n\n\n\\[\\begin{align}\n\\text{Left:}& f_1(x) = x ^ 2; \\qquad \\nabla^2 f_1(x) = 2 \\\\\n\\text{Right:}& f_2(x) = x ^ 4 + 2 x ^ 3 - 5 x ^ 2; \\qquad \\nabla^2 f_2(x) = 12x^2 + 12 x - 10\\\\\n\\end{align}\\]\n\nFor \\(f_1\\), \\(\\mathbf{H}(\\mathbf{x}) \\succeq 0\\), and the solution is a minimizer.\nFor \\(f_2\\), \\(\\nabla^2 f_2(-2.5) = 35\\), \\(\\nabla^2 f_2(0) = -10\\) and \\(\\nabla^2 f_2(1) = 14\\). So \\(x = -2.5\\) and \\(1\\) are local minimizers and \\(0\\) is a local maximizer.\n\n\n\n\\(\\mathbf{H}(\\mathbf{x})\\) is called the Hessian matrix, which will be frequently used in second-order methods. We can easily check this property for our examples: These conditions are sufficient, but again, they only discuss local properties, not global properties."
  },
  {
    "objectID": "slides/04-lin-reg.html#optimization-algorithm",
    "href": "slides/04-lin-reg.html#optimization-algorithm",
    "title": "Linear Regression üìà",
    "section": "Optimization Algorithm",
    "text": "Optimization Algorithm\n\n\n1. Start with \\(\\mathbf{x}^{(0)}\\)\n\nFor \\(i = 1, 2, \\dots\\) until convergence, find \\(\\mathbf{x}^{(i)}\\) s.t. \\(f(\\mathbf{x}^{(i)}) &lt; f(\\mathbf{x}^{(i-1)})\\)\n\n\n\n\n\nStopping criterion:\n\nGradient of the objective function: \\(\\lVert \\nabla f(\\mathbf{x}^{(i)}) \\rVert &lt; \\epsilon\\)\n\n(Relative) change of distance: \\(\\frac{\\lVert \\mathbf{x}^{(i)} - \\mathbf{x}^{(i-1)} \\rVert} {\\lVert \\mathbf{x}^{(i-1)}\\rVert}&lt; \\epsilon\\) or \\(\\lVert \\mathbf{x}^{(i)} - \\mathbf{x}^{(i-1)} \\rVert &lt; \\epsilon\\)\n\n(Relative) change of functional value: \\(\\frac{| f(\\mathbf{x}^{(i)}) - f(\\mathbf{x}^{(i-1)})|}{|f(\\mathbf{x}^{(i)})|} &lt; \\epsilon\\) or \\(| f(\\mathbf{x}^{(i)}) - f(\\mathbf{x}^{(i-1)})| &lt; \\epsilon\\)\n\nStop at a pre-specified number of iterations\n\n\n\n\n\nMost optimization algorithms follow the same idea: starting from a point x(0) (which is usually specified by the user) and move to a new point x(1) that improves the objective function value. Repeatedly performing this to get a sequence of points x(0),x(1),x(2),x(3),‚Ä¶ until the certain stopping criterion is reached. Most algorithms differ in terms of how to move from the current point x(k) to the next, better target point x(k+1). This may depend on the smoothness or structure of f, constrains on the domain, computational complexity, memory limitation, and many others."
  },
  {
    "objectID": "slides/04-lin-reg.html#bfgs-demo",
    "href": "slides/04-lin-reg.html#bfgs-demo",
    "title": "Linear Regression üìà",
    "section": "\nBFGS Demo",
    "text": "BFGS Demo\n\n\n\nf1 &lt;- function(x) x ^ 2\nf2 &lt;- function(x) {\n    x ^ 4 + 2 * x ^ 3 - 5 * x ^ 2\n}\noptim(par = 3, fn = f1, method = \"BFGS\")\n\n$par\n[1] -8.384004e-16\n\n$value\n[1] 1.75742e-29\n\n$counts\nfunction gradient \n       8        3 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\n\noptim(par = 10, fn = f2, method = \"BFGS\")\n\n$par\n[1] 0.9999996\n\n$value\n[1] -2\n\n$counts\nfunction gradient \n      37       12 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\n\noptim(par = 3, fn = f2, method = \"BFGS\")$par\n\n[1] -2.5"
  },
  {
    "objectID": "slides/04-lin-reg.html#second-order-newtons-method",
    "href": "slides/04-lin-reg.html#second-order-newtons-method",
    "title": "Linear Regression üìà",
    "section": "Second-order Newton‚Äôs Method\n",
    "text": "Second-order Newton‚Äôs Method\n\n\nSecond order Taylor expansion at a current point \\(\\mathbf{x}\\):\n\n\\[f(\\mathbf{x}^\\ast) \\approx f(\\mathbf{x}) + \\nabla f(\\mathbf{x})' (\\mathbf{x}^\\ast - \\mathbf{x}) + \\frac{1}{2} (\\mathbf{x}^\\ast - \\mathbf{x})' \\mathbf{H}(\\mathbf{x}) (\\mathbf{x}^\\ast - \\mathbf{x})\\]\n\nTake derivative w.r.t \\(\\mathbf{x}^\\ast\\) on both sides: \\[0 = \\nabla f(\\mathbf{x}^\\ast) = 0 + \\nabla f(\\mathbf{x}) + \\mathbf{H}(\\mathbf{x}) (\\mathbf{x}^\\ast - \\mathbf{x})\\] \\[\\boxed{\\mathbf{x}^{(i+1)} = \\mathbf{x}^{(i)} -  \\mathbf{H}(\\mathbf{x}^{(i)})^{-1} \\nabla f(\\mathbf{x}^{(i)})}\\]\nFor numerical stability, introduce a step size \\(\\delta \\in (0, 1)\\): \\[\\mathbf{x}^{(i+1)} = \\mathbf{x}^{(i)} -  {\\color{red}{\\delta}} \\, \\mathbf{H}(\\mathbf{x}^{(i)})^{-1} \\nabla f(\\mathbf{x}^{(i)})\\]\n\nIf \\(\\mathbf{H}\\) is difficult to compute, we may use some matrix to substitute it. For example, if we simplify use the identity matrix \\(\\mathbf{I}\\), then this reduces to a first-order method to be introduced later. However, if we can get a good approximation, we can still solve this linear system and get to a better point. Then the question is how to obtain such matrix in a computationally inexpensive way. The Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno (BFSG) algorithm is such an approach by iteratively updating its (inverse) estimation. For details, please see the SMLR book. We have already used the BFGS method previously in the optim() example. - when x(k+1) is not too far away from x(k), the quadratic approximation is fairly accurate."
  },
  {
    "objectID": "slides/04-lin-reg.html#first-order-gradient-descent",
    "href": "slides/04-lin-reg.html#first-order-gradient-descent",
    "title": "Linear Regression üìà",
    "section": "First-Order Gradient Descent\n",
    "text": "First-Order Gradient Descent\n\n\n\n\n\n\n\nNote\n\n\n\nWhen \\(\\mathbf{H}\\) or \\(\\mathbf{H}^{-1}\\) is difficult to compute (usually in deep learning)\n\nGet an approximate one in a computationally inexpensive way. The BFGS algorithm is such an approach by iteratively updating its (inverse) estimation.\nUse first-order methods\n\n\n\n\n\n\nWhen using \\(\\mathbf{H}= \\mathbf{I}\\), we update \\[\\mathbf{x}^{(i+1)} = \\mathbf{x}^{(i)} - \\delta \\nabla f(\\mathbf{x}^{(i)}).\\]\n\nIt is crucial to figure out a good step size \\(\\delta\\), usually \\(&lt;1\\).\n\nA too large \\(\\delta\\) may not converge.\nA too small \\(\\delta\\) takes too many iterations.\n\n\n\nAlternatively, line search could be used."
  },
  {
    "objectID": "slides/04-lin-reg.html#demo-ss_res-contour",
    "href": "slides/04-lin-reg.html#demo-ss_res-contour",
    "title": "Linear Regression üìà",
    "section": "\nDemo \\(SS_{res}\\) Contour",
    "text": "Demo \\(SS_{res}\\) Contour\nThe objective function is \\(\\ell(\\boldsymbol \\beta) = \\frac{1}{2n}||\\mathbf{y} - \\mathbf{X} \\boldsymbol \\beta ||^2\\) with solution \\({\\bf b} = \\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1} \\mathbf{X}'\\mathbf{y}\\).\n\nCodeset.seed(2025)\nn &lt;- 200\n\n# create some data with linear model\nX &lt;- MASS::mvrnorm(n, c(0, 0), matrix(c(1, 0.7, 0.7, 1), 2, 2))\ny &lt;- rnorm(n, mean = 2 * X[, 1] + X[, 2])\n  \nbeta1 &lt;- seq(-1, 4, 0.005)\nbeta2 &lt;- seq(-1, 4, 0.005)\nallbeta &lt;- data.matrix(expand.grid(beta1, beta2))\nrss &lt;- matrix(apply(allbeta, 1, \n                    function(b, X, y) sum((y - X %*% b) ^ 2), X, y),\n              length(beta1), length(beta2))\n  \n# quantile levels for drawing contour\nquanlvl = c(0.01, 0.025, 0.05, 0.2, 0.5, 0.75)\n  \n# plot the contour\ncontour(beta1, beta2, rss, levels = quantile(rss, quanlvl), las = 1)\nbox()\n  \n# the truth\nb &lt;- solve(t(X) %*% X) %*% t(X) %*% y\npoints(b[1], b[2], pch = 19, col = \"blue\", cex = 2)"
  },
  {
    "objectID": "slides/04-lin-reg.html#demo-gradient-descent",
    "href": "slides/04-lin-reg.html#demo-gradient-descent",
    "title": "Linear Regression üìà",
    "section": "\nDemo Gradient Descent",
    "text": "Demo Gradient Descent\n\\[\n\\begin{align}\n\\frac{\\partial \\ell(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta} = -\\frac{1}{n} \\sum_{i=1}^n (y_i - x_i' \\boldsymbol \\beta) x_i = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial \\ell_i(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta}.\n\\end{align}\n\\] where \\(\\ell_i(\\boldsymbol \\beta) = \\frac{1}{2}(y_i - x_i' \\boldsymbol \\beta)^2\\).\nFirst set an initial beta value, say \\(\\boldsymbol \\beta = \\mathbf{0}\\) for all entries, then proceed with the update\n\\[\\begin{align}\n\\boldsymbol \\beta^\\text{new} =& \\boldsymbol \\beta^\\text{old} - \\delta \\frac{\\partial \\ell(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta}\\\\\n=&\\boldsymbol \\beta^\\text{old} + \\delta \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i' \\boldsymbol \\beta) x_i.\\\\\n\\end{align}\\]"
  },
  {
    "objectID": "slides/04-lin-reg.html#demo-gradient-descent-1",
    "href": "slides/04-lin-reg.html#demo-gradient-descent-1",
    "title": "Linear Regression üìà",
    "section": "\nDemo Gradient Descent",
    "text": "Demo Gradient Descent\n\nSet \\(\\boldsymbol{\\beta}^{(0)} = \\mathbf{0}\\), \\(\\delta = 0.2\\)"
  },
  {
    "objectID": "slides/04-lin-reg.html#demo-effect-of-delta",
    "href": "slides/04-lin-reg.html#demo-effect-of-delta",
    "title": "Linear Regression üìà",
    "section": "\nDemo Effect of \\(\\delta\\)\n",
    "text": "Demo Effect of \\(\\delta\\)\n\n\nThe descent path is very smooth because we choose a very small step size. However, if the step size is too large, we may observe unstable results or even unable to converge. For example, if We set \\(\\delta = 1\\) or \\(\\delta = 1.5\\)."
  },
  {
    "objectID": "slides/04-lin-reg.html#mini-batch-stochastic-gradient-descent-sgd",
    "href": "slides/04-lin-reg.html#mini-batch-stochastic-gradient-descent-sgd",
    "title": "Linear Regression üìà",
    "section": "Mini-batch Stochastic Gradient Descent (SGD)",
    "text": "Mini-batch Stochastic Gradient Descent (SGD)\nIn deep learning,\n\nCalculating the gradient using entire data can be costly (memory limit).\nConsider update the parameter using a subset of data of size \\(m \\ll n\\), called minibatch: \\(\\mathcal{D}_B = \\{x_j, y_j \\}_{j = 1}^m \\subset \\mathcal{D}\\)\n\n\n\\[\n\\begin{align}\n\\frac{\\partial \\ell_B(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta} = -\\frac{1}{m} \\sum_{j=1}^m (y_j - x_j' \\boldsymbol \\beta) x_j = \\frac{1}{m} \\sum_{j=1}^m \\frac{\\partial \\ell_j(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta}\n\\end{align}\n\\] where \\(\\ell_j(\\boldsymbol \\beta) = \\frac{1}{2}(y_j - x_j' \\boldsymbol \\beta)^2\\).\n\\[\\begin{align}\n\\boldsymbol \\beta^\\text{new} =& \\boldsymbol \\beta^\\text{old} - \\delta \\frac{\\partial \\ell_B(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta}\\\\\n=&\\boldsymbol \\beta^\\text{old} + \\delta \\frac{1}{m} \\sum_{j=1}^m (y_j - x_j' \\boldsymbol \\beta) x_j.\\\\\n\\end{align}\\]"
  },
  {
    "objectID": "slides/04-lin-reg.html#sgd",
    "href": "slides/04-lin-reg.html#sgd",
    "title": "Linear Regression üìà",
    "section": "SGD",
    "text": "SGD\n\n\\(\\nabla_{\\boldsymbol \\beta} \\ell_B(\\boldsymbol \\beta)\\) approximates \\(\\nabla_{\\boldsymbol \\beta} \\ell(\\boldsymbol \\beta)\\) with stochasticity due to random sampling.\nDifferent samples give us a different gradient value, making it stochastic!\nSuppose we have \\(n=1000\\) training points, and batch size is \\(m=100\\), it needs \\(n/m = 10\\) updates (iterations can be parallelized) to pass all data into the algorithm, which completes one epoch.\n\nTo gaurantee convergence, the learning rate \\(\\delta\\) should\n\nbe decreasing (with epochs or iterations)\nhave \\(\\sum_{k=1}^{\\infty}\\delta_k = \\infty\\) and \\(\\sum_{k=1}^{\\infty}\\delta_k^2 &lt; \\infty\\)\n\n\n\n\nIterations: Decay happens more frequently (every batch). This is more precise and often used for large datasets. Epochs: Decay happens less frequently (after all batches of one epoch). This is simpler and often used for smaller datasets or in step-based schedules."
  },
  {
    "objectID": "slides/04-lin-reg.html#demo-sgd",
    "href": "slides/04-lin-reg.html#demo-sgd",
    "title": "Linear Regression üìà",
    "section": "\nDemo SGD",
    "text": "Demo SGD\n\n\n\n\n\n\n\n\n\n\n\n\nmssc6250-s25.github.io/website"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MSSC 6250 - Statistical Machine Learning (Spring 2025)",
    "section": "",
    "text": "Announcement:\n\nNo office hours first week.\n\n\nThis schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nTo Do\nSlides\nCode\nHomework\nProject\n\n\n\n\n1\nTue, Jan 14\nSyllabus/Overview of Statistical Learning\nüìñ\nüñ•Ô∏èüñ•Ô∏è\n\n\n\n\n\n\nThu, Jan 16\nBias-variance tradeoff\n\nüñ•Ô∏è\n\n\n\n\n\n2\nTue, Jan 21\nNO CLASS: Cold Weather\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Jan 25\nLinear Regression and Numerical Optimization\n\n\n\n\n\n\n\n3\nTue, Jan 28\nStochastic Gradient Descent; Ridge Regression\nüìñ\nüñ•Ô∏è\n\n‚úçÔ∏è\n\n\n\n\nThu, Feb 1\nCross-Validation\n\n\n\n\n\n\n\n4\nTue, Feb 4\nLASSO and Elastic Net\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Feb 8\nPolynomial Regression and Regression Splines\n\n\n\n‚úçÔ∏è\n\n\n\n5\nTue, Feb 11\nSmoothing Splines and Generalized Additive Models\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Feb 15\nBayesian Inference\n\n\n\n\n\n\n\n6\nTue, Feb 18\nBayesian Linear Regression\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Feb 22\nBinary Logistic Regression\n\n\n\n\n\n\n\n7\nTue, Feb 25\nMultinomial Logistic Regression\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Feb 29\nDiscriminant Analysis\n\n\n\n\n\n\n\n8\nTue, Mar 4\nNaive Bayes\nüìñ\n\n\n\n\n\n\n\nThu, Mar 7\nMidterm Presentation I\n\n\n\n‚úçÔ∏è\n‚úÖ\n\n\n9\nTue, Mar 11\nNO CLASS: Spring break\n\n\n\n\n\n\n\n\nThu, Mar 14\nNO CLASS: Spring break\n\n\n\n\n\n\n\n10\nTue, Mar 18\nK-Nearest Neighbors Regression\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Mar 21\nK-Nearest Neighbors Classification\n\n\n\n\n\n\n\n11\nTue, Mar 25\nGaussian Process Regression\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Mar 28\nGaussian Process Classification\n\n\n\n\n\n\n\n12\nTue, Apr 1\nSupport Vector Machine\nüìñ\n\n\n\n\n\n\n\nThu, Apr 4\nSupport Vector Machine\n\n\n\n\n\n\n\n13\nTue, Apr 8\nCART and Bagging\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Apr 11\nRandom Forests and Boosting\n\n\n\n‚úçÔ∏è\n\n\n\n14\nTue, Apr 15\nMidterm Presentation II\nüìñ\nüñ•Ô∏è\n\n\n‚úÖ\n\n\n\nThu, Apr 17\nNO CLASS: Easter break\n\n\n\n\n\n\n\n15\nTue, Apr 22\nPrincipal Component Analysis\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, Apr 25\nK-Means Clustering\n\n\n\n\n\n\n\n16\nTue, Apr 29\nModel-based Clustering\nüìñ\nüñ•Ô∏è\n\n\n\n\n\n\nThu, May 1\nNeural Networks\n\n\n\n\n\n\n\n17\nThu, May 8\nFinal Project Submission\n\n\n\n\n‚úÖ\n\n\n\n\nI reserve the right to make changes to the schedule.",
    "crumbs": [
      "Course Information",
      "Schedule"
    ]
  },
  {
    "objectID": "code/04-linear-reg-code.html",
    "href": "code/04-linear-reg-code.html",
    "title": "04-Linear Regression Code Demo",
    "section": "",
    "text": "Show/Hideadvertising_data &lt;- read.csv(\"../data/Advertising.csv\")\nadvertising_data &lt;- advertising_data[, 2:5]\nhead(advertising_data)\n\n     TV radio newspaper sales\n1 230.1  37.8      69.2  22.1\n2  44.5  39.3      45.1  10.4\n3  17.2  45.9      69.3   9.3\n4 151.5  41.3      58.5  18.5\n5 180.8  10.8      58.4  12.9\n6   8.7  48.9      75.0   7.2\n\nShow/Hidelm_out &lt;- lm(advertising_data$sales ~ ., data = advertising_data)\nsummary(lm_out)\n\n\nCall:\nlm(formula = advertising_data$sales ~ ., data = advertising_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\nShow/Hideconfint(lm_out)\n\n                  2.5 %     97.5 %\n(Intercept)  2.32376228 3.55401646\nTV           0.04301371 0.04851558\nradio        0.17154745 0.20551259\nnewspaper   -0.01261595 0.01054097"
  },
  {
    "objectID": "code/04-linear-reg-code.html#r-implementation",
    "href": "code/04-linear-reg-code.html#r-implementation",
    "title": "04-Linear Regression Code Demo",
    "section": "",
    "text": "Show/Hideadvertising_data &lt;- read.csv(\"../data/Advertising.csv\")\nadvertising_data &lt;- advertising_data[, 2:5]\nhead(advertising_data)\n\n     TV radio newspaper sales\n1 230.1  37.8      69.2  22.1\n2  44.5  39.3      45.1  10.4\n3  17.2  45.9      69.3   9.3\n4 151.5  41.3      58.5  18.5\n5 180.8  10.8      58.4  12.9\n6   8.7  48.9      75.0   7.2\n\nShow/Hidelm_out &lt;- lm(advertising_data$sales ~ ., data = advertising_data)\nsummary(lm_out)\n\n\nCall:\nlm(formula = advertising_data$sales ~ ., data = advertising_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.938889   0.311908   9.422   &lt;2e-16 ***\nTV           0.045765   0.001395  32.809   &lt;2e-16 ***\nradio        0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper   -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\nShow/Hideconfint(lm_out)\n\n                  2.5 %     97.5 %\n(Intercept)  2.32376228 3.55401646\nTV           0.04301371 0.04851558\nradio        0.17154745 0.20551259\nnewspaper   -0.01261595 0.01054097"
  },
  {
    "objectID": "code/04-linear-reg-code.html#python-implementation",
    "href": "code/04-linear-reg-code.html#python-implementation",
    "title": "04-Linear Regression Code Demo",
    "section": "Python implementation",
    "text": "Python implementation\n\nShow/Hideimport pandas as pd\nimport numpy as np\n\n\n\nShow/Hideadvertising_data = pd.read_csv(\"../data/Advertising.csv\")\nadvertising_data = advertising_data.iloc[:, 1:5]\nX = advertising_data.drop(columns=[\"sales\"])\ny = advertising_data[\"sales\"]\n\n\nscikit-learn\n\nShow/Hidefrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nreg.intercept_\n\n2.9388893694594014\n\nShow/Hidereg.coef_\n\narray([ 0.04576465,  0.18853002, -0.00103749])\n\n\nstatsmodels\n\nShow/Hidefrom statsmodels.formula.api import ols\nols_out = ols(formula='sales ~ TV + radio + newspaper', data=advertising_data).fit()\nols_out.params\n\nIntercept    2.938889\nTV           0.045765\nradio        0.188530\nnewspaper   -0.001037\ndtype: float64\n\nShow/Hideprint(ols_out.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  sales   R-squared:                       0.897\nModel:                            OLS   Adj. R-squared:                  0.896\nMethod:                 Least Squares   F-statistic:                     570.3\nDate:                Mon, 13 Jan 2025   Prob (F-statistic):           1.58e-96\nTime:                        11:56:31   Log-Likelihood:                -386.18\nNo. Observations:                 200   AIC:                             780.4\nDf Residuals:                     196   BIC:                             793.6\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.9389      0.312      9.422      0.000       2.324       3.554\nTV             0.0458      0.001     32.809      0.000       0.043       0.049\nradio          0.1885      0.009     21.893      0.000       0.172       0.206\nnewspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n==============================================================================\nOmnibus:                       60.414   Durbin-Watson:                   2.084\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\nSkew:                          -1.327   Prob(JB):                     1.44e-33\nKurtosis:                       6.332   Cond. No.                         454.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nShow/Hidecoef_summary = ols_out.summary2().tables[1]  # Get the coefficients table\nprint(coef_summary)\n\n              Coef.  Std.Err.          t         P&gt;|t|    [0.025    0.975]\nIntercept  2.938889  0.311908   9.422288  1.267295e-17  2.323762  3.554016\nTV         0.045765  0.001395  32.808624  1.509960e-81  0.043014  0.048516\nradio      0.188530  0.008611  21.893496  1.505339e-54  0.171547  0.205513\nnewspaper -0.001037  0.005871  -0.176715  8.599151e-01 -0.012616  0.010541\n\nShow/Hideconf_intervals = ols_out.conf_int()\nprint(conf_intervals)\n\n                  0         1\nIntercept  2.323762  3.554016\nTV         0.043014  0.048516\nradio      0.171547  0.205513\nnewspaper -0.012616  0.010541"
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Important\n\n\n\nHomework 1 released.",
    "crumbs": [
      "Weekly Materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#participate",
    "href": "weeks/week-2.html#participate",
    "title": "Week 2",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Linear Regression\nr fontawesome::fa(\"table\") Data Set - Advertising.csv",
    "crumbs": [
      "Weekly Materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#reading-and-resources",
    "href": "weeks/week-2.html#reading-and-resources",
    "title": "Week 2",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nüìñ ISL Ch 3.1 - 3.4\nüìñ Dr.¬†Yu MSSC 5780 Regression Analysis Week 1 to 6\nüìñ MML Ch 7.1, 8.1, 8.2.1, 8.2.2",
    "crumbs": [
      "Weekly Materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#perform",
    "href": "weeks/week-2.html#perform",
    "title": "Week 2",
    "section": "Perform",
    "text": "Perform\nüìã Homework 1\n\n\n\n\n\nBack to course schedule ‚èé",
    "crumbs": [
      "Weekly Materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "üìñ Read the syllabus.\nüìñ Get your laptop and computing environment ready!\nüìñ Refresh your probability/statistics and linear algebra.",
    "crumbs": [
      "Weekly Materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#prepare",
    "href": "weeks/week-1.html#prepare",
    "title": "Week 1",
    "section": "",
    "text": "üìñ Read the syllabus.\nüìñ Get your laptop and computing environment ready!\nüìñ Refresh your probability/statistics and linear algebra.",
    "crumbs": [
      "Weekly Materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#participate",
    "href": "weeks/week-1.html#participate",
    "title": "Week 1",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Welcome to MSSC 6250\nüñ•Ô∏è Slides - Overview of Machine Learning\nüñ•Ô∏è Slides - Bias-variance Tradeoff",
    "crumbs": [
      "Weekly Materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#reading-and-resources",
    "href": "weeks/week-1.html#reading-and-resources",
    "title": "Week 1",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nüìñ ISL Ch 1, 2\nüìñ Dr.¬†Yu MSSC 5780 Probability and Statistics Review\nüìñ Dr.¬†Yu MSSC 5780 Linear Algebra Review\nüìñ Check any math you need from MML Ch 2 - 6",
    "crumbs": [
      "Weekly Materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#perform",
    "href": "weeks/week-1.html#perform",
    "title": "Week 1",
    "section": "Perform",
    "text": "Perform\nüìã Decide whether this is the right course for you by the drop deadline 1/21.\n\n\n\n\nBack to course schedule ‚èé",
    "crumbs": [
      "Weekly Materials",
      "Week 1"
    ]
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (‚ÄúPublic License‚Äù). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 ‚Äì Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter‚Äôs License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 ‚Äì Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor ‚Äì Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor ‚Äì Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter‚Äôs License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 ‚Äì License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter‚Äôs License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter‚Äôs License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter‚Äôs License You apply.\n\n\n\nSection 4 ‚Äì Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 ‚Äì Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 ‚Äì Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 ‚Äì Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 ‚Äì Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the ‚ÄúLicensor.‚Äù The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark ‚ÄúCreative Commons‚Äù or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Important\n\n\n\nHomework 1 due Friday, Feb 9, 11:59 PM",
    "crumbs": [
      "Weekly Materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#participate",
    "href": "weeks/week-4.html#participate",
    "title": "Week 4",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Lasso\n\nr fontawesome::fa(\"table\") Data Set - Prostate.csv",
    "crumbs": [
      "Weekly Materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#reading-and-resources",
    "href": "weeks/week-4.html#reading-and-resources",
    "title": "Week 4",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nüìñ ISL Ch 6.1, 6.2",
    "crumbs": [
      "Weekly Materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#perform",
    "href": "weeks/week-4.html#perform",
    "title": "Week 4",
    "section": "Perform",
    "text": "Perform\n\n\n\n\nBack to course schedule ‚èé",
    "crumbs": [
      "Weekly Materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\nHomework 1 due Friday, Feb 7, 11:59 PM",
    "crumbs": [
      "Weekly Materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#participate",
    "href": "weeks/week-3.html#participate",
    "title": "Week 3",
    "section": "Participate",
    "text": "Participate\nüñ•Ô∏è Slides - Ridge Regression and Cross Validation\nr fontawesome::fa(\"table\") Data Set - mtcars.csv",
    "crumbs": [
      "Weekly Materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#reading-and-resources",
    "href": "weeks/week-3.html#reading-and-resources",
    "title": "Week 3",
    "section": "Reading and Resources",
    "text": "Reading and Resources\nüìñ ISL Ch 5.1, 6.2.1\nüìñ MML Ch 8.2.3 - 8.2.4",
    "crumbs": [
      "Weekly Materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#exercise",
    "href": "weeks/week-3.html#exercise",
    "title": "Week 3",
    "section": "Exercise",
    "text": "Exercise\n\n\n\n\n\nBack to course schedule ‚èé",
    "crumbs": [
      "Weekly Materials",
      "Week 3"
    ]
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "MSSC 6250 - Statistical Machine Learning (Spring 2025)",
    "section": "",
    "text": "The course discuss machine learning from statistical and modeling points of view, covering supervised learning and unsupervised learning models and algorithms. Supervised learning methods include various regression and classification methods, and unsupervised learning methods involves dimension reduction and clustering techniques. Topics include Bayesian linear regression, shrinkage and regularization, regression splines, Gaussian processes, logistic regression, discriminant analysis, nearest neighbors, tree-based methods, principal components, K-means, Gaussian mixture clustering, neural networks, etc.",
    "crumbs": [
      "Course Information",
      "Overview"
    ]
  },
  {
    "objectID": "course-news.html",
    "href": "course-news.html",
    "title": "News/Announcements",
    "section": "",
    "text": "Any announcement will be posted in this page. The latest news will also be put on top of the main page.",
    "crumbs": [
      "Course Information",
      "News/Annoucements"
    ]
  },
  {
    "objectID": "course-news.html#jan-14-2025",
    "href": "course-news.html#jan-14-2025",
    "title": "News/Announcements",
    "section": "Jan 14, 2025",
    "text": "Jan 14, 2025\n\nNo office hours first week.",
    "crumbs": [
      "Course Information",
      "News/Annoucements"
    ]
  },
  {
    "objectID": "slides/03-bias-var.html#supervised-learning-1",
    "href": "slides/03-bias-var.html#supervised-learning-1",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Supervised Learning",
    "text": "Supervised Learning\nSupervised learning investigates and models the relationships between responses and inputs."
  },
  {
    "objectID": "slides/03-bias-var.html#relationship-as-functions",
    "href": "slides/03-bias-var.html#relationship-as-functions",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Relationship as Functions",
    "text": "Relationship as Functions\n\nRepresent relationships between variables using functions y = f(x).\n\nPlug in the inputs and receive the output.\n\ny = f(x) = 3x + 7 is a function with input x and output y.\nIf x = 5, y = 3 \\times 5 + 7 = 22.\n\n\n\n\n\nIn mathematics, how do we describe a Relationship Between Variables? We use a function. Right.\nThe function y = f(x) gives us the relationship between an output Y and one or more inputs x.\n\nYou plug in the values of inputs and receive back the output value.\nFor example, the formula y = f(x) = 3x + 7 is a function with input x and output y. If x = 5, y = 3 \\times 5 + 7 = 22.\n\n\nBecause this is a linear function, we know that x and y are linearly related.\nWith a value of x, I can give you 100% correct value of y, which is right on this straight line. right. In other words, their relationship is 100% deterministic."
  },
  {
    "objectID": "slides/03-bias-var.html#different-relationships",
    "href": "slides/03-bias-var.html#different-relationships",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Different Relationships",
    "text": "Different Relationships\n\n\n\nCan you come up with any real-world examples describing relationships between variables deterministically?\n\n\nThe relationship between x and y can be more than linear.\nThe relationship can be also quadratic, cubic or any other possible relationship."
  },
  {
    "objectID": "slides/03-bias-var.html#different-relationships-1",
    "href": "slides/03-bias-var.html#different-relationships-1",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Different Relationships",
    "text": "Different Relationships\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: https://www.ck12.org/physics/acceleration-due-to-gravity/lesson/Acceleration-Due-to-Gravity-MS-PS/\n\n\n\n\n\n\nHere I give your two examples. The first example is the conversion of F and C degrees. Their relationship is linear and F = 32 + 1.8 C.\nSo you give me a C degree, I can tell you its corresponding F degree fro sure. Right.\nThe second example comes from physics. the displacement of an object is a quadratic function of time.\nSo here s(t) = v0 * t + 0.5 * a * t^2. v0 is the initial velocity, and a is acceleration, and t is time.\nAgain the relationship between displacement and time is 100% deterministic.\nA value of time corresponds to an unique value of displacement given v0 and a."
  },
  {
    "objectID": "slides/03-bias-var.html#relationship-between-variables-is-not-perfect",
    "href": "slides/03-bias-var.html#relationship-between-variables-is-not-perfect",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Relationship between Variables is Not Perfect",
    "text": "Relationship between Variables is Not Perfect\n\n\n\nCan you provide some real examples that the variables are related each other, but not perfectly related?"
  },
  {
    "objectID": "slides/03-bias-var.html#relationship-between-variables-is-not-perfect-1",
    "href": "slides/03-bias-var.html#relationship-between-variables-is-not-perfect-1",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Relationship between Variables is Not Perfect",
    "text": "Relationship between Variables is Not Perfect\n\n\nüíµ In general, one with more years of education earns more.\nüíµ Any two with the same years of education may have different annual income.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere I give you a simple example: the relationship between income and years of education.\nüíµ In general, one with more years of education earns more.\nüíµ Any two with the same years of education may have different annual income.\nBecause your income level depends on so many other factors, not just years of education.\nSo when you plot the scatter plot of the two variables, you will find that there is some trend, but the data are sort of scattered or jittered or variated around some function that describes the relationship between income the years of education.\n\nRed dots are observed values or the years of education and income pairs."
  },
  {
    "objectID": "slides/03-bias-var.html#variation-around-the-functionmodel",
    "href": "slides/03-bias-var.html#variation-around-the-functionmodel",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Variation around the Function/Model",
    "text": "Variation around the Function/Model\n\n\nWhat are the unexplained variation coming from?\n\n\n\n\n\n\nOther factors accounting for parts of variability of income.\n\nAdding more explanatory variables to a model can reduce the variation size around the model.\n\n\nPure measurement error.\nJust that randomness plays a big role. ü§î\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat other factors (variables) may affect a person‚Äôs income?\n\n\n\nyour income = f(years of education, major, GPA, college, parent's wealth, ...)\n\nAnd the data Variation around the Function, or in general the regression model is just as important as the model, if not more!\n\nBasically, what statistics does is explain variation in the context of what remains unexplained.\nThe scatter plot suggests that there might be other factors that account for large parts of variability.\nIf that is the case, adding more explanatory variables ( Xs ) to a model can sometimes usefully reduce the size of the scatter around the model.\nPerhaps just that randomness plays a big role."
  },
  {
    "objectID": "slides/03-bias-var.html#supervised-learning-mapping",
    "href": "slides/03-bias-var.html#supervised-learning-mapping",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Supervised Learning Mapping",
    "text": "Supervised Learning Mapping\n\n\n\nExplain the relationship between X and Y and make predictions through a model Y = f(X) + \\epsilon\n\n\n\\epsilon: irreducible random error (Aleatoric Uncertainty)\n\nindependent of X\n\n\nmean zero with some variance.\n\n\n\nf(\\cdot): unknown function1 describing the relationship between X and (the mean of) Y. (Epistemic Uncertainty)\n\n\n\n\nIn Intro Stats, what is the form of f and what assumptions you made on the random error \\epsilon ?\n\n\n\n\n\nf(X) = \\beta_0 + \\beta_1X with unknown parameters \\beta_0 and \\beta_1.\n\n\\epsilon \\sim N(0, \\sigma^2).\n\n\nOK. Now after collecting the data of the variables we are interested, we know their relationship, most of the time, is not perfect, and stochastic in some way and in some sense.\nAnd how do we model such stochastic relationship? Well the answer is a regression model.\nSuppose we are interested in the relationship between two variables, call X and Y. In particular, we like to know how changes of X affect value of Y, or we want to use X to predict Y.\nIn this sense, Y is called response, outcome, label, dependent variable, e.g., income\n\n\nX is called predictor, covariate, feature, regressor, explanatory or independent variable, e.g., years of education, which is known and fixed.\nExplain the relationship between X and Y and make predictions through a model Y = f(X) + \\epsilon. This is a very general regression model we can built to learn the relationship b/w x and y.\n\nf(\\cdot) is fixed but unknown and describes the true relationship between X and Y. \n\n\n\\epsilon is a irreducible random error which is assumed to be independent of X and has mean zero with some variance.\n\n\\epsilon is used to represent those measurement errors or the variation that cannot be explained or captured by the predictor X.\nIntro Stats:\n\n\nf(X) = \\beta_0 + \\beta_1X with unknown parameters \\beta_0 and \\beta_1.\n\n\\epsilon \\sim N(0, \\sigma^2).\n\n\n\nX and Y are assumed to be linearly related, which may not be correct.\nNext week, we will learn simple linear regression from the scratch and in much more detail. Here I just give you an overview.\n\n\nf(\\cdot) is assumed fixed from frequentist point of view; f(\\cdot) is random in the Bayesian framework."
  },
  {
    "objectID": "slides/03-bias-var.html#true-unknown-function-f-of-the-model-y-fx-epsilon",
    "href": "slides/03-bias-var.html#true-unknown-function-f-of-the-model-y-fx-epsilon",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "True Unknown Function f of the Model Y = f(X) + \\epsilon\n",
    "text": "True Unknown Function f of the Model Y = f(X) + \\epsilon\n\n\n\n\nBlue curve: true underlying relationship between (the mean) income and years of education.\n\nBlack lines: error associated with each observation\n\n\n\nBig problem: f(x) is unknown and needs to be estimated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs go back to the income-education example. Red dots are observed values or the years of education and income pairs. Suppose we like to use a regression model y = f(x) + \\epsilon to describe the relationship between income and education.\nAnd the *Blue** curve on the right shows the true underlying relationship between income and years of education, which is the function f in our regression model.\nAnd each Black vertical line indicates an error associated with each observation.\nSo again, each red dot or observation is the value of the function f(x) plus some random error with its magnitude shown in a black vertical line.\nAgain, in regression, we assume years of education is fixed. It is income level that varies around the function f."
  },
  {
    "objectID": "slides/03-bias-var.html#how-to-estimate-f",
    "href": "slides/03-bias-var.html#how-to-estimate-f",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "How to Estimate f?",
    "text": "How to Estimate f?\n\nUse training data \\mathcal{D} = \\{ (x_i, y_i) \\}_{i=1}^n (E) to train or teach our model to learn f (T).\nUse test data \\mathcal{D}_{test} = \\{ (x_j, y_j) \\}_{j=1}^m (E) to test or evaluate how well the model makes inference or prediction (P).\n\n\n\nModels are either parametric or nonparametric.\n\n\n\n\n\nParametric methods involve a two-step model-based approach:\n\n1Ô∏è‚É£ Make an assumption about the shape of f, e.g.¬†linear regression  f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p \n\n2Ô∏è‚É£ Use \\mathcal{D} to train the model, e.g., learn the parameters \\beta_j, j = 0, \\dots, p using least squares.\n\n\n\n\n\n\n\nNonparametric methods do not make assumptions about the shape of f.\n\nSeek an estimate of f that gets close to the data points without being too rough or wiggly."
  },
  {
    "objectID": "slides/03-bias-var.html#parametric-vs.-nonparametric-models",
    "href": "slides/03-bias-var.html#parametric-vs.-nonparametric-models",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Parametric vs.¬†Nonparametric Models",
    "text": "Parametric vs.¬†Nonparametric Models\n\n\n\nParametric (Linear regression)\n\n\n\n\n\n\n\n\n\n\n\nNonparametric (LOESS)"
  },
  {
    "objectID": "slides/03-bias-var.html#no-free-lunch",
    "href": "slides/03-bias-var.html#no-free-lunch",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "No Free Lunch",
    "text": "No Free Lunch\n\n\nThere is no free lunch in machine learning: no one method dominates all others over all possible data sets.\n\n\n\nAll models are wrong, but some are useful. ‚Äì George Box (1919-2013)\n\n\n\nFor any given training data, decide which method (model & algorithm) produces the best results.\nSelecting the best approach is one of the most challenging parts of machine learning.\nNeed some way to measure how well its predictions actually match the training/test data.\n\n\n\n\nFor numeric y: mean square error (MSE) for y with \\hat{f}, the estimate of f: \\text{MSE}_{\\texttt{Tr}}(y) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2, \\quad \\quad \\text{MSE}_{\\texttt{Te}}(y) = \\frac{1}{m} \\sum_{j=1}^m (y_j - \\hat{f}(x_j))^2\n\n\n\n\n\nAre \\text{MSE}_{\\texttt{Tr}} and \\text{MSE}_{\\texttt{Te}} the same? When to use which?"
  },
  {
    "objectID": "slides/03-bias-var.html#mean-square-error",
    "href": "slides/03-bias-var.html#mean-square-error",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Mean Square Error",
    "text": "Mean Square Error\n\n\n\\text{MSE}_{\\texttt{Tr}} measures how much \\hat{f}(x_i) is close to the training data y_i (goodness of fit). However, most of the time\n\n\n\n\nWe do not care how well the method works on the training data.\n\n\n\n\nWe are interested in the predictive accuracy when we apply our method to previously unseen test data.\n\n\nWe want to know whether \\hat{f}(x_j) is (approximately) equal to y_j, where (x_j, y_j) is previously unseen or a test data point not used in training our model.\n\n\n\n\n\\text{MSE}_{\\texttt{Tr}} or \\text{MSE}_{\\texttt{Te}} is smaller?\n\n\n\n\n\n\\text{MSE}_{\\texttt{Tr}} &lt; \\text{MSE}_{\\texttt{Te}}.\n\n\nTO DO: Provide an EXAMPLE of why training error is smaller\nMost statistical learning methods either directly or indirectly seek to minimize the training MSE.\nTraining data are the information we have and probably only have. Without other constraints or information about how we train the model, we tend to make use of all possible information in the training data to train our model.\nWe want to know whether \\hat{f}(x_j) is (approximately) equal to y_j, where (x_j, y_j) is a previously unseen test observation not used to train the model."
  },
  {
    "objectID": "slides/03-bias-var.html#model-complexityflexibility",
    "href": "slides/03-bias-var.html#model-complexityflexibility",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Model Complexity/Flexibility",
    "text": "Model Complexity/Flexibility\n\nA more complex model produces a more flexible or wiggly regression curve \\hat{f}(x) that matches the training data better.\n\ny = \\beta_0+ \\beta_1x + \\beta_2x^2 + \\cdots + \\beta_{10}x^{10} + \\epsilon is more complex than y = \\beta_0+ \\beta_1x + \\epsilon\n\n\n\n\n\nOverfitting: A too complex model fits the training data extremely well and too hard, picking up some patterns and variations simply caused by random noises that are not the properties of the true f, and not existed in the any unseen test data.  \n\n\n\n\nUnderfitting: A model that is too simple to capture complex patterns or shapes of the true f(x). The estimate \\hat{f}(x) is rigid and far away from data."
  },
  {
    "objectID": "slides/03-bias-var.html#section-2",
    "href": "slides/03-bias-var.html#section-2",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "",
    "text": "How \\text{MSE}_{\\texttt{Tr}} and \\text{MSE}_{\\texttt{Te}} change with model complexity?"
  },
  {
    "objectID": "slides/03-bias-var.html#model-complexityflexibility-and-mse",
    "href": "slides/03-bias-var.html#model-complexityflexibility-and-mse",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Model Complexity/Flexibility and MSE",
    "text": "Model Complexity/Flexibility and MSE\n\nIt‚Äôs common that no test data are available. Can we select a model that minimize \\text{MSE}_{\\texttt{Tr}}, since the training data and test data appear to be closed related?\n\n\n\n\n\nOrange: Under fit\nGreen: Over fit\nBlue: Best fit\n\\color{darkgray}{\\text{MSE}_{\\texttt{Tr}}} is decreasing with the complexity.\n\\color{red}{\\text{MSE}_{\\texttt{Te}}} is U-shaped: goes down then up with the complexity.\n\n\n\nMSE\nOverfit\nUnderfit\n\n\n\nTrain\ntiny\nbig\n\n\nTest\nbig\nbig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo! There is no guarantee that the method with the lowest \\text{MSE}_{\\texttt{Tr}} will also have the lowest \\text{MSE}_{\\texttt{Te}}."
  },
  {
    "objectID": "slides/03-bias-var.html#bias-variance-tradeoff",
    "href": "slides/03-bias-var.html#bias-variance-tradeoff",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\nGiven any new input x_0,\n\n\\text{MSE}_{\\hat{f}} = E\\left[\\left(\\hat{f}(x_0) - f(x_0)\\right)^2\\right] = \\left[\\text{Bias}\\left(\\hat{f}(x_0) \\right)\\right]^2 + \\text{Var}\\left(\\hat{f}(x_0)\\right)\nwhere \\text{Bias}\\left(\\hat{f}(x_0) \\right) = E\\left[ \\hat{f}(x_0)\\right] - f(x_0).\n\n\n\n\n\n\n\nWarning\n\n\n\\hat{f}(x_0) is a random variable! Why?\n\n\n\n\n\nThe expected test MSE of y_0 at x_0 is \\text{MSE}_{y_0} = E\\left[\\left(y_0 - \\hat{f}(x_0)\\right)^2\\right] = \\text{MSE}_{\\hat{f}} + \\text{Var}(\\epsilon)\n\n\n\n\n\n\n\nNote\n\n\n\nWe never know the true expected test MSE, and prefer the model with the smallest expected test MSE estimate.\n\n\n\n\n\nMSE can be decomposed into two effects/measures\nMSE is a combination of two performance measures."
  },
  {
    "objectID": "slides/03-bias-var.html#bias-variance-tradeoff-and-model-complexity",
    "href": "slides/03-bias-var.html#bias-variance-tradeoff-and-model-complexity",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "Bias-Variance Tradeoff and Model Complexity",
    "text": "Bias-Variance Tradeoff and Model Complexity\n\\text{MSE}_{\\hat{f}} = \\left[\\text{Bias}\\left(\\hat{f}(x_0) \\right)\\right]^2 + \\text{Var}\\left(\\hat{f}(x_0)\\right)\n\nOverfitting: Low bias and High variance\nUnderfitting: High bias and Low variance"
  },
  {
    "objectID": "slides/03-bias-var.html#lab-bias-variance-tradeoff",
    "href": "slides/03-bias-var.html#lab-bias-variance-tradeoff",
    "title": "Bias-Variance Tradeoff üéØ",
    "section": "\nLab: Bias-Variance Tradeoff",
    "text": "Lab: Bias-Variance Tradeoff\n\nModel 1: Under-fitting y = \\beta_0+\\beta_1x+\\epsilon\nModel 2: Right-fitting y = \\beta_0+\\beta_1x+ \\beta_2x^2 + \\epsilon\nModel 3: Over-fitting y = \\beta_0+\\beta_1x+ \\beta_2x^2 + \\cdots + \\beta_9x^9 + \\epsilon\nTo see expectation/bias and variance, we need replicates of training data."
  },
  {
    "objectID": "slides/02-overview.html#imageobject-recognition",
    "href": "slides/02-overview.html#imageobject-recognition",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Image/Object Recognition",
    "text": "Image/Object Recognition\n\n\n\n\n\n\nMachine learning is everywhere.\nCan we train a machine to have an ability to recognize dogs and cats?\nhundreds of thousands of objects?"
  },
  {
    "objectID": "slides/02-overview.html#recommender-system",
    "href": "slides/02-overview.html#recommender-system",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Recommender System",
    "text": "Recommender System\n\n\nwatch one video on YouTube, then lots of other videos that never show up pop up.\nCheck some items on Amazon, then it recommends some other items saying ‚Äúyou may also like XXX‚Äù\nThis kind of recommendation relies on machine learning.\nHow does the model know you may also like those items. Well the model learn this from other customer surfing and purchasing history."
  },
  {
    "objectID": "slides/02-overview.html#covid-detection",
    "href": "slides/02-overview.html#covid-detection",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "COVID Detection",
    "text": "COVID Detection"
  },
  {
    "objectID": "slides/02-overview.html#stock-price-forecasting",
    "href": "slides/02-overview.html#stock-price-forecasting",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Stock Price Forecasting",
    "text": "Stock Price Forecasting"
  },
  {
    "objectID": "slides/02-overview.html#risk-factors-for-cancer",
    "href": "slides/02-overview.html#risk-factors-for-cancer",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Risk Factors for Cancer",
    "text": "Risk Factors for Cancer"
  },
  {
    "objectID": "slides/02-overview.html#how-we-solve-real-life-problems",
    "href": "slides/02-overview.html#how-we-solve-real-life-problems",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "How We Solve Real-Life Problems",
    "text": "How We Solve Real-Life Problems\n\nWe describe and formulate our problems/questions by models.\n\n\n\nSolve problems/Answer questions by doing inference and/or predictions from the built model using the information from the data and computer algorithms.\n\n\n\n\nSo what is (Statistical) Machine Learning?\n\n\nMathematical/statistical or machine learning models.\nwe know the eating habits from a data of 100 american people. How do we know the the eating habits of all the people in the united states.\nIf we know someone‚Äôs GPA and major, how can we predict his salary after graduation?"
  },
  {
    "objectID": "slides/02-overview.html#machine-learning-vs.-statistical-learning",
    "href": "slides/02-overview.html#machine-learning-vs.-statistical-learning",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Machine Learning vs.¬†Statistical Learning",
    "text": "Machine Learning vs.¬†Statistical Learning\n\n\nMachine learning (ML) is a field in Computer Science (CS).\n\n\n‚ÄúA computer program is said to learn from experience (E) with respect to some class of tasks (T) and performance measure (P) if its performance at tasks in T, as measured by P, improves with experience E.‚Äù ‚Äì Tom Mitchell, Professor of ML at Carnegie Mellon University\n\n\ncomputer programs = computing software and system\nexperience = data (objective and/or subjective)\ntasks = problems being solved by the computing system\n\n\n\nThe ML algorithms are mainly for predictive modeling problems, and many have been borrowed from Statistics, for example, linear regression.\nML is a CS perspective on modeling data with a focus on algorithmic methods.\n\n\n\n\nWait, I am a statistical modeler (computational statistician), and this is exactly what I am doing!\n\nMachine Learning is the study of computer algorithms that improve automatically through experience."
  },
  {
    "objectID": "slides/02-overview.html#machine-learning-vs.-statistical-learning-1",
    "href": "slides/02-overview.html#machine-learning-vs.-statistical-learning-1",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Machine Learning vs.¬†Statistical Learning",
    "text": "Machine Learning vs.¬†Statistical Learning\n\n\nStatistical learning, used to be called Applied Statistics (what a ‚Äúfancy‚Äù name, ü§£), arose as a subfield of Statistics.\n\n\n\nStatistical learning refers to a set of tools for modeling and understanding complex datasets. ‚Äì An Introduction to Statistical Learning\n\n\n‚Ä¶to extract important patterns and trends, and understand ‚Äúwhat the data says.‚Äù We call this learning from data. ‚Äì The Elements of Statistical Learning\n\n\ntools = mathematics, computing hardware/software/architecture, programming languages, algorithms, etc.\nStatistical learning is a mathematical perspective on modeling data with a focus on goodness of fit.\n\nhttps://machinelearningmastery.com/relationship-between-applied-statistics-and-machine-learning/"
  },
  {
    "objectID": "slides/02-overview.html#machine-learning-vs.-statistical-learning-2",
    "href": "slides/02-overview.html#machine-learning-vs.-statistical-learning-2",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Machine Learning vs.¬†Statistical Learning",
    "text": "Machine Learning vs.¬†Statistical Learning\n\nMachine learning emphasizes algorithms and automation.\nStatistical learning emphasizes modeling, interpretability, and uncertainty.\n\n\n\nThe distinction is blur:\n\nA machine learner needs a well-built statistical model that quantifies uncertainty about prediction\nA statistical learner needs a computationally efficient algorithm to deal with large complex data.\n\n\n\n\n\n\nBUT, one thing for sure. Machine learning has the upper hand in marketing!"
  },
  {
    "objectID": "slides/02-overview.html#fancy-terms-and-larger-grant",
    "href": "slides/02-overview.html#fancy-terms-and-larger-grant",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Fancy Terms and Larger Grant!",
    "text": "Fancy Terms and Larger Grant!\n\nSource: http://statweb.stanford.edu/~tibs/stat315a/glossary.pdf"
  },
  {
    "objectID": "slides/02-overview.html#section-1",
    "href": "slides/02-overview.html#section-1",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "",
    "text": "Source: All of Statistics"
  },
  {
    "objectID": "slides/02-overview.html#types-of-learning",
    "href": "slides/02-overview.html#types-of-learning",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Types of Learning",
    "text": "Types of Learning\n\nSource: https://towardsdatascience.com/machine-learning-algorithms-in-laymans-terms-part-1-d0368d769a7b"
  },
  {
    "objectID": "slides/02-overview.html#supervised-learning",
    "href": "slides/02-overview.html#supervised-learning",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nResponse Y (output, outcome, target, label, dependent/endogenous variable)\nVector of p predictors X = (X_1, X_2, \\dots, X_p) (inputs, features, regressors, covariates, explanatory/exogenous/independent variable).\n\n\n\nRegression: Y is numeric (e.g price, blood pressure). (if p = 1, simple regression; if p &gt; 1, multiple regression)\nClassification: Y is categorical (e.g survived/died, digit 0-9 (MNIST), cancer class of tissue sample).\n\n\n\n\nTraining data \\mathcal{D} = \\{(x_1, y_1), \\dots ,(x_n , y_n)\\} =\\{(x_i, y_i)\\}_{i=1}^n, x_i \\in \\mathbf{R}^p, y_i \\in \\mathbf{R}^d.\n\n\n\nGoal: Use training data (E) to train our model for better (w.r.t. P) inference/prediction (T) on the response.\n\nLearn a mapping from inputs to outputs Y = f(X)."
  },
  {
    "objectID": "slides/02-overview.html#regression-example",
    "href": "slides/02-overview.html#regression-example",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Regression Example",
    "text": "Regression Example\n\nGoal: Establish the relationship between salary and demographic variables.\n\n\nSource: ISL Fig. 1.1"
  },
  {
    "objectID": "slides/02-overview.html#statistics-nah-machine-learning-neat",
    "href": "slides/02-overview.html#statistics-nah-machine-learning-neat",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Statistics Nah‚Ä¶ Machine Learning Neat!",
    "text": "Statistics Nah‚Ä¶ Machine Learning Neat!\n\nSource: https://towardsdatascience.com/the-actual-difference-between-statistics-and-machine-learning-64b49f07ea3"
  },
  {
    "objectID": "slides/02-overview.html#statistics-nah-machine-learning-neat-1",
    "href": "slides/02-overview.html#statistics-nah-machine-learning-neat-1",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Statistics Nah‚Ä¶ Machine Learning Neat!",
    "text": "Statistics Nah‚Ä¶ Machine Learning Neat!"
  },
  {
    "objectID": "slides/02-overview.html#classification-example",
    "href": "slides/02-overview.html#classification-example",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Classification Example",
    "text": "Classification Example\n\nGoal: Build a customized spam filtering system\n\n\nSource: http://penplusbytes.org/strategies-for-dealing-with-e-mail-spam/"
  },
  {
    "objectID": "slides/02-overview.html#classification-example-1",
    "href": "slides/02-overview.html#classification-example-1",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Classification Example",
    "text": "Classification Example\n\n\n\n\nSource: https://www.yesware.com/blog/email-spam/\n\n\n\n\nData: 4601 emails sent to George at HP, before 2000. Each is labeled as spam or email.\nInputs: relative frequencies of 57 of the commonly occurring words and punctuation marks.\n\n\n\n\n\nSource: ESL Tbl. 1.1"
  },
  {
    "objectID": "slides/02-overview.html#objectives-of-supervised-learning",
    "href": "slides/02-overview.html#objectives-of-supervised-learning",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Objectives of Supervised Learning",
    "text": "Objectives of Supervised Learning\nBased on the training data (E) we‚Äôd like to :\n\n(T) Prediction: Accurately predict unseen test cases.\n\nGiven a new test input (age, year, education), what is predicted salary level?\nGiven a new bunch of words in the email, the email is a spam or normal message?\n\n\n\n\n\n(T) Inference Understand which inputs affect the outcome, and how.\n\nIf education is up one level, how much salary will increase on average?\nIf ‚Äú!‚Äù increases one more time, how much percentage does the probability of being labeled as spam go up?\n\n\n\n\n\n\n(P) Assess the quality of our predictions and inferences.\n\nUse evalution metrics to assess the performance of a machine learning model/algorithm."
  },
  {
    "objectID": "slides/02-overview.html#unsupervised-learning",
    "href": "slides/02-overview.html#unsupervised-learning",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nNo outcome variable, just a set of features measured on a set of samples.\nOur training sample is \\mathcal{D} = \\{x_1, \\dots, x_n\\}, x_i \\in \\mathbf{R}^p.\n\n\n\nObjective is more fuzzy\n\nfind groups of samples or features that behave similarly (Clustering)\n\nfind linear combinations of features with the most variation (Dimension Reduction).\n\n\n\n\n\n\nDifficult to know how well you are doing.\nCan be useful as a pre-processing step for supervised learning."
  },
  {
    "objectID": "slides/02-overview.html#clustering-example",
    "href": "slides/02-overview.html#clustering-example",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Clustering Example",
    "text": "Clustering Example\n\nCustomer Segmentation: dividing customers into groups or clusters on the basis of common characteristics.\n\n\nSource: https://www.datacamp.com/community/tutorials/introduction-customer-segmentation-python"
  },
  {
    "objectID": "slides/02-overview.html#deep-learning",
    "href": "slides/02-overview.html#deep-learning",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Deep Learning",
    "text": "Deep Learning\n\nAn (artifical) neural network is a machine learning model inspired by the biological neural networks that constitute animal brains.\n\n\n\n\n\nSource: Wiki"
  },
  {
    "objectID": "slides/02-overview.html#section-2",
    "href": "slides/02-overview.html#section-2",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "",
    "text": "A neural network with several hidden layers is called a deep neural network, or deep learning.\n\nSource: ISL Ch 10"
  },
  {
    "objectID": "slides/02-overview.html#accuracy-and-interpretability-trade-off",
    "href": "slides/02-overview.html#accuracy-and-interpretability-trade-off",
    "title": "Overview of Statistical Machine Learning üíª",
    "section": "Accuracy and Interpretability Trade-Off",
    "text": "Accuracy and Interpretability Trade-Off\n\n\n\n\nSource: ISL Fig. 2.7\n\nflexibility: a model is flexible if the model performs quite good for various types of data.\n\n\n\n\nmssc6250-s25.github.io/website"
  }
]
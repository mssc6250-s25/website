{
  "hash": "83b708af5b398efa8a03b0933f30ec48",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Homework 1 - Bias-Variance Tradeoff and Linear Regression\"\neditor: source\n---\n\n::: {.hidden}\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\bgamma{\\boldsymbol \\gamma}\n\\def\\bep{\\boldsymbol \\epsilon}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n<!-- \\DeclareMathOperator*{\\argmin}{arg\\,min} -->\n\\def\\Trace{\\text{Trace}}\n:::\n\n\n::: callout-important\n[Due **Friday, Feb 14, 11:59 PM**]{.red}\n:::\n<!-- # Homework Instruction and Requirement -->\n- Please submit your work in **one** **PDF** file to **D2L \\> Assessments \\> Dropbox**. *Multiple files or a file that is not in pdf format are not allowed.*\n\n- Any relevant code should be attached.\n\n- Problems started with **(MSSC Phd)** are required for CMPS PhD students, and optional for other students for extra credits.\n\n- Readings: **ISL** Chapter 2 and 3.\n\n## Homework Questions\n\n(@) **ISL** Sec. 2.4: 1\n\n(@) **ISL** Sec. 2.4: 3\n\n(@) **ISL** Sec. 2.4: 5\n\n(@) **ISL** Sec. 3.7: 4\n\n(@) **ISL** Sec. 3.7: 6\n\n(@) **ISL** Sec. 3.7: 14\n\n(@) __Simulation of simple linear regression.__ Consider the model $y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i, ~~ \\epsilon_i \\stackrel{iid} \\sim N(0, \\sigma^2), ~~ i = 1, \\dots, n,$ or equivalently, $p(y_i \\mid x, \\beta_0, \\beta_1) = N\\left(\\beta_0 + \\beta_1x_i, \\sigma^2 \\right).$ Set $\\beta_0 = 2, \\beta_1 = -1.5, \\sigma = 1, n = 100, x_i = i.$ Generate 1000 simulated data sets and fit the model to each data set. \n<!-- and obtain 100 corresponding regression coefficient estimates $b_0$ and $b_1$. -->\nThe sampling distribution of $b_1$ is $$b_1 \\sim N\\left(\\beta_1, \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\right).$$\n\n    (a) Find the average of the 1000 estimates $b_1$. Is it close to its true expected value?\n    \n    (b) Find the variance of 1000 estimates $b_1$. Is it close to its true value?\n    \n    (c) Draw the histogram of $b_1$. Comment on your graph.\n    \n    (d) Obtain 1000 95\\% confidence interval for $\\beta_1$. What is your pertentage of coverage for $\\beta_1$? Comment your result.\n    \n\n\n(@) __Simulation of bias-variance tradeoff.__ Let $f(x) = x ^ 2$ be the true regression function. Simulate the data using $y_i = f(x_i) + \\epsilon_i, i = 1, \\dots, 100$, where $x_i = 0.01i$ and $\\epsilon_i \\stackrel{iid} \\sim N(0, 0.3^2).$ \n\n    (a) Generate 250 training data sets.\n    (b) For each data set, fit the following three models:\n      - Model 1: $y = \\beta_0+\\beta_1x+\\epsilon$\n      - Model 2: $y = \\beta_0+\\beta_1x+ \\beta_2x^2 + \\epsilon$\n      - Model 3: $y = \\beta_0+\\beta_1x+ \\beta_2x^2 + \\cdots + \\beta_9x^9 + \\epsilon$\n    (c) Calculate *empirical* MSE of $\\hat{f}$, bias of $\\hat{f}$ and variance of $\\hat{f}$. Then show that $$\\text{MSE}_{\\hat{f}} \\approx \\text{Bias}^2(\\hat{f}) + \\var(\\hat{f}).$$\n    Specifically, for each value of $x_i$, $i = 1, \\dots, 100$,\n    \n    $$\\text{MSE}_{\\hat{f}} =  \\frac{1}{250}\\sum_{k=1}^{250} \\left(\\hat{f}_k(x_i) - f(x_i) \\right) ^2,$$\n    $$\\text{Bias}(\\hat{f}) = \\overline{\\hat{f}}(x_i) - f(x_i),$$ where $\\overline{\\hat{f}}(x_i) = \\frac{1}{250}\\sum_{k = 1}^{250}\\hat{f}_k(x_i)$ is the sample mean of $\\hat{f}(x_i)$ that approximates $\\E_{\\hat{f}}\\left(\\hat{f}(x_i)\\right).$\n    $$ \\var(\\hat{f}) =  \\frac{1}{250}\\sum_{k=1}^{250} \\left(\\hat{f}_k(x_i) - \\overline{\\hat{f}}(x_i) \\right) ^2.$$\n    **[Note:]** If you calculate the variance using the built-in function such as `var()` in R, the identity holds only approximately because of the $250 - 1$ term in the denominator in the sample variance formula. If instead $250$ is used in the denominator, the identity holds exactly.\n    \n    (d) For each model, plot first ten estimated $f$, $\\hat{f}_{1}(x), \\dots, \\hat{f}_{10}(x)$, and the average of $\\hat{f}$, $\\frac{1}{250}\\sum_{k=1}^{250}\\hat{f}_{k}(x)$ in one figure, as @fig-trainedf below. What's your finding?\n    (e) Generate one more data set and use it as the test data. Calculate the *overall* training MSE (for training $y$) and *overall* test MSE (for test $y$) for each model.\n     $$MSE_\\texttt{Tr} =  \\frac{1}{250} \\sum_{k = 1}^{250} \\frac{1}{100} \\sum_{i=1}^{100} \\left(\\hat{f}_{k}(x_i) - y_{i}^k\\right)^2$$\n     $$MSE_\\texttt{Te} =  \\frac{1}{250} \\sum_{k = 1}^{250} \\frac{1}{100} \\sum_{i=1}^{100} \\left(\\hat{f}_{k}(x_i) - y_{i}^{\\texttt{Test}}\\right)^2$$\n    \n\n::: {.cell .fig-cap-location-margin}\n::: {.cell-output-display}\n![Trained $f$](../slides/images/03-bias-var/mse-var.png){#fig-trainedf width=100%}\n:::\n:::\n\n<!-- (@) __Gradient descent.__ Write your own gradient descent algorithm for linear regression. Implement it and compare with the built-in function such as `optim()` to make sure that it converges correctly. -->\n\n\n(@) **(MSSC PhD)** __Stochastic gradient descent.__ Write your own stochastic gradient descent algorithm for linear regression. Implement it and compare with the built-in function such as `optim()` to make sure that it converges correctly.\n\n<!-- ## Optional Exercises -->\n\n<!-- 1. Show that $\\text{MSE}_{y_0} = \\E\\left[\\left(y_0 - \\hat{f}(x_0)\\right)^2\\right] = \\text{MSE}_{\\hat{f}} + \\text{Var}(\\epsilon)$ -->\n\n<!-- 2. Prove the Gaussâ€“Markov theorem: for linear regression the least squares estimate of a parameter $\\mathbf{c}'\\bbeta$ has variance no bigger than that of any other linear unbiased estimate $\\mathbf{c}'\\bbeta$. -->\n\n<!-- 3. Suppose we fit the model ${\\bf y} = {\\bf X}_1 \\bbeta_1 + \\bep$ when the true model is actually ${\\bf y} = {\\bf X}_1 \\bbeta_1 + {\\bf X}_2 \\bbeta_2 + \\bep$. For both models, assume $\\E(\\bep) = {\\bf 0}$ and $\\var (\\bep) = \\sigma^2 {\\bf I}$. Let ${\\bf b}_1$ be the ordinary least-squares estimate of $\\bbeta_1$. Find the $\\E({\\bf b}_1)$, $\\var({\\bf b}_1)$ and $\\text{MSE}({\\bf b}_1)$. Compare them with the ones from the true model. This is an underfitting example of linear regression. -->\n\n<!-- 4. Suppose we fit the model ${\\bf y} = {\\bf X} \\bbeta + {\\bf Z}\\bgamma + \\bep$ when the true model is actually ${\\bf y} = {\\bf X} \\bbeta + \\bep$. For both models, assume $\\E(\\bep) = {\\bf 0}$ and $\\var (\\bep) = \\sigma^2 {\\bf I}$. Let ${\\bf b}$ be the ordinary least-squares estimate of $\\bbeta$. Find the $\\E({\\bf b})$, $\\var({\\bf b})$ and $\\text{MSE}({\\bf b})$. Compare them with the ones from the true model. This is an overfitting example of linear regression. -->\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "aa2ef9f4c83d5b9c4c808671b66f0d4b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"08- Bayesian Linear Regression Code Demo\"\nauthor: 'Dr. Cheng-Han Yu'\nformat: \n  html:\n    toc: true\n    code-link: true\n    code-fold: show\n    code-summary: \"Show/Hide\"\n    code-tools: true\n# filters: \n#   - include-code-files\n---\n\n## R implementation\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(rstan)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(tidybayes)\nlibrary(janitor)\nlibrary(broom.mixed)\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(bikes)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbike_model <- rstanarm::stan_glm(rides ~ temp_feel, data = bikes,\n                                 family = gaussian,\n                                 prior_intercept = normal(5000, 1000),\n                                 prior = normal(100, 40), \n                                 prior_aux = exponential(0.0008),\n                                 chains = 4, iter = 5000*2, seed = 2025)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000667 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 6.67 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.187 seconds (Warm-up)\nChain 1:                0.201 seconds (Sampling)\nChain 1:                0.388 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 9e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.182 seconds (Warm-up)\nChain 2:                0.206 seconds (Sampling)\nChain 2:                0.388 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 8e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.109 seconds (Warm-up)\nChain 3:                0.213 seconds (Sampling)\nChain 3:                0.322 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.5e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.136 seconds (Warm-up)\nChain 4:                0.211 seconds (Sampling)\nChain 4:                0.347 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_trace(bike_model, size = 0.1)\n```\n\n::: {.cell-output-display}\n![](08-bayes-code_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_dens_overlay(bike_model)\n```\n\n::: {.cell-output-display}\n![](08-bayes-code_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::neff_ratio(bike_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)   temp_feel       sigma \n    0.99465     0.99180     0.95030 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::rhat(bike_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)   temp_feel       sigma \n  1.0000752   1.0000909   0.9999646 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(bike_model, effects = c(\"fixed\", \"aux\"),\n     conf.int = TRUE, conf.level = 0.80)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 Ã— 5\n  term        estimate std.error conf.low conf.high\n  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)  -2191.     355.    -2653.    -1735. \n2 temp_feel       82.1      5.07     75.7      88.7\n3 sigma         1282.      41.1    1231.     1336. \n4 mean_PPD      3487.      80.3    3385.     3590. \n```\n\n\n:::\n:::\n\n\n## Python implementation\n\n- Check [PyMC demo](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/GLM_linear.html#glm-linear)\n\n- Error occurs when rendering the quarto file. But it can be run in a Python script.\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pymc as pm\nimport numpy as np\nimport pandas as pd\nimport arviz as az\nimport xarray as xr\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# Load your dataset\nbikes = pd.read_csv(\"../data/bikes.csv\")\n# Assuming `rides` is the target variable and `temp_feel` is the predictor\nrides = bikes[\"rides\"].values\ntemp_feel = bikes[\"temp_feel\"].values\n```\n:::\n\n<!-- ```{.python include=\"bayes.py\"} -->\n<!-- ``` -->\n\n\n<!-- ```{python} -->\n<!-- #| echo: false -->\n<!-- # name cannot start with number -->\n<!-- %run bayes.py -->\n<!-- ``` -->\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pymc as pm\n# Define the Bayesian model\nwith pm.Model() as bike_model:\n    # Priors\n    intercept = pm.Normal(\"intercept\", mu=5000, sigma=1000)\n    slope = pm.Normal(\"slope\", mu=100, sigma=40)\n    # sigma = pm.Exponential(\"sigma\", lam=0.0008)\n    sigma = pm.HalfCauchy(\"sigma\", beta=10)\n    # Linear model\n    mu = intercept + slope * temp_feel\n\n    # Likelihood\n    likelihood = pm.Normal(\"rides\", mu=mu, sigma=sigma, observed=rides)\n\n    # Sampling\n    draws = pm.sample(5000, tune=5000, chains=4, random_seed=2025)\n\n\n# pm.summary(draws)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npm.summary(draws)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\naz.plot_trace(draws)\n```\n:::\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntrace.posterior[\"y_model\"] = trace.posterior[\"Intercept\"] + trace.posterior[\"x\"] * xr.DataArray(x)\n```\n:::\n\n\n::: {.cell}\n\n```{.python .cell-code}\n_, ax = plt.subplots(figsize=(7, 7))\naz.plot_lm(trace=trace, y=\"y\", num_samples=100, axes=ax, y_model=\"y_model\")\nax.set_title(\"Posterior predictive regression lines\")\nax.set_xlabel(\"x\");\n```\n:::\n\n",
    "supporting": [
      "08-bayes-code_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
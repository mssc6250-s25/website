{
  "hash": "58ad39b87756ddedf9043228947e2659",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Support Vector Machines `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 384 512\" style=\"height:1em;width:0.75em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M374.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-320 320c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l320-320zM128 128A64 64 0 1 0 0 128a64 64 0 1 0 128 0zM384 384a64 64 0 1 0 -128 0 64 64 0 1 0 128 0z\"/></svg>`{=html}'\nsubtitle: \"MSSC 6250 Statistical Machine Learning\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"April 01 2025\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    #     - \"macros.tex\"\n    highlight-style: github\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t\n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[mssc6250-s25.github.io/website](https://mssc6250-s25.github.io/website/)\"\n    theme: [\"simple\", \"styles.scss\"]\n    echo: false\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    code-line-numbers: false\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n---\n\n\n# {visibility=\"hidden\"}\n\n\\def\\cD{{\\cal D}}\n\\def\\cL{{\\cal L}}\n\\def\\cX{{\\cal X}}\n\\def\\cF{{\\cal F}}\n\\def\\cH{{\\cal H}}\n\\def\\bA{\\mathbf{A}}\n\\def\\bB{\\mathbf{B}}\n\\def\\bX{\\mathbf{X}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bU{\\mathbf{U}}\n\\def\\bD{\\mathbf{D}}\n\\def\\bV{\\mathbf{V}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\bK{\\mathbf{K}}\n\\def\\bx{\\mathbf{x}}\n\\def\\by{\\mathbf{y}}\n\\def\\bs{\\mathbf{s}}\n\\def\\br{\\mathbf{r}}\n\\def\\bu{\\mathbf{u}}\n\\def\\be{\\mathbf{e}}\n\\def\\bv{\\mathbf{v}}\n\\def\\bp{\\mathbf{p}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bz{\\mathbf{z}}\n\\def\\bzero{\\mathbf{0}}\n\\def\\balpha{\\boldsymbol \\alpha}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\btheta{\\boldsymbol \\theta}\n\\def\\bxi{\\boldsymbol \\xi}\n\\def\\bmu{\\boldsymbol \\mu}\n\\def\\bepsilon{\\boldsymbol \\epsilon}\n\\def\\T{\\text{T}}\n\\def\\Trace{\\text{Trace}}\n\\def\\Cov{\\text{Cov}}\n\\def\\Var{\\text{Var}}\n\\def\\E{\\text{E}}\n\\def\\pr{\\text{pr}}\n\\def\\Prob{\\text{P}}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\n\n\n\n\n\n## Support Vector Machines (SVMs)\n\n- SVMs have been shown to perform well in a variety of settings, and are often considered one of the best \"out of the box\" classifiers.\n\n- Start with the **maximal margin classifier** (1960s), then the **support vector classifier** (1990s), and then the **support vector machine**.\n\n\n\n## Classifier\n\n- $\\cD_n = \\{\\bx_i, y_i\\}_{i=1}^n$\n\n- In SVM, we code the binary outcome $y$ as [1 or -1]{.green}, representing one class and the other. \n\n- The goal is to find a *linear* classifier $f(\\bx) = \\beta_0 + \\bx' \\bbeta$ so that the classification rule is the sign of $f(\\bx)$:\n\n$$\n\\hat{y} = \n\\begin{cases}\n        +1, \\quad \\text{if} \\quad f(\\bx) > 0\\\\ \n        -1, \\quad \\text{if} \\quad f(\\bx) < 0\n\\end{cases}\n$$\n\n## Separating Hyperplane\n\n- The $f(\\bx) = \\beta_0 + \\bx' \\bbeta = 0$ is a **hyperplane**, which is a subspace of dimension $p-1$ in the $p$-dimensional space.\n\n\n- $f(\\bx) = \\beta_0 + \\beta_1X_1+\\beta_2X_2 = 0$ is a straight line (hyperplane of dimension one) in the 2-dimensional space.\n\n- The classification rule is $y_i f(\\bx_i) >0$.\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/13-svm/unnamed-chunk-2-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Maximum-margin Classifier\n\n- If our data can be *perfectly* separated using a hyperplane, there exists an *infinite* number of such hyperplanes. But which one is the *best*?\n\n- A natural choice is the __maximal margin hyperplane__ (*optimal separating hyperplane*), which is the separating hyperplane that is *farthest from the training points*.\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/13-svm/unnamed-chunk-3-1.png){fig-align='center' width=70%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/13-svm/unnamed-chunk-4-1.png){fig-align='center' width=70%}\n:::\n:::\n\n:::\n\n::::\n\n\n::: notes\n\nthe maximal margin hyperplane depends on the support vectors. If the support vectors are moved, the maximal margin hyperplane would move too.\n\n:::\n\n\n## Maximum-margin Classifier\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(e1071)\nsvm_fit <- svm(y ~ ., data = data.frame(x, y), type = 'C-classification', \n               kernel = 'linear', scale = FALSE, cost = 10000)\n```\n:::\n\n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n- The training points lied on the dashed lines are **support vectors**:\n  + if they were moved, the maximal margin hyperplane would move too.\n  + the hyperplane depends directly on the support vectors, but **not** on the other observations, provided that their movement does not cause it to cross the boundary.\n\n- It can lead to overfitting when $p$ is large. (No misclassification on training set)\n\n- Hope the classifier will also have a large margin on the test data.\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/13-svm/unnamed-chunk-6-1.png){fig-align='center' width=85%}\n:::\n:::\n\n:::\n\n::::\n\n::: aside\n\nUse [from sklearn import svm](https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation) for Python implementation.\n\n:::\n\n\n\n\n\n::: notes\n\nWe use the e1071 package to fit the SVM. There is a cost parameter C\n, with default value 1. This parameter has a significant impact on non-separable problems. However, for our separable case, we will set this to be a very large value, meaning that the cost for having a wrong classification is very large. We also need to specify the linear kernel.\n\n- index gives the index of all support vectors\n- coefs provides the yiαi for the support vectors\n- SV are the xi values correspond to the support vectors\n- rho is negative β0\n\nthey “support” the maximal margin hyperplane in the sense vector\nthat if these points were moved slightly then the maximal margin hyperplane would move as well.\n\nhttps://afit-r.github.io/svm\n\nkernlab::ksvm()\n\nwe can compute the (perpendicular) distance from each training observation to a given separating hyperplane; the smallest such distance is the minimal distance from the observations to the hyperplane, and is known as the margin. The maximal margin hyperplane is the separating hyperplane for which the margin is margin largest—that is, it is the hyperplane that has the farthest minimum distance to the training observations.\n\n:::\n\n\n\n\n\n## Linearly Separable SVM\n\nIn linear SVM, $f(\\bx) = \\beta_0 + \\bx' \\bbeta$. When $f(\\bx) = 0$, it corresponds to a hyperplane that separates the two classes:\n\n$$\\{ \\bx: \\beta_0 + \\bx'\\boldsymbol \\beta = 0 \\}$$\n\n- For this separable case, all observations with $y_i = 1$ are on one side $f(\\bx) > 0$, and observations with $y_i = -1$ are on the other side. \n\n- The __distance from any point $\\bx_0$ to the hyperplane__ is\n\n$$\\frac{1}{\\lVert \\bbeta \\lVert} |f(\\bx_0)|$$\nFor $p = 2$, and the plane $\\beta_0 + \\beta_1 X_1 + \\beta_2X_2 = 0$, the distance is $$ \\frac{ |\\beta_0 + \\beta_1 x_{01} + \\beta_2x_{02}|}{\\sqrt{\\beta_1^2 + \\beta^2_2}}$$ \n\n\n\n\n## Optimization for Linearly Separable SVM\n\n\\begin{align}\n\\underset{\\bbeta, \\beta_0}{\\text{max}} \\quad & M \\\\\n\\text{s.t.} \\quad & \\frac{1}{\\lVert \\bbeta \\lVert} y_i(\\bx' \\bbeta + \\beta_0) \\geq M, \\,\\, i = 1, \\ldots, n.\n\\end{align}\n\n- The constraint requires that each point be on the correct side of the hyperplane, with some cushion.\n\n- The scale of $\\bbeta$ can be arbitrary, so just set it as $\\lVert \\bbeta \\rVert = 1$:\n\n\\begin{align}\n\\underset{\\bbeta, \\beta_0}{\\text{max}} \\quad & M \\\\\n\\text{s.t.}\n \\quad & \\lVert \\bbeta \\lVert = 1, \\\\\n \\quad &  y_i(\\bx' \\bbeta + \\beta_0) \\geq M, \\,\\, i = 1, \\ldots, n.\n\\end{align}\n\n. . .\n\n- How to solve it? Learn it in MSSC 5650.\n\n\n\n\n## Linearly Non-separable SVM with Slack Variables\n\n- Often, no separating hyperplane exists, so there is no maximal\nmargin classifier.\n\n- The previous optimization problem has no solution with $M > 0$.\n\n- *Idea*: develop a hyperplane that _almost_ separates the classes, using a so-called **soft margin**: **soft margin classifier**.\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/13-svm/unnamed-chunk-7-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Why Linearly Non-separable Support Vector Classifier\n\n- Even if a separating hyperplane does exist, the maximum-margin classifier might not be desirable.\n\n- The maximal margin hyperplane is extremely sensitive to a change in a single observation: it may overfit the training data. ([*low-bias high-variance*]{.green})\n\n\n::: xsmall\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: ISL Fig. 9.5](./images/13-svm/fig9-5.png){fig-align='center' width=85%}\n:::\n:::\n\n:::\n\n\n\n## Soft Margin Classifier\n\n- Consider a classifier based on a hyperplane that does *NOT* perfectly separate the two classes, but\n  + [Better classification of _most_ of the training observations.]{.green}\n  + [Greater robustness to individual observations]{.green}\n\n  \n- It could be worthwhile to misclassify a few training points\nin order to do a better job in classifying the remaining observations.\n\n- Allow some points to be on the incorrect side of the margin ([8]{.blue} and [1]{.pink}), or even the incorrect side of the hyperplane ([12]{.blue} and [11]{.pink} training points misclassified by the classifier).\n\n\n::: xsmall\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: ISL Fig. 9.6](./images/13-svm/9-6.png){fig-align='center' width=55%}\n:::\n:::\n\n:::\n\n\n\n## Optimization for Soft Margin Classifier\n\n\\begin{align}\n\\underset{\\bbeta, \\beta_0, \\epsilon_1, \\dots, \\epsilon_n}{\\text{max}} \\quad & M \\\\\n\\text{s.t.}\n \\quad & \\lVert \\bbeta \\lVert = 1, \\\\\n \\quad &  y_i(\\bx' \\bbeta + \\beta_0) \\geq M(1 - \\epsilon_i), \\\\\n  \\quad & \\epsilon_i \\ge 0, \\sum_{i=1}^n\\epsilon_i \\le B, \\,\\, i = 1, \\ldots, n,\n\\end{align}\nwhere $B > 0$ is a tuning parameter. \n\n- $\\epsilon_1, \\dots, \\epsilon_n$ are **slack variables** that allow individual points to be on the wrong side of the margin or the hyperplane.\n\n- The $i$th point is on the\n  + *correct* side of the margin when $\\epsilon_i = 0$\n  + *[wrong]{.blue} side of the [margin]{.blue}* when $\\epsilon_i > 0$\n  + *[wrong]{.blue} side of the [hyperplane]{.blue}* when $\\epsilon_i > 1$\n\n\n::: notes\n  + tells us where the $i$th observation is located, relative to the hyperplane and relative to the margin.\n:::\n\n\n\n## Optimization for Soft Margin Classifier\n\n\\begin{align}\n\\underset{\\bbeta, \\beta_0, \\epsilon_1, \\dots, \\epsilon_n}{\\text{max}} \\quad & M \\\\\n\\text{s.t.}\n \\quad & \\lVert \\bbeta \\lVert = 1, \\\\\n \\quad &  y_i(\\bx' \\bbeta + \\beta_0) \\geq M(1 - \\epsilon_i), \\\\\n  \\quad & \\epsilon_i \\ge 0, \\sum_{i=1}^n\\epsilon_i \\le B, \\,\\, i = 1, \\ldots, n,\n\\end{align}\nwhere $B > 0$ is a tuning parameter. \n\n- $B$ determines the number and severity of the violations\nto the margin/hyperplane we tolerate.\n  + $B = 0$: *no budget* for violations ($\\epsilon_1 = \\cdots = \\epsilon_n = 0$)\n  + $B > 0$: *no more than $B$ points* can be on the wrong side of the hyperplane. ($\\epsilon_i > 1$)\n  + As $B$ increases, more violations and wider margin. (more bias less variance)\n  + Choose $B$ via cross-validation.\n\n::: notes\nB as a budget for the amount that the margin can be violated\nby the n observations\n:::\n\n\n\n\n## Optimization for Soft Margin Classifier\n\n:::: {.columns}\n\n::: {.column width=\"33%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/13-svm/unnamed-chunk-10-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"33%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/13-svm/unnamed-chunk-11-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"33%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/13-svm/unnamed-chunk-12-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::::\n\n\n\n## Optimization for Soft Margin Classifier\n\n:::{.callout-warning}\n\n:::{style=\"font-size: 1.1em;\"}\n\nThe argument `cost` in `e1071::svm()` and `C` in [`sklearn.svm.SVC()`](https://scikit-learn.org/stable/modules/svm.html#svc) is the $C$ defined in the **primal form** \n\\begin{align}\n\\underset{\\bbeta, \\beta_0}{\\text{min}} \\quad & \\frac{1}{2}\\lVert \\bbeta \\rVert^2 + C \\sum_{i=1}^n \\epsilon_i \\\\\n\\text{s.t} \\quad & y_i (\\bx_i' \\bbeta + \\beta_0) \\geq (1 - \\epsilon_i), \\\\\n\\text{} \\quad & \\epsilon_i \\geq 0, \\,\\, i = 1, \\ldots, n,\n\\end{align}\n\n:::\n\nso small cost $C$ means larger budget $B$.\n\n:::\n\n\n\n## SVM, LDA and Logistic Regression\n\n:::{.callout-note}\n\n:::{style=\"font-size: 1.3em;\"}\n\n- **SVM** decision rule is based only on a [subset]{.blue} of the training data (robust to the behavior of data that are far away from the hyperplane.)\n\n- **LDA** depends on the mean of [_all_]{.blue} of the observations within each class, and within-class covariance matrix computed using *all* of the data.\n\n- **Logistic regression**, unlike LDA, is insensitive to observations far from the decision boundary too.\n\n:::\n\n:::\n\n\n\n## Classification with Non-Linear Decision Boundaries\n\n- The soft margin classifier is a natural approach for classification in the two-class setting, if the boundary between the two classes is linear.\n\n- In practice we are often faced with non-linear class boundaries.\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/13-svm/unnamed-chunk-13-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n## Classification with Non-Linear Decision Boundaries\n\n- In regression, we enlarge the feature space using functions of the predictors to address this non-linearity.\n\n- In SVM (logistic regression too!), we could address non-linear boundaries by enlarging the feature space.\n\n- For example, rather than fitting a support vector classifier using $p$ features, $X_1, \\dots, X_p$, we could instead fit a support vector classifier using $2p$ features $X_1,X_1^2,X_2,X_2^2, \\dots , X_p, X_p^2$.\n\n. . .\n\n:::{.midi}\n\n\\begin{align}\n\\underset{\\beta_0, \\beta_{11}, \\beta_{12}, \\dots, \\beta_{p1}, \\beta_{p2}, \\epsilon_1, \\dots, \\epsilon_n}{\\text{max}} \\quad & M \\\\\n\\text{s.t.}\n \\quad &  y_i\\left(\\beta_0 + \\sum_{i = 1}^n \\beta_{j1}x_{ij} + \\sum_{i = 1}^n \\beta_{j2}x_{ij}^2\\right) \\geq M(1 - \\epsilon_i), \\\\\n  \\quad & \\epsilon_i \\ge 0, \\sum_{i=1}^n\\epsilon_i \\le B, \\,\\, i = 1, \\ldots, n,\\\\\n  \\quad & \\sum_{j=1}^p\\sum_{k=1}^2\\beta_{jk}^2 = 1.\n\\end{align}\n\n:::\n\n\n::: notes\n\nIn the enlarged\nfeature space, the decision boundary that results from (9.16) is in fact linear.\nBut in the original feature space, the decision boundary is of the form\nq(x) = 0, where q is a quadratic polynomial, and its solutions are generally\nnon-linear.\n\nThe support vector machine, which\nwe present next, allows us to enlarge the feature space used by the support\nvector classifier in a way that leads to efficient computations\n\n:::\n\n\n<!-- $$f(\\bx) = \\beta_0 + \\sum_{i=1}^n\\alpha_i\\langle \\bx, \\bx_{i} \\rangle = \\beta_0 + \\sum_{i\\in \\mathcal{S}}\\alpha_i\\langle \\bx, \\bx_{i} \\rangle$$ -->\n\n\n\n\n## Solution to Support Vector Classifier\n\n- The solution to the support vector classifier optimization involves only the *inner products of the observations*: $\\langle \\bx_i, \\bx_{i'} \\rangle = \\sum_{j=1}^px_{ij}x_{i'j}$\n\n- The linear support vector classifier can be represented as\n$$f(\\bx) = \\beta_0 + \\sum_{i\\in \\mathcal{S}}\\alpha_i\\langle \\bx, \\bx_{i} \\rangle$$\nwhere $\\mathcal{S}$ is the collection of indices of the support points.\n\n- $\\alpha_i$ is nonzero only for the support vectors in the solution\n\n. . .\n\n<!-- - To train the parameters $\\alpha_i$s and $\\beta_0$, all we need are the $n \\choose 2$ inner products $\\langle \\bx_i, \\bx_{i'} \\rangle$ between all pairs of training observations. -->\n\n- To evaluate the function $f(\\bx_0)$, we compute $\\langle \\bx_0, \\bx_{i} \\rangle$.\n\n\n\n\n## Nonlinear SVM via Kernel Trick\n\n<!-- - $\\alpha_i$ is nonzero only for the support vectors in the solution -->\n\n<!-- $$f(\\bx) = \\beta_0 + \\sum_{i\\in \\mathcal{S}}\\alpha_i\\langle \\bx, \\bx_{i} \\rangle$$ -->\n<!-- where $\\mathcal{S}$ is the collection of indices of these support points. -->\n\n<!-- . . . -->\n\n\n- The support vector machine (SVM) is an extension of the support vector classifier that results from enlarging the feature space in a specific way, using **kernels**.\n\n- The kernel approach is an efficient computational approach for enlarging our feature space and non-linear boundary.\n\n. . .\n\n\n- Kernel Trick:\n$$f(\\bx) = \\beta_0 + \\sum_{i\\in \\mathcal{S}}\\alpha_i K\\left(\\bx, \\bx_{i}\\right) $$\n\n- **Linear kernel**: $K\\left(\\bx_0, \\bx_{i}\\right) = \\langle \\bx_0, \\bx_{i'} \\rangle = \\sum_{j=1}^px_{0j}x_{ij}$\n\n- **Polynomial kernel**: $K\\left(\\bx_0, \\bx_{i}\\right) = \\left(1 + \\sum_{j=1}^px_{0j}x_{ij}\\right)^d$\n\n- **Radial kernel**: $K\\left(\\bx_0, \\bx_{i'}\\right) = \\exp \\left(-\\gamma\\sum_{j=1}^p (x_{0j}-x_{ij})^2 \\right)$\n\n\n\n\n## Radial Kernel Decision Doundary\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/13-svm/unnamed-chunk-14-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## SVM as a Penalized Model\n\n\\begin{align}\n\\underset{\\bbeta, \\beta_0, \\epsilon_1, \\dots, \\epsilon_n}{\\text{max}} \\quad & M \\\\\n\\text{s.t.}\n \\quad & \\lVert \\bbeta \\lVert = 1, \\\\\n \\quad &  y_i(\\bx_i' \\bbeta + \\beta_0) \\geq M(1 - \\epsilon_i), \\\\\n  \\quad & \\epsilon_i \\ge 0, \\sum_{i=1}^n\\epsilon_i \\le B, \\,\\, i = 1, \\ldots, n,\n\\end{align}\n\n. . .\n\n\\begin{align}\n\\underset{\\bbeta, \\beta_0}{\\text{min}} \\left\\{ \\sum_{i=1}^n  \\max \\left[ 0, 1 - y_i (\\bx_i' \\bbeta + \\beta_0) \\right] + \\lambda \\lVert \\bbeta \\lVert ^ 2 \\right\\}\n\\end{align} \nwhere $\\sum_{i=1}^n  \\max \\left[ 0, 1 - y_i (\\bx' \\bbeta + \\beta_0) \\right]$ is known as **hinge loss**.\n\n- Large $\\lambda$ (large $B$): small $\\beta_j$s, high-bias and low-variance.\n\n- Small $\\lambda$ (small $B$): low-bias and high-variance.\n\n\n\n\n## Loss Functions\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n\n- The hinge loss is zero for observations for which $y_i (\\bx' \\bbeta + \\beta_0) \\ge 1$ (correct side of the margin).\n\n- The logistic loss is not zero anywhere.\n\n- SVM is better when classes are well separated.\n\n- Logistic regression is preferred in more overlapping regimes.\n\n\n:::\n\n\n\n::: {.column width=\"60%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/13-svm/unnamed-chunk-15-1.png){fig-align='center' width=80%}\n:::\n:::\n\n:::\n\n::::\n\n\n::: notes\n, but small for points far from the boundary.\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "68953ce1da270dbd411d920cbca16e23",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'K-nearest Neighbors `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 640 512\" style=\"height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M72 88a56 56 0 1 1 112 0A56 56 0 1 1 72 88zM64 245.7C54 256.9 48 271.8 48 288s6 31.1 16 42.3V245.7zm144.4-49.3C178.7 222.7 160 261.2 160 304c0 34.3 12 65.8 32 90.5V416c0 17.7-14.3 32-32 32H96c-17.7 0-32-14.3-32-32V389.2C26.2 371.2 0 332.7 0 288c0-61.9 50.1-112 112-112h32c24 0 46.2 7.5 64.4 20.3zM448 416V394.5c20-24.7 32-56.2 32-90.5c0-42.8-18.7-81.3-48.4-107.7C449.8 183.5 472 176 496 176h32c61.9 0 112 50.1 112 112c0 44.7-26.2 83.2-64 101.2V416c0 17.7-14.3 32-32 32H480c-17.7 0-32-14.3-32-32zm8-328a56 56 0 1 1 112 0A56 56 0 1 1 456 88zM576 245.7v84.7c10-11.3 16-26.1 16-42.3s-6-31.1-16-42.3zM320 32a64 64 0 1 1 0 128 64 64 0 1 1 0-128zM240 304c0 16.2 6 31 16 42.3V261.7c-10 11.3-16 26.1-16 42.3zm144-42.3v84.7c10-11.3 16-26.1 16-42.3s-6-31.1-16-42.3zM448 304c0 44.7-26.2 83.2-64 101.2V448c0 17.7-14.3 32-32 32H288c-17.7 0-32-14.3-32-32V405.2c-37.8-18-64-56.5-64-101.2c0-61.9 50.1-112 112-112h32c61.9 0 112 50.1 112 112z\"/></svg>`{=html}'\nsubtitle: \"MSSC 6250 Statistical Machine Learning\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"March 17 2025\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    #     - \"macros.tex\"\n    highlight-style: github\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t\n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[mssc6250-s25.github.io/website](https://mssc6250-s25.github.io/website/)\"\n    theme: [\"simple\", \"styles.scss\"]\n    echo: false\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    code-line-numbers: true\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  cache: true\n  fig-align: center\n---\n\n\n# {visibility=\"hidden\"}\n\n\n\n\\def\\bg{\\mathbf{g}}\n\\def\\bu{\\mathbf{u}}\n\\def\\bv{\\mathbf{v}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bx{\\mathbf{x}}\n\\def\\by{\\mathbf{y}}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\btheta{\\boldsymbol \\theta}\n\\def\\bmu{\\boldsymbol \\mu}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\bX{\\mathbf{X}}\n\\def\\T{\\text{T}}\n\\def\\E{\\text{E}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n<!-- # Nonparametric Methods {background-color=\"#447099\"} -->\n\n\n## Nonparametric Examplar-based Methods\n\n- So far we have mostly focused on parametric models, either unconditional $p({\\bf y} \\mid \\btheta)$ or conditional $p({\\bf y} \\mid \\bx, \\btheta)$, where $\\btheta$ is a fixed-dimensional vector of parameters. ^[For example, $\\beta$ coefficients in linear regression.]\n\n- The parameters are estimated from the training set $\\mathcal{D} = \\{(\\bx_i, \\by_i)\\}_{i=1}^n$ but after model fitting, the data is not used anymore.\n\n. . .\n\n- The nonparametric models that *keep the training data around at the test time* are called **examplar-based models**, **instance-based learning** or **memory-based learning**.\n  + *K-nearest neighbors classification and regression*\n  + [Kernel regression](https://math4780-f23.github.io/website/slides/12-nonpara-reg.html#/nonparametric-regression-1)\n  + [Local regression, e.g., LOESS](https://math4780-f23.github.io/website/slides/12-nonpara-reg.html#/local-regression)\n  + Kernel density estimation\n\n. . .\n\n- The examplar-based models usually perform a **local averaging** technique based on the similarity or distance between a test input $\\bx_0$ and each of the training inputs $\\bx_i$.\n\n<!-- # K-nearest Neighbors -->\n\n\n## K-nearest Neighbor Regression\n\n- **K-nearest neighbor (KNN)** is a nonparametric method that can be used for regression and classification.\n\n- In KNN, we don't have parameters $\\bbeta$, and $f(\\bx_0) = \\bx_0'\\bbeta$ in linear regression.^[$\\widehat{y}_0 = \\widehat{f}(x_0) = \\bx_0'\\hat{\\bbeta}$.]\n\n- We directly estimate $f(\\bx_0)$ using our *examples* or *memory*.\n\n$$ \\widehat{y}_0 = \\frac{1}{k} \\sum_{x_i \\in N_k(x_0)} y_i,$$ \nwhere the neighborhood of $x_0$, $N_k(x_0)$, defines the *$k$ training data points that are closest to $x_0$*.\n\n- Closeness (Similarity) is defined using a distance measure, such as the Euclidean distance.\n\n\n\n## 1-Nearest Neighbor Regression\n\n::: {.cell}\n::: {.cell-output-display}\n![](11-knn_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n\n## Tuning $k$\n\n- $y_i = 2\\sin(x_i) + \\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0, 1), \\quad i = 1, \\dots, 200$\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-knn_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n::: notes\n\n- put all weights on one single training point. The predictive value at some test $x$ relies solely on one single training point that is closest to $x$.\n- Carry lots of noises\n\n:::\n\n\n\n\n## The Bias-variance Trade-off\n\n::: {.cell}\n::: {.cell-output-display}\n![](11-knn_files/figure-revealjs/unnamed-chunk-5-1.png){width=100%}\n:::\n:::\n\n\n\n:::notes\n\nIf we consider different values of $k$, we can observe the trade-off between bias and variance. \n- What's the training error when K=1?\n- What is the fitted curve like when K is all training data points?\n\n:::\n\n\n## Bias-Variance Trade-Off\n\n\\begin{aligned}\n\\E\\Big[ \\big( Y - \\widehat f(x_0) \\big)^2 \\Big]\n&= \\underbrace{\\E\\Big[ ( Y - f(x_0))^2 \\big]}_{\\text{Irreducible Error}} +\n\\underbrace{\\Big(f(x_0) - \\E[\\widehat f(x_0)]\\Big)^2}_{\\text{Bias}^2} + \n\\underbrace{\\E\\Big[ \\big(\\widehat f(x_0) - \\E[\\widehat f(x_0)] \\big)^2 \\Big]}_{\\text{Variance}}\n\\end{aligned}\n  \n* As $k \\uparrow$, bias $\\uparrow$ and variance $\\downarrow$ (smoother)\n* As $k \\downarrow$, bias $\\downarrow$ and variance $\\uparrow$ (more wiggly)\n  \n\n\n::: notes\n\nwhen k is small, the estimated model is unstable.\nAlso, each time we observe a new training data, we may get a very different estimation. (due to the closest sample and )\nWhen k is large, the estimated model eventually deviates (systematically) from the truth.\nIf we use more “neighbouring” points, say k, the variance would reduce to approximately σ2/k. But the bias2 will increase as neighbours are far away from x0.\nk determines the model complexity\n\n:::\n\n\n\n## Degrees of Freedom\n\n- $k$ determines the model complexity and degrees of freedom (df).\n\n- In general, the df can be defined as \n\n$$\\text{df}(\\hat{f}) = \\frac{1}{\\sigma^2}\\text{Trace}\\left( \\cov(\\hat{\\by}, \\by)\\right)= \\frac{1}{\\sigma^2}\\sum_{i=1}^n \\cov(\\hat{y}_i, y_i)$$\n\n- $k = 1$: $\\hat{f}(x_i) = y_i$ and $\\text{df}(\\hat{f}) = n$\n\n- $k = n$: $\\hat{f}(x_i) = \\bar{y}$ and $\\text{df}(\\hat{f}) = 1$\n\n- For general $k$, $\\text{df}(\\hat{f}) = n/k$.\n\n. . .\n\n- Linear regression with $p$ coefficients: $\\text{df}(\\hat{f}) = \\text{Trace}\\left( {\\bf H} \\right) = p$\n\n- For any linear smoother $\\hat{\\by} = {\\bf S} \\by$, $\\text{df}(\\hat{f}) = \\text{Trace}({\\bf S})$.\n\n\n::: notes\n\n- How y and y_hat move together, how true y and fitted y move together\n- The covariance term quantifies how much the predictions depend on the observed data.\n- $\\sum_{i=1}^n \\cov(\\hat{y}_i, y_i)$  measures how much $\\hat{y}_i$ depend on the observed data.\n- If a model is very flexible (e.g., an overfitted model), the predictions are highly dependent on the observed values, meaning higher covariance.\n- as a model increases in complexity, it becomes more data-dependent, meaning the predicted values closely follow the observed ones.\n- Simpler models have lower covariance, meaning they do not adapt too closely to the specific fluctuations in the data.\n- Degrees of freedom measure the model’s ability to fit data independently.\n:::\n\n\n\n## K-nearest Neighbor Classification\n\n- Instead of taking average in regression, KNN classification uses *majority voting*:\n\n:::{.center}\n\n[*Look for the most popular class label among its neighbors*.]{.green}\n\n:::\n\n- 1NN decision boundary is a **Voronoi diagram**.\n\n:::{.center}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-knn_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=70%}\n:::\n:::\n\n:::\n\n::: notes\n\n- Voronoi tessellation. https://en.wikipedia.org/wiki/Voronoi_diagram\n- Every boundary is the middle line of any two training points.\n- In each region, there is one training point, and all possible data points in the region will be labelled as the category the training point belongs to.\n\n:::\n\n\n\n## Example: [ESL.mixture.rda](./data/ESL.mixture.rda)\n\n<!-- - An artificial data with 200 binary labels. -->\n\n- The KNN decision boundary is nonlinear.\n\n- R: `class::knn()`, [`kknn::kknn()`](https://github.com/KlausVigo/kknn), [`FNN::knn()`](https://cran.r-project.org/web/packages/FNN/index.html), [`parsnip::nearest_neighbor()`](https://parsnip.tidymodels.org/reference/nearest_neighbor.html)\n\n- Python: [`from sklearn.neighbors import KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-knn_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=70%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-knn_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=70%}\n:::\n:::\n\n:::\n\n::::\n\n\n\n## Example: [ESL.mixture.rda](./data/ESL.mixture.rda)\n\n\n:::: {.columns}\n\n::: {.column width=\"33%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](11-knn_files/figure-revealjs/unnamed-chunk-9-1.png){width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"33%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](11-knn_files/figure-revealjs/unnamed-chunk-10-1.png){width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"33%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](11-knn_files/figure-revealjs/unnamed-chunk-11-1.png){width=100%}\n:::\n:::\n\n:::\n\n::::\n\n\n\n## Confusion Matrix\n\n::: small\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nknn_fit <- class::knn(train = x, test = x, cl = y, k = 15)\ncaret::confusionMatrix(table(knn_fit, y))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n``` my_classfull\nConfusion Matrix and Statistics\n\n       y\nknn_fit  0  1\n      0 82 13\n      1 18 87\n                                          \n               Accuracy : 0.845           \n                 95% CI : (0.7873, 0.8922)\n    No Information Rate : 0.5             \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.69            \n                                          \n Mcnemar's Test P-Value : 0.4725          \n                                          \n            Sensitivity : 0.8200          \n            Specificity : 0.8700          \n         Pos Pred Value : 0.8632          \n         Neg Pred Value : 0.8286          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4100          \n   Detection Prevalence : 0.4750          \n      Balanced Accuracy : 0.8450          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n\n\n:::\n:::\n\n:::\n\n\n\n\n## Choosing K\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-line-numbers=\"false\"}\nset.seed(2025)\nlibrary(caret)\ncontrol <- trainControl(method = \"cv\", number = 10)\nknn_cvfit <- train(y ~ ., method = \"knn\", \n                   data = data.frame(\"x\" = x, \"y\" = as.factor(y)),\n                   tuneGrid = data.frame(k = seq(1, 40, 1)),\n                   trControl = control)\npar(mar = c(4, 4, 0, 0))\nplot(knn_cvfit$results$k, 1 - knn_cvfit$results$Accuracy,\n     xlab = \"K\", ylab = \"Classification Error\", type = \"b\",\n     pch = 19, col = 2)\n```\n\n::: {.cell-output-display}\n![](11-knn_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n\n## Best K\n\n<!-- # ```{r} -->\n<!-- # knn_cvfit$bestTune -->\n<!-- # ``` -->\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](11-knn_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: small\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n``` my_classfull\nConfusion Matrix and Statistics\n\n       y\nknn_fit  0  1\n      0 82 14\n      1 18 86\n                                          \n               Accuracy : 0.84            \n                 95% CI : (0.7817, 0.8879)\n    No Information Rate : 0.5             \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.68            \n                                          \n Mcnemar's Test P-Value : 0.5959          \n                                          \n            Sensitivity : 0.8200          \n            Specificity : 0.8600          \n         Pos Pred Value : 0.8542          \n         Neg Pred Value : 0.8269          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4100          \n   Detection Prevalence : 0.4800          \n      Balanced Accuracy : 0.8400          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n\n\n:::\n:::\n\n:::\n\n:::\n\n::::\n\n\n\n## Scaling and Distance Measures\n\n- By default, we use Euclidean distance ($\\ell_2$ norm) $d^2(\\bu, \\bv) = \\lVert \\bu - \\bv \\rVert_2^2 = \\sum_{j=1}^p (u_j - v_j)^2$\n\n- This measure is not *scale invariant*: Multiplying the data with a factor changes the distance!\n\n. . .\n\n- Often consider a normalized version:\n\n$$d^2(\\bu, \\bv) = \\sum_{j=1}^p \\frac{(u_j - v_j)^2}{\\sigma_j^2}$$\n\n. . .\n\n- __Mahalanobis distance__ takes the covariance structure into account\n\n$$d^2(\\bu, \\bv) = (\\bu - \\bv)' \\Sigma^{-1} (\\bu - \\bv),$$\n\n- If $\\Sigma = \\bI$, Mahalanobis = Euclidean\n- If $\\Sigma = diag(\\sigma^2_1, \\dots, \\sigma^2_p)$, Mahalanobis = normalized version\n\n\n\n## Mahalanobis distance\n\n- Red and green points have the same Euclidean distance to the center.\n\n- The red point is farther away from the center in terms of Mahalanobis distance.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-knn_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=960}\n:::\n:::\n\n::: notes\n\nIn the following plot, the red cross and orange cross have the same Euclidean distance to the center. However, the red cross is more of a \"outlier\" based on the joint distribution. The Mahalanobis distance would reflect this. \n\n:::\n\n\n\n## Example: Image Data [`ElemStatLearn::zip.train`](https://hastie.su.domains/ElemStatLearn/)\n\n- Digits 0-9 scanned from envelopes by the U.S. Postal Service\n\n- $16 \\times 16$ pixel images, totally $p=256$ variables\n\n- At each pixel, we have the gray scale as the numerical value\n\n- 1NN with Euclidean distance gives 5.6% error rate\n\n- 1NN with [tangent distance]{.green} (Simard et al., 1993) gives 2.6% error\n\n::: {.cell}\n::: {.cell-output-display}\n![](11-knn_files/figure-revealjs/unnamed-chunk-17-1.png){width=100%}\n:::\n:::\n\n\n::: {.cell}\n\n:::\n\n\n\n## Example: 3NN on Image Data\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# fit 3nn model and calculate the error\nknn.fit <- class::knn(zip.train[, 2:257], zip.test[, 2:257], zip.train[, 1], k = 3)\n# overall prediction error\nmean(knn.fit != zip.test[, 1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05480817\n```\n\n\n:::\n:::\n\n::: midi\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# the confusion matrix\ntable(knn.fit, zip.test[, 1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n``` my_class500\n       \nknn.fit   0   1   2   3   4   5   6   7   8   9\n      0 355   0   8   1   0   2   3   0   4   1\n      1   0 258   0   0   2   0   0   1   0   0\n      2   2   0 182   2   0   2   1   1   0   0\n      3   0   0   1 153   0   3   0   1   4   0\n      4   0   3   1   0 182   0   2   4   0   3\n      5   0   0   0   8   2 147   1   0   2   1\n      6   0   2   1   0   2   1 163   0   1   0\n      7   1   1   2   1   2   0   0 138   1   4\n      8   0   0   3   0   1   1   0   1 151   0\n      9   1   0   0   1   9   4   0   1   3 168\n```\n\n\n:::\n:::\n\n:::\n\n\n\n## Example: 3NN on Image Data\n\n::: {.cell}\n::: {.cell-output-display}\n![](11-knn_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\n\n\n\n## Computational Issues\n\n- Need to store the entire training data for prediction. (Lazy learner)\n\n- Needs to calculate the distance from $x_0$ to all training sample and sort them.^[R [`FNN`](https://cran.r-project.org/web/packages/FNN/FNN.pdf) package for faster computations when $n$ is large.]\n\n- Distance measures may affect accuracy.\n\n::: notes\n\n$K$NN can be quite computationally intense for large sample size because to find the nearest neighbors, we need to calculate and compare the distances to each of the data point. In addition, it is not memory friendly because we need to store the entire training dataset for future prediction. In contrast, for linear model, we only need to store the estimated $\\bbeta$ parameters. Some algorithms have been developed to search for the neighbors more efficiently. You can try the `FNN` package for these faster computations when $n$ is large.\n\n:::\n\n\n## Curse of Dimensionality\n\n- KNN does does not work well in high-dimensional space ($p \\gg n$) due to **curse of dimensionality**.\n\n> *As $p$ increases, it's getting harder to find $k$ neighbors in the input space. KNN needs to explore a large range of values along each input dimension to grab the \"neighbors\".*\n\n- The \"neighbors\" of $x_0$ are in fact far away from $x_0$, and so they may not be good predictors about the behavior of the function at $x_0$.\n\n- The method is not local anymore despite the name \"nearest neighbor\"!\n\n- In high dimensions KNN often performs worse than linear regression.\n\n\n::: notes\n\nIn high dimensions, all points tend to become equidistant from each other. This means that the difference between the nearest and farthest neighbor becomes very small, making it difficult for KNN to distinguish between close and distant points.\n:::\n\n\n## Curse of Dimensionality\n\n- Data points are uniformly spread out on $[0, 1]^p$.\n\n- In 10 dimensions we need to cover 80% of the range of each coordinate to capture 10% of the data.\n\n::: {.cell .fig-cap-location-bottom layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: ESL Fig. 2.6](./images/11-knn/curse.png){fig-align='center' width=70%}\n:::\n:::\n\n\n::: notes\n- The problem comes from the fact that the volume of space grows exponentially fast with dimension, so we may have to look quite far away in space to find our nearest neighbor.\n:::\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
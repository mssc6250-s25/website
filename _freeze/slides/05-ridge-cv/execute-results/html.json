{
  "hash": "e688a66a3dafed7507984feb21a43ac1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Ridge Regression and Cross Validation `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 640 512\" style=\"height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M128 64c0-17.7 14.3-32 32-32H320c17.7 0 32 14.3 32 32V416h96V256c0-17.7 14.3-32 32-32H608c17.7 0 32 14.3 32 32s-14.3 32-32 32H512V448c0 17.7-14.3 32-32 32H320c-17.7 0-32-14.3-32-32V96H192V256c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32h96V64z\"/></svg>`{=html}'\nsubtitle: \"MSSC 6250 Statistical Machine Learning\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"January 03 2025\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    #     - \"macros.tex\"\n    highlight-style: arrow\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n    self-contained: false\n    slide-number: c/t\n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[mssc6250-s25.github.io/website](https://mssc6250-s25.github.io/website/)\"\n    theme: [\"simple\", \"styles.scss\"]\n    echo: false\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  echo: false\n---\n\n\n\n\n# {visibility=\"hidden\"}\n\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Ridge Regression {background-color=\"#A7D5E8\"}\n\n\n\n## When Ordinary Least Squares (OLS) Doesn't Work Well: Collinearity\n- When predictors are highly correlated, $\\var(b_j)$ is much inflated.^[Although $\\var(b_j)$ is still the smallest among all linear unbiased estimators.]\n  + A tiny change in the training set causes a large change in $b_j$, leading to *unreliable* estimation and possibly prediction.\n\n. . .\n\n- ${\\bf X'X} = \\begin{bmatrix} 1 & 0.99 \\\\ 0.99 & 1 \\end{bmatrix}$ $\\quad ({\\bf X'X})^{-1} = \\begin{bmatrix}  50.3 & -49.7 \\\\ -49.7 & 50.3 \\end{bmatrix}$\n\n- $\\var(b_j) = 50.3\\sigma^2$\n  + An increase in 50-fold over the ideal case when the two regressors are orthogonal.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsolve(matrix(c(1, 0.99, 0.99, 1), 2, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]  [,2]\n[1,]  50.3 -49.7\n[2,] -49.7  50.3\n```\n\n\n:::\n:::\n\n\n\n\n::: notes\nthe eigen vector direction with the smallest eigen value\nhttps://online.stat.psu.edu/stat857/node/155/\nhttps://robjhyndman.com/hyndsight/loocv-linear-models/\n:::\n\n## When OLS Doesn't Work Well: Optimization Perspective\n\n- $\\beta_1 = \\beta_2 = 1$ and $\\cor(x_1, x_2) = 0.99$\n\n- The relatively \"flat\" valley in the objective function walks along the eigen-vector of ${\\bf X}'{\\bf X}$ having the smallest eigen-value.\n\n:::: columns\n::: {.column width=\"60%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/05-ridge-cv/unnamed-chunk-3-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neigen(\n    matrix(c(1, 0.99, 0.99, 1), \n             2, 2)\n    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\neigen() decomposition\n$values\n[1] 1.99 0.01\n\n$vectors\n      [,1]   [,2]\n[1,] 0.707 -0.707\n[2,] 0.707  0.707\n```\n\n\n:::\n:::\n\n\n:::\n::::\n\n\n::: notes\n\n- From optimization point of view, the objective function ($\\ell_2$ loss) is \"flat\" along certain directions in the parameter domain.\n- a small eigen-value in XTX makes the corresponding eigen-value large in the inverse.\n:::\n\n\n## When OLS Doesn't Work Well: Optimization Perspective\n\n- A little change in the training set perturbs the objective function. The LSEs lie on a valley centered around the truth.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/05-ridge-cv/unnamed-chunk-5-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n## When OLS Doesn't Work Well: High Variance\n\n- The optimizer could land anywhere along the valley, leading to *large variance* of the LSE.\n\n- Over many simulation runs, the LSE lies around the line of $\\beta_1 + \\beta_2 = 2$, the direction of the eigen-vector of the smallest eigen-value.\n\n\n:::: columns\n::: column\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/05-ridge-cv/unnamed-chunk-6-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::: column\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/05-ridge-cv/unnamed-chunk-7-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n:::\n\n::::\n\n\n\n\n## When OLS Doesn't Work Well: Large-$p$-small-$n$ (High Dimensional Data)\n- OLS stays well in the world of \"large-$n$-small-$p$\".\n\n- When $p > n$, ${\\bf X}'{\\bf X}$ is not invertible.\n\n- There is no unique $\\bbeta$ estimate.\n\n. . .\n\n\n[**Intuition**: *Too many degrees of freedom ($p$) to specify a model, but not enough information ($n$) to decide which one is the one.*]{style=\"font-size: 1.2em\"}\n\n- Too flexible and ends up with overfitting\n\n\n\n## Remedy for Large Variance and Large-$p$-small-$n$\n\n- Make ${\\bf X}'{\\bf X}$ invertible when $p > n$ by [regularizing]{.blue} coefficient behavior! \n\n- A good estimator *balances bias and variance well*, or *minimizes the mean square error* $$\\text{MSE}(\\hat{\\beta}) = E[(\\hat{\\beta} - \\beta)^2] = \\var(\\hat{\\beta}) + \\text{Bias}(\\hat{\\beta})^2$$\n\n- A slightly **biased** estimator $\\hat{\\bbeta}$ that has *much smaller variance and MSE* than the LSE ${\\bf b}$.\n\n:::: columns\n::: column\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/05-ridge-cv/unnamed-chunk-8-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n:::\n\n::: column\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/05-ridge-cv/unnamed-chunk-9-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n::::\n\n## Ridge Regression\n\n- Idea: Add a *ridge* (diagonal matrix) to ${\\bf X} ' {\\bf X}$.^[This is a special case of the [Tikhonov regularization](https://en.wikipedia.org/wiki/Ridge_regression#Tikhonov_regularization).]\n$$\\widehat{\\bbeta}^\\text{r} = (\\bX' \\bX + n \\lambda \\bI)^{-1} \\bX' \\by,$$\n\n. . .\n\n- To regularize coefficients, add an $\\ell_2$ **penalty** to the residual sum of squares, for some *tuning parameter* $\\lambda > 0$. \n\n\n\n$$\n\\begin{align}\n\\widehat{\\bbeta}^\\text{r} =& \\, \\argmin_{\\bbeta} \\underbrace{\\lVert \\by - \\bX \\bbeta\\rVert_2^2}_{SS_{res}} + n \\lambda \\lVert\\bbeta\\rVert_2^2\\\\\n=& \\, \\argmin_{\\bbeta} \\underbrace{\\frac{1}{n} \\sum_{i=1}^n (y_i - x_i'\\bbeta)^2}_{\\text{MSE}_{\\texttt{Tr}}} + \\lambda \\sum_{j=1}^p \\beta_j^2\\\\\n=& \\, \\argmin_{\\bbeta} \\color{green} - \\text{ goodness of fit } + \\text{ model complexity/flexibility}\n\\end{align}\n$$\n\n\n## Properties of Ridge Regression\n\n$$\n\\begin{align}\n\\widehat{\\bbeta}^\\text{r} =& \\argmin_{\\bbeta} \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i'\\bbeta)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2,\n\\end{align}\n$$\n\nProperties of ridge regression:\n\n- Has *less* degrees of freedom in the sense that [_the cost gets higher when larger coefficients are used_]{.green}.\n\n. . .\n\n- Favors small-magnitude coefficient estimates (**Shrinkage**) to avoid cost penalty. \n\n. . .\n\n- The shrinkage parameter $\\lambda$ controls the degree of penalty.\n  + $\\lambda \\rightarrow 0$: No penalty, and $\\widehat{\\bbeta}^\\text{r} = \\bf b$.\n  + $\\lambda \\rightarrow \\infty$: Unbearable penalty, and $\\widehat{\\bbeta}^\\text{r} \\rightarrow \\mathbf{0}$\n  \n  \n  \n  \n## Ridge Penalty\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n$$\\lambda \\lVert \\bbeta \\rVert^2_2 = \\lambda \\bbeta' \\bI \\bbeta$$\n\n- The penalty contour is *circle-shaped*\n\n- Force the objective function to be more *convex*\n\n- A more stable or less varying solution\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/05-ridge-cv/unnamed-chunk-10-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n\n\n\n\n## Geometrical Meaning of Ridge Regression\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: https://online.stat.psu.edu/stat508/lessons/Lesson06](./images/05-ridge-cv/ridge_pc.png){fig-align='center' width=52%}\n:::\n:::\n\n\n\n\n\n::: notes\n- Perform PCA of X\n- Project y onto the PCs\n- Shrinks the projection\n- Reassemble the PCs using all the shrunken length\n:::\n\n\n## More Convex Loss Function\n\n- Adding a ridge penalty forces the objective to be more convex due to the added eigenvalues.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neigen(matrix(c(1, 0.99, 0.99, 1), 2, 2) + diag(2))[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$values\n[1] 2.99 1.01\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/05-ridge-cv/unnamed-chunk-13-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\n::: notes\nHence, by adding this to the OLS objective function, the solution is more stable, in the sense that each time we observe a new set of data, this contour looks pretty much the same. This may be interpreted in several different ways such as: 1) the objective function is more convex and less affected by the random samples; 2) the variance of the estimator is smaller because the eigenvalues of $\\bX^\\T \\bX + n \\lambda \\bI$ are large. \n:::\n\n\n## Bias Caused by the Ridge Penalty # {visibility=\"hidden\"}\n\nNow, let's apply the same analysis on the ridge regression estimator. For the theoretical justification of this analysis, please read the [SMLR textbook](https://teazrq.github.io/SMLR/ridge-regression.html#bias-and-variance-of-ridge-regression). \nWe will set up a simulation study with the following steps, with both $\\widehat{\\bbeta}^\\text{ridge}$ and $\\widehat{\\bbeta}^\\text{ols}$:\n\n  1) Generate a set of $n = 100$ observations\n  2) Estimate the ridge estimator $\\widehat{\\bbeta}^\\text{ridge}$ with $\\lambda = 0.3$. Hence, $n \\lambda = 30$.\n  3) Repeat steps 1) and 2) $\\text{nsim} = 200$ runs\n  4) Average all estimations and compare that with the truth $\\bbeta$\n  5) Compute the variation of these estimates across all runs\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n::: notes\n\n:::\n\n\n\n## The Bias-variance Tradeoff\n  * As $\\lambda \\downarrow$, bias $\\downarrow$ and variance $\\uparrow$\n  * As $\\lambda \\uparrow$, bias $\\uparrow$ and variance $\\downarrow$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/05-ridge-cv/unnamed-chunk-15-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\n\n\n::: notes\nThis effect is gradually changing as we increase the penalty level. The following simulation shows how the variation of $\\bbeta$ changes. We show this with two penalty values, and see how the estimated parameters are away from the truth. \n\nNow, we may ask the question: is it worth it? In fact, this bias and variance will be then carried to the predicted values $x^\\T \\widehat{\\bbeta}^\\text{ridge}$. Hence, we can judge if this is beneficial from the prediction accuracy. And we need some procedure to do this. \n\n__Remark__: The bias-variance trade-off will appear frequently in this course. And the way to evaluate the benefit of this is to see if it eventually reduces the prediction error ($\\text{Bias}^2 + \\text{Variance}$ plus a term called __irreducible error__, which will be introduced in later chapter). \n:::\n\n## Lower Test MSE\n\n\n\n:::: {.columns}\n\n::: {.column width=\"38%\"}\n\n- When $b_j$ has large variance or $p > n$, ridge regression could have lower test MSE and better predictive performance.\n\n- [$\\text{MSE}_{\\texttt{Tr}}$ (purple)]{.purple}\n\n- **Squared bias (black)**\n\n- [**Variance (green)**]{.green}\n\n\n\n:::\n\n::: {.column width=\"62%\"}\n::: small\n\n\n\n::: {.cell layout-align=\"left\"}\n::: {.cell-output-display}\n![Source: ISL Figure 6.5](./images/05-ridge-cv/6_5.png){fig-align='left' width=100%}\n:::\n:::\n\n\n\n:::\n:::\n::::\n\n\n## `MASS::lm.ridge()`\n\n- The `lambda` parameter in `lm.ridge()` specifies the $n\\lambda$ in our notation.\n\n- OLS is *scale equivalent*: $X_jb_j$ remains the same regardless of how $X_j$ is scaled.\n\n- Ridge coefficient estimates can *change substantially* when multiplying a given predictor by a constant, i.e., $X_j\\hat{\\beta}^{r}_{j, \\lambda}$ depends on $\\lambda$, the scaling of $X_j$, and even the scaling of *other* predictors.\n\n- [__Standardize all predictors!__]{.green}^[In practice, if the intercept is not our interest, we also standardize the response.]\n\n. . .\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(mtcars, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               mpg cyl disp  hp drat   wt qsec vs am gear carb\nMazda RX4     21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\nMazda RX4 Wag 21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\nDatsun 710    22.8   4  108  93 3.85 2.32 18.6  1  1    4    1\n```\n\n\n:::\n\n```{.r .cell-code}\n(fit <- MASS::lm.ridge(mpg ~ ., data = mtcars, lambda = 1)) ## ridge fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              cyl     disp       hp     drat       wt     qsec       vs \n16.53766 -0.16240  0.00233 -0.01493  0.92463 -2.46115  0.49259  0.37465 \n      am     gear     carb \n 2.30838  0.68572 -0.57579 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n::: notes\n\n:::\n\n## Scaling Issues of `lm.ridge()`\n\n- `coef(fit)` transforms these back to the *original* scale. \n\n- `fit$coef` shows the coefficients of the standardized predictors.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncoef(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              cyl     disp       hp     drat       wt     qsec       vs \n16.53766 -0.16240  0.00233 -0.01493  0.92463 -2.46115  0.49259  0.37465 \n      am     gear     carb \n 2.30838  0.68572 -0.57579 \n```\n\n\n:::\n\n```{.r .cell-code}\nfit$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   cyl   disp     hp   drat     wt   qsec     vs     am   gear   carb \n-0.285  0.285 -1.008  0.487 -2.370  0.866  0.186  1.134  0.498 -0.915 \n```\n\n\n:::\n:::\n\n\n\n\n## Scaling Issues of `lm.ridge()`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   cyl   disp     hp   drat     wt   qsec     vs     am   gear   carb \n-0.285  0.285 -1.008  0.487 -2.370  0.866  0.186  1.134  0.498 -0.915 \n```\n\n\n:::\n:::\n\n\n\n- `lm.ridge()` uses $n$ instead of $n-1$ when calculating the standard deviation.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# each column has mean 0 and var 1\nX <- scale(data.matrix(mtcars[, -1]), center = TRUE, scale = TRUE)\n\n# center y but not scaling\ny <- scale(mtcars$mpg, center = TRUE, scale = FALSE)\n```\n:::\n\n\n\n\n<br>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# lambda = 1\nmy_ridge_coef <- solve(t(X) %*% X + diag(ncol(X))) %*% t(X) %*% y\nt(my_ridge_coef)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        cyl disp    hp  drat    wt qsec   vs   am  gear   carb\n[1,] -0.294 0.27 -1.02 0.495 -2.39 0.87 0.19 1.15 0.505 -0.937\n```\n\n\n:::\n:::\n\n\n\n## Scaling Issues of `lm.ridge()`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   cyl   disp     hp   drat     wt   qsec     vs     am   gear   carb \n-0.285  0.285 -1.008  0.487 -2.370  0.866  0.186  1.134  0.498 -0.915 \n```\n\n\n:::\n:::\n\n\n\n<br>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# use n instead of (n-1) for standardization\nn <- nrow(X); X <- X * sqrt(n / (n - 1))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n        cyl  disp    hp  drat    wt  qsec    vs   am  gear   carb\n[1,] -0.285 0.285 -1.01 0.487 -2.37 0.866 0.186 1.13 0.498 -0.915\n```\n\n\n:::\n:::\n\n\n\n::: notes\nMore discussion at https://stats.stackexchange.com/questions/288754/lm-ridge-returns-different-results-that-are-from-manual-calculation\n:::\n\n\n## Ridge Trace\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nridge_fit <- lm.ridge(mpg ~ ., data = mtcars, lambda = 0:40)\nmatplot(coef(ridge_fit)[, -1], type = \"l\", xlab = \"Lambda\", ylab = \"Coefficients\")\ntext(rep(1, 10), coef(ridge_fit)[1,-1], colnames(mtcars)[2:11])\n```\n\n::: {.cell-output-display}\n![](images/05-ridge-cv/unnamed-chunk-25-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n- Select a value of $\\lambda$ at which the ridge estimates are stable.\n\n\n\n::: notes\n\n:::\n\n\n\n\n## Methods for Choosing $\\lambda$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nMASS::select(ridge_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nmodified HKB estimator is 2.59 \nmodified L-W estimator is 1.84 \nsmallest value of GCV  at 15 \n```\n\n\n:::\n:::\n\n\n\n- Hoerl, Kennard, and Baldwin (1975): $\\lambda \\approx \\frac{p \\hat{\\sigma}^2}{{\\bf b}'{\\bf b}}$\n\n- Lawless and Wang (1976): $\\lambda \\approx \\frac{np \\hat{\\sigma}^2}{{\\bf b'X}'{\\bf Xb}}$\n\n- [Golub, Health, and Wahba (1979)](https://pages.stat.wisc.edu/~wahba/ftp1/oldie/golub.heath.wahba.pdf): Generalized Cross Validation\n\n\n## Cross Validation\n\n- Cross Validation (CV) is a **resampling** method. \n\n- Resampling methods refit a model of interest to data sampled from the training set.\n\n- CV can be used to \n  + estimate the test error when there is no test data. _(Model assessment)_\n  + select the tuning parameters that controls the model complexity/flexibility. _(Model selection)_\n\n\n\n\n::: notes\nHence, by adding this to the OLS objective function, the solution is more stable, in the sense that each time we observe a new set of data, this contour looks pretty much the same. This may be interpreted in several different ways such as: 1) the objective function is more convex and less affected by the random samples; 2) the variance of the estimator is smaller because the eigenvalues of $\\bX^\\T \\bX + n \\lambda \\bI$ are large. \n:::\n\n\n## \n\n::: {.algo} \n\n**$k$-Fold Cross Validation**\n\n1. Randomly divide the training set into $k$ *folds*, of approximately equal size.\n\n2. Use 1 fold for validation to compute MSE, and the remaining $k - 1$ partitions for training.\n\n3. Repeat $k$ times. Each time a different fold is treated as a validation set.\n\n4. Average $k$ metrics, $\\text{MSE}_{CV} = \\frac{1}{k}\\sum_{i=1}^k\\text{MSE}_i$.\n\n5. Use the CV estimate $\\text{MSE}_{CV}$ to select the \"best\" tuning parameter.\n:::\n\n\n\n::: small\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Five-fold cross validation. Source: Data science in a box](./images/05-ridge-cv/cross-validation.png){fig-align='center' width=45%}\n:::\n:::\n\n\n\n:::\n\n::: notes\nCan compute other performance measures.\n:::\n\n## $k$-Fold Cross Validation in `R` {visibility=\"hidden\"}\n\n- [`caret`](https://topepo.github.io/caret/index.html) package: **C**lassification **A**nd **RE**gression **T**raining ^[Be careful that not all models are available in `caret`. `caret` do not use `MASS::lm.ridge`, but the `elasticnet` package to fit ridge regression.]\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n::: notes\nCross-validation can be setup using the `caret` package. However, you should be careful that not all models are available in `caret`, and you should always check the documentation to see how to implement them. For example, if you use `method = \"ridge\"`, they do not use `lm.ridge` to fit the model, instead, they use a package called `elasticnet`, which can do the same job. However, the definition of parameters may vary. Hence, it is always good to check the main [help pages](https://topepo.github.io/caret/) for the package. We will use the caret package later for other models. \nhttps://rsample.tidymodels.org/reference/vfold_cv.html\n:::\n\n## [`glmnet`](https://glmnet.stanford.edu/index.html) package  ![](images/05-ridge-cv/glmnet_logo.png){.absolute left=\"450\" top=\"0\" width=\"70\"}\n\n- The parameter `alpha` controls the ridge (`alpha = 0`) and lasso (`alpha = 1`) penalties.\n\n- Supply a *decreasing* sequence of `lambda` values.\n\n- `lm.ridge()` use $SS_{res}$ and $n\\lambda$, while `glmnet()` use $\\text{MSE}_{\\texttt{Tr}}$ and $\\lambda$.\n\n- Argument `x` should be a matrix.\n\n\n:::: columns\n::: column\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglmnet(x = data.matrix(mtcars[, -1]), \n       y = mtcars$mpg, \n       alpha = 0, \n       lambda = 5:0/nrow(mtcars))\n```\n:::\n\n\n:::\n\n\n::: column\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm.ridge(formula = mpg ~ ., \n         data = mtcars, \n         lambda = 5:0/nrow(mtcars))\n```\n:::\n\n\n:::\n\n\n:::{.callout-note style=\"font-size: 1.2em\"}\n- `lm.ridge()` and `glmnet()` coefficients do not match exactly, specially when transforming back to original scale.\n- No need to worry too much as we focus on predictive performance.\n:::\n\n::::\n\n\n\n::: notes\nhttps://stats.stackexchange.com/questions/74206/ridge-regression-results-different-in-using-lm-ridge-and-glmnet\n:::\n\n\n\n## \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/05-ridge-cv/unnamed-chunk-31-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n##\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/05-ridge-cv/unnamed-chunk-32-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n\n## $k$-Fold Cross Validation using `cv.glmnet()`^[There are other ways to do CV for ridge regression in `R`, for example, the [`caret`](https://topepo.github.io/caret/) (**C**lassification **A**nd **RE**gression **T**raining) package and the [`rsample`](https://rsample.tidymodels.org/reference/vfold_cv.html) package in [`tidymodels`](https://www.tidymodels.org/) ecosystem.]\n\n- The $\\lambda$ values are automatically selected, on the *$\\log_{e}$* scale.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nridge_cv_fit <- cv.glmnet(x = data.matrix(mtcars[, -1]), y = mtcars$mpg, alpha = 0,\n                          nfolds = 10, type.measure = \"mse\")\nplot(ridge_cv_fit$glmnet.fit, \"lambda\")\n```\n\n::: {.cell-output-display}\n![](images/05-ridge-cv/unnamed-chunk-33-1.png){fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n\n::: notes\n- Why s and not lambda? In case we want to allow one to specify the model size in other ways in the future.\ns: Value(s) of the penalty parameter lambda at which predictions are required. Default is the entire sequence used to create the model.\n\n:::\n\n\n\n## Determine $\\lambda$\n\n:::: columns\n::: column\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(ridge_cv_fit)\n```\n\n::: {.cell-output-display}\n![](images/05-ridge-cv/unnamed-chunk-34-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n:::\n\n\n::: column\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nridge_cv_fit$lambda.min\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.75\n```\n\n\n:::\n\n```{.r .cell-code}\n# largest lambda s.t. error is within 1 s.e of the min\nridge_cv_fit$lambda.1se \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 16.1\n```\n\n\n:::\n\n```{.r .cell-code}\ncoef(ridge_cv_fit, s = \"lambda.min\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                  s1\n(Intercept) 21.11734\ncyl         -0.37134\ndisp        -0.00525\nhp          -0.01161\ndrat         1.05477\nwt          -1.23420\nqsec         0.16245\nvs           0.77196\nam           1.62381\ngear         0.54417\ncarb        -0.54742\n```\n\n\n:::\n:::\n\n\n:::\n::::\n\n\n::: notes\ncoef(fit2, s = \"lambda.1se\")\nThis plots the cross-validation curve (red dotted line) along with upper and lower standard deviation curves along the 𝜆\n sequence (error bars). Two special values along the 𝜆\n sequence are indicated by the vertical dotted lines. lambda.min is the value of 𝜆\n that gives minimum mean cross-validated error, while lambda.1se is the value of 𝜆\n that gives the most regularized model such that the cross-validated error is within one standard error of the minimum.\n:::\n\n\n\n## Generalized Cross-Validation\n- The generalized cross-validation (GCV) is a modified version of the **leave-one-out CV** ($n$-fold CV).\n\n- The GCV criterion is given by $$\\text{GCV}(\\lambda) = \\frac{1}{n}\\sum_{i=1}^n \\left[ \\frac{y_i - x_i' \\widehat{\\bbeta}^\\text{r}_\\lambda}{1 - \\frac{\\Trace(\\bS_\\lambda)}{n}} \\right]^2$$\n\nwhere $\\bS_\\lambda$ is the _hat matrix_ corresponding to the ridge regression:\n\n$$\\bS_\\lambda = \\bX (\\bX' \\bX + n\\lambda \\bI)^{-1} \\bX'$$\n\n\n\n\n::: notes\nThe interesting fact about leave-one-out CV in the linear regression setting is that we do not need to explicitly fit all leave-one-out models.\n\n- ESL p. 244\n\n- lm.ridge code\n:::\n\n\n## Generalized Cross-Validation\n\n:::: columns \n::: {.column width=\"49%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(ridge_fit$lambda, \n     ridge_fit$GCV, \n     type = \"l\", col = \"darkgreen\", \n     ylab = \"GCV\", xlab = \"Lambda\", \n     lwd = 3)\n```\n\n::: {.cell-output-display}\n![](images/05-ridge-cv/unnamed-chunk-36-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"49%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nidx <- which.min(ridge_fit$GCV)\nridge_fit$lambda[idx]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 15\n```\n\n\n:::\n\n```{.r .cell-code}\nround(coef(ridge_fit)[idx, ], 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb \n21.13 -0.37 -0.01 -0.01  1.05 -1.23  0.16  0.77  1.62  0.54 -0.55 \n```\n\n\n:::\n:::\n\n\n:::\n::::\n\n\nSelect the best $\\lambda$ that produces the smallest GCV.\n\n\n::: notes\nYou can clearly see that the GCV decreases initially, as $\\lambda$ increases, this is because the reduced variance is more beneficial than the increased bias. However, as $\\lambda$ increases further, the bias term will eventually dominate and causing the overall prediction error to increase. The fitted MSE under this model is \n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n:::\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
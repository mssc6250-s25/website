{
  "hash": "c625b039f866b9a0a157774ccd4dde12",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Gaussian Processes ♾️\"\nsubtitle: \"MSSC 6250 Statistical Machine Learning\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"March 25 2025\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    #     - \"macros.tex\"\n    highlight-style: github\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t\n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[mssc6250-s25.github.io/website](https://mssc6250-s25.github.io/website/)\"\n    theme: [\"simple\", \"styles.scss\"]\n    echo: false\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n---\n\n\n# {visibility=\"hidden\"}\n\n\n\n\n\\def\\cD{{\\cal D}}\n\\def\\cL{{\\cal L}}\n\\def\\cX{{\\cal X}}\n\\def\\cF{{\\cal F}}\n\\def\\cH{{\\cal H}}\n\\def\\bA{\\mathbf{A}}\n\\def\\bB{\\mathbf{B}}\n\\def\\bX{\\mathbf{X}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bU{\\mathbf{U}}\n\\def\\bD{\\mathbf{D}}\n\\def\\bV{\\mathbf{V}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\bY{\\mathbf{Y}}\n\\def\\bZ{\\mathbf{Z}}\n\\def\\bK{\\mathbf{K}}\n\\def\\bx{\\mathbf{x}}\n\\def\\by{\\mathbf{y}}\n\\def\\bs{\\mathbf{s}}\n\\def\\br{\\mathbf{r}}\n\\def\\bu{\\mathbf{u}}\n\\def\\be{\\mathbf{e}}\n\\def\\bv{\\mathbf{v}}\n\\def\\bp{\\mathbf{p}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bz{\\mathbf{z}}\n\\def\\bzero{\\mathbf{0}}\n\\def\\balpha{\\boldsymbol \\alpha}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\btheta{\\boldsymbol \\theta}\n\\def\\bSigma{\\boldsymbol \\Sigma}\n\\def\\bxi{\\boldsymbol \\xi}\n\\def\\bmu{\\boldsymbol \\mu}\n\\def\\bep{\\boldsymbol \\epsilon}\n\\def\\T{\\text{T}}\n\\def\\Trace{\\text{Trace}}\n\\def\\Cov{\\text{Cov}}\n\\def\\Corr{\\text{Corr}}\n\\def\\Var{\\text{Var}}\n\\def\\E{\\text{E}}\n\\def\\pr{\\text{pr}}\n\\def\\Prob{\\text{P}}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\n\n\n\n\n\n\n\n# Gaussian Variables\n\n\n## Univariate Guassian\n\n- $Y \\sim N(\\mu, \\sigma^2)$\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/12-gp/unnamed-chunk-2-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n<!-- ## Why Normal? -->\n\n<!-- We love Gaussian distribution because it has lots of great properties. -->\n\n<!-- . . . -->\n\n<!-- - Central limit theorem -->\n\n<!-- - If $Y$ and Z are jointly normally distributed and are uncorrelated, then they are independent. -->\n\n<!-- - Maximum entropy: $N(\\mu, \\sigma^2)$ has maximum entropy of any distribution with mean $\\mu$ and variance $\\sigma^2$ ([Principle of maximum entropy](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy#:~:text=The%20principle%20of%20maximum%20entropy,proposition%20that%20expresses%20testable%20information).) -->\n\n<!-- -  -->\n\n\n## Multivariate Guassian\n\n- \"Multivariate\" $=$ two or more random variables\n\n- $\\bY \\in \\mathbb{R}^d \\sim N_d\\left(\\bmu, \\bSigma\\right)$ \n\n. . .\n\n\n- Bivariate Gaussian ($d=2$):\n\n$$\\bY = \\begin{pmatrix} Y_1 \\\\ Y_2 \\end{pmatrix}$$ $$\\bmu = \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}$$ $$\\bSigma = \\begin{pmatrix} \\sigma_1^2  & \\rho_{12}\\sigma_1\\sigma_2\\\\ \\rho_{12}\\sigma_1\\sigma_2 & \\sigma_2^2 \\end{pmatrix}$$\n\n\n:::notes\n<!-- has a multivariate Gaussian distribution with -->\n\n<!-- - mean vector $\\bmu \\in \\mathbb{R}^d$ -->\n\n<!-- - covariance matrix $\\bSigma \\in \\mathbb{R}^{d\\times d}$ -->\n:::\n\n\n\n##\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/12-gp/unnamed-chunk-3-1.png){fig-align='center' width=90%}\n:::\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n\n$$\\bmu = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$ $$\\bSigma = \\begin{pmatrix}1 & 0\\\\ 0 & 1 \\end{pmatrix}$$\n:::\n\n::::\n\n\n\n##\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/12-gp/unnamed-chunk-4-1.png){fig-align='center' width=90%}\n:::\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n\n$$\\bmu = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$$ $$\\bSigma = \\begin{pmatrix}1 & 0\\\\ 0 & 0.2 \\end{pmatrix}$$\n:::\n\n::::\n\n\n\n##\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/12-gp/unnamed-chunk-5-1.png){fig-align='center' width=90%}\n:::\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n\n$$\\bmu = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$ $$\\bSigma = \\begin{pmatrix}1 & 0.9\\\\ 0.9 & 1 \\end{pmatrix}$$\n:::\n\n::::\n\n\n\n\n## Three or More Variables\n\n- Hard to visualize in dimensions $> 2$, so stack points next to each other.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/12-gp/unnamed-chunk-6-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n::: notes\n\ny1 and y2 are highly correlated, y1 and y2 have similar values, and so when we put them next to each other, the lines do not go up or down that much. Instead, the lines are quite horizontal.\n\n:::\n\n\n\n\n##\n\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/12-gp/unnamed-chunk-7-1.png){fig-align='center' width=90%}\n:::\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n\n$d = 5$ \n\n::: {.small}\n\n$$\\bmu = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0\\\\0\\\\0\\end{pmatrix} \\quad \\quad \\bSigma = \\begin{pmatrix}1 & 0.99 & 0.98 & 0.97 & 0.96\\\\ 0.99 & 1 & 0.99 & 0.98 & 0.97 \\\\ 0.98 & 0.99 & 1 & 0.99 & 0.98 \\\\ 0.97 & 0.98 & 0.97 & 1 & 0.99\\\\0.96 & 0.97 & 0.98 & 0.99 & 1\n\\end{pmatrix}$$\n\n:::\n\n- Each line is one sample (path).\n\n\n:::\n\n::::\n\n\n\n\n##\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/12-gp/unnamed-chunk-8-1.png){fig-align='center' width=90%}\n:::\n:::\n\n:::\n\n\n\n::: {.column width=\"40%\"}\n\n$d = 50$ \n\n::: {.small}\n\n$$\\bmu = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\0\\\\0\\end{pmatrix} \\quad \\quad \\bSigma = \\begin{pmatrix}\n1 & 0.99 & 0.98 & 0.97 & 0.96 & \\cdots \\\\ \n0.99 & 1 & 0.99 & 0.98 & 0.97 & \\cdots\\\\ \n0.98 & 0.99 & 1 & 0.99 & 0.98 & \\cdots\\\\ \n0.97 & 0.98 & 0.97 & 1 & 0.99 & \\cdots\\\\\n0.96 & 0.97 & 0.98 & 0.99 & 1 & \\cdots \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots &  \\vdots &\n\\end{pmatrix}$$\n\n:::\n\n- Each line is one sample (path).\n\n- Think of **Gaussian processes** as an *infinite dimensional* distribution\nover functions\n  + all we need to do is change notation\n  \n:::\n\n::::\n\n\n\n# Gaussian Processes\n\n\n## Gaussian Processes \n\n- A **stochastic process** $f(x), x \\in \\mathcal{X} \\subset \\mathbb{R}^D$, is a\nfunction whose values are random variables, for any value of $x$.\n\n- Usually for $D = 1$, the process is a *temporal* process, and for $D > 1$,\nit is referred to as a *spatial* process.\n\n. . .\n\n- A **Gaussian process** (GP) is a process where all finite-dimensional distributions are multivariate Gaussian, for any choice of $n$ and $x_1\\dots, x_n \\in \\mathbb{R}^D$:\n\n$$f(x_1), \\dots, f(x_n) \\sim N_n\\left(\\bmu, \\bSigma \\right)$$\n\n- Write $f(\\cdot) \\sim GP$ to denote that the *function* $f()$ is a GP.\n\n\n##\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/12-gp/unnamed-chunk-9-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n##\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/12-gp/unnamed-chunk-10-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n## Mean and Covariance Function\n\n- To fully specify a Gaussian distribution we need the mean and covariance, $Y \\sim N(\\mu, \\Sigma)$\n\n. . .\n\n- To fully specify a Gaussian process we need the [**mean and covariance function**]{.green}, \n$$f(\\cdot) \\sim GP\\left(m(\\cdot), k(\\cdot, \\cdot)\\right)$$ where\n\n$$m(x) = \\E(f(x))$$\n$$k(x, x') = \\Cov(f(x), f(x'))$$\n\n\n. . .\n\n- Popular choices of $m(\\cdot)$ are $m(x) = 0$ or $m(x) = \\text{const}$ for all $x$, or $m(x) = \\beta'x$\n\n- Care more about the covariance function or **kernel function** as it governs the how the process looks like by defining the *similarity* between data points.\n\n\n\n\n## Covariance Function Kernel\n\n$$\\Cov(f(x), f(x')) = k(x, x')$$\n\n- $k(x, x')$ must be a positive semi-definite function, leading to valid covariance matrices\n  + Given locations $x_1, \\dots, x_n$, the $n \\times n$ Gram matrix $K$ with $K_{ij} = k(x_i, x_j)$ must be a positive semi-definite matrix.\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"80%\"}\n\n- Often assume $k(x, x')$ is a function of only the *distance between locations*: $$\\Cov(f(x), f(x')) = k(\\|x-x'\\|) = k(r)$$ \n  + the GP is a **stationary process**.\n  + the covariance function is **isotropic**.\n\n:::\n\n\n::: {.column width=\"20%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/12-gp/circle.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n- The covariance function determines the nature of the GP, hypothesis space of functions.\n\n::::\n\n\n\n## Squared Exponential (SE) Kernel\n\n$$k(x, x' \\mid \\tau, h) = \\tau^2 \\exp\\left(-\\frac{(x - x')^2}{2h^2} \\right)$$\n\n. . .\n\n\n:::: {.columns}\n\n::: {.column width=\"65%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/12-gp/unnamed-chunk-12-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n\n::: {.column width=\"35%\"}\n\n$$k(x, x') = \\exp\\left(-\\frac{1}{2}(x - x')^2 \\right)$$\n\n:::\n\n::::\n\n\n## Squared Exponential (SE) Kernel\n\n\n:::: {.columns}\n\n::: {.column width=\"65%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/12-gp/unnamed-chunk-13-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"35%\"}\n\n$$k(x, x') = \\exp\\left(-\\frac{1}{2}\\frac{(x - x')^2}{ {\\color{red}{0.25}}^2} \\right)$$\n\n\n- The parameter $h$ is the **characteristic length-scale** that controls the number of level-zero upcrossings.\n\n:::\n\n::::\n\n\n## Squared Exponential (SE) Kernel\n\n:::: {.columns}\n\n::: {.column width=\"65%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/12-gp/unnamed-chunk-14-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"35%\"}\n\n$$k(x, x') = \\exp\\left(-\\frac{1}{2}\\frac{(x - x')^2}{ {\\color{red}{4}}^2} \\right)$$\n\n:::\n\n::::\n\n\n\n## Squared Exponential (SE) Kernel\n\n:::: {.columns}\n\n::: {.column width=\"65%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/12-gp/unnamed-chunk-15-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"35%\"}\n\n$$k(x, x') = {\\color{red}{10^2}}\\exp\\left(-\\frac{1}{2}(x - x')^2 \\right)$$\n\n- The parameter $\\tau$ is the variance of $f(x)$ that controls the vertical variation of the process.\n\n:::\n\n::::\n\n\n## Exponential Kernel\n\n$$k(x, x' \\mid \\tau, h) = \\tau^2 \\exp\\left(-\\frac{|x - x'|}{h} \\right)$$\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/12-gp/unnamed-chunk-16-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Brownian Motion\n\n$$k(x, x') = \\min(x, x')$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/12-gp/unnamed-chunk-17-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n## White Noise\n\n$$k(x, x') = \\begin{cases} 1 & \\quad \\text{if } x = x' \\\\\n0 & \\quad \\text{otherwise}\n\\end{cases}$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/12-gp/unnamed-chunk-18-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n## Why GP?\n\n- The GP inherits its properties primarily from the covariance function $k$:\n  + Smoothness\n  + Differentiability\n  + Variance\n\n. . .\n\n- Sums of Gaussians are Gaussian.\n\n. . .\n\n- Marginal distributions of multivariate Gaussians are still Gaussian.\n\n. . .\n\n- Any affine transformation of a Gaussian is a Gaussian.\n\n. . .\n\n- Conditional distributions are still Gaussian.\n\n::: {.midi}\n\n$$\\bY = \\begin{pmatrix} \\bY_1 \\\\ \\bY_2 \\end{pmatrix} \\sim N\\left(\\bmu,  \\bSigma \\right), \\quad \\bmu = \\begin{pmatrix} \\bmu_1 \\\\ \\bmu_2 \\end{pmatrix},  \\quad \\bSigma = \\begin{pmatrix} \\Sigma_{11}  & \\Sigma_{12}\\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}$$\n$$(\\bY_2 \\mid \\by_1 = \\by_1) \\sim N\\left(\\bmu_2 + \\Sigma_{21} \\Sigma_{11}^{-1} (\\by_1 - \\bmu_1), \\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12}\\right)$$\n\n:::\n\n\n\n## Conditional Updates of Gaussian Processes\n\n- If $f(\\cdot) \\sim GP$, \n\n$$f(x_1), \\dots, f(x_n), f(x^*) \\sim N(\\bmu, \\bSigma)$$\n\n- If we observe its value at $x_1, \\dots, x_n$, then\n\n$$f(x^*) \\mid f(x_1), \\dots, f(x_n) \\sim N(\\bmu^*, \\bSigma^*)$$ where $\\bmu^*$ and $\\bSigma^*$ are as on the previous slide.\n\n- We still believe $f$ is a GP even we've observed its value\nat a number of locations.\n\n\n\n# Gaussian Process Regression\n\n\n## Bayesian Conditioning Updates of GP: Prior\n\n- Instead of assigning priors to parameters in the regression function, we assign a function prior to the regression function: \n\n$$f(\\cdot) \\sim GP(0, k(\\cdot, \\cdot))$$\n\n- For any points $x_1, \\dots, x_n, x^*$,\n\n$$f(x_1), \\dots, f(x_n), f(x^*) \\sim N\\left(0, \\bSigma \\right)$$\n\n$$\\bSigma = \\left(\\begin{array}{ccc|c}\nk(x_1, x_1) & \\cdots & k(x_1, x_n) & k(x_1, x^*) \\\\\n\\vdots &  & \\vdots & \\vdots \\\\\nk(x_n, x_1) & \\cdots & k(x_n, x_n) & k(x_n, x^*) \\\\\n\\hline\nk(x^*, x_1) & \\cdots & k(x^*, x_n) & k(x^*, x^*)\n\\end{array} \\right) = \n\\left(\\begin{array}{ccc|c}\n &  &  &  \\\\\n & K &  & K_* \\\\\n &  &  &  \\\\\n\\hline\n & K_*^T &  & K_{**}\n\\end{array} \\right) $$\n\n\n\n## Bayesian Conditioning Updates of GP: Posterior\n\n- Given observed information $f(x_1), \\dots, f(x_n)$\n\n$$f(x^*) \\mid f(x_1), \\dots, f(x_n) \\sim N\\left( \\bmu^*, \\bSigma^*\\right)$$ where\n\n$$\\bmu^* = K_{*}^TK^{-1}{\\bf f}$$ with ${\\bf f} = \\left(f(x_1), \\dots, f(x_n)\\right)^T$\n\n\n$$\\bSigma^* = K_{**} - K_{*}^TK^{-1}K_{*}$$\n\n\n\n## No Noise/Nugget - Interpolation\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/12-gp/unnamed-chunk-19-1.png){fig-align='center' width=100%}\n:::\n:::\n\n- [-----]{.blue}: posterior mean $\\bmu^*$\n\n- [-----]{.red}: 95% posterior predictive interval $\\bmu^* \\pm 1.96 \\bSigma^*$\n\n- So far we treat $f(x)$, the function values as data points.\n\n- There is no noise, and every posterior curve interpolates data points.\n\n\n\n\n## Noisy Observations/with Nugget - GP Regression (GPR)\n\n- In reality, we don't or can't observe $f(x)$ and like to estimate it.\n\n$$\\begin{align} y_i &= f(x_i) + \\epsilon_i, \\quad \\epsilon_i\\sim N(0, \\sigma^2) \\\\\nf(\\cdot) &\\sim GP(0, k(\\cdot, \\cdot; \\theta))\\end{align}$$\n\n. . .\n\n$$y_1, \\dots, y_n, f(x^*) \\sim N\\left(0, \\bSigma \\right), \\quad \\bSigma = \n\\left(\\begin{array}{ccc|c}\n &  &  &  \\\\\n & K + \\sigma^2I &  & K_* \\\\\n &  &  &  \\\\\n\\hline\n & K_*^T &  & K_{**}\n\\end{array} \\right) $$\n\n. . .\n\n$$f(x^*) \\mid y_1, \\dots, y_n \\sim N\\left( \\bmu^*, \\bSigma^*\\right), \\quad \\bmu^* = K_{*}^T{\\color{red}{(K + \\sigma^2I)^{-1}}}{\\bf y}, \\quad \\bSigma^* = K_{**} - K_{*}^T{\\color{red}{(K + \\sigma^2I)^{-1}}}K_{*}$$ with ${\\bf y} = \\left(y_1, \\dots, y_n\\right)^T$\n\n\n## Noisy Observations/with Nugget - GP Regression (GPR)\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/12-gp/unnamed-chunk-20-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Hyperparameter Tuning\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/12-gp/hyperparameter.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n## Empirical Bayes \n\n- Uses the observed data to estimate parameters $\\theta = (\\sigma^2, \\tau^2, h)$\n\n- Find an empirical Bayes estimate for $\\theta$ from the **marginal likelihood** \n\n$$ p\\left( \\by \\mid \\theta \\right) = \\int p\\left( \\by \\mid {\\bf f} \\right) p\\left( {\\bf f} \\mid \\theta \\right) \\, d {\\bf f} = N\\left(\\mathbf{0}, K(\\tau, h)+\\sigma^2I \\right)$$\n\n- $\\hat{\\theta}_{EB} = \\argmax_{\\theta}\\log p\\left( \\by \\mid \\theta \\right)$.\n\n\n\n## Full Bayesian Inference\n\n$$\\begin{align}\ny_i &= f(x_i) + \\epsilon_i, \\, \\, \\epsilon_i \\stackrel{\\rm iid}{\\sim} N(0, \\sigma^2), \\quad i = 1, \\dots, n,\\\\\nf(\\cdot) &\\sim GP\\left(\\mu, k(\\cdot, \\cdot)\\right),\\,\\, \\Cov(f(x_i), f(x_j)) = k(x_i, x_j)\\\\\n\\sigma^2 &\\sim IG(a_{\\sigma}, b_{\\sigma})\\\\\n\\tau^2 &\\sim IG(a_{\\tau}, b_{\\tau})\\\\\nh &\\sim Ga(a_{h}, b_{h})\\\\\n\\mu & \\sim N(0, b_{\\mu})\n\\end{align}$$\n\n- The model is Gibbsable, or the Metropolis-Hastings algorithm can be\nused when ${\\bf f}$ is integrated out.\n\n\n\n\n# Gaussian Process Classification\n\n\n\n\n::: notes\n- PML Ch 17\n- GPML Ch 3\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
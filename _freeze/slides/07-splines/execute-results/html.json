{
  "hash": "f4fce11216f59e29d1dc299321d6467d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Splines and General Additive Models 〰️\"\nsubtitle: \"MSSC 6250 Statistical Machine Learning\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"February 10 2025\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    #     - \"macros.tex\"\n    highlight-style: github\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t\n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[mssc6250-s25.github.io/website](https://mssc6250-s25.github.io/website/)\"\n    theme: [\"simple\", \"styles.scss\"]\n    echo: false\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    code-line-numbers: true\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n  echo: false\n---\n\n\n# {visibility=\"hidden\"}\n\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bw{\\mathbf{w}}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\bW{\\mathbf{W}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n\n\n\n# Nonlinear Relationships\n\n## When $y$ and $x$ are Not Linearly Associated\n\n- Linear models can describe *non-linear* relationship.\n\n- [**Feature engineering**](https://math4780-f23.github.io/website/slides/10-diag-linearity.html#/title-slide): When $y$ and $x$ are not linearly associated, we *transform $x$ (sometimes also $y$) so that $y$ and the transformed $x$ become linearly related*. \n\n. . .\n\n- Popular methods\n  + Basis function approach\n    * [Polynominal regression (MSSC 5780, ISL  7.1)](https://math4780-f23.github.io/website/slides/11-poly-reg.html#/title-slide)\n    * [Piecewise regression (MSSC 5780, ISL 7.3, 7.4)](https://math4780-f23.github.io/website/slides/11-poly-reg.html#/piecewise-polynomial-regression)\n    * [__Regression splines__]{.green}\n  + [Local regression (MSSC 5780, ISL 7.6)](https://math4780-f23.github.io/website/slides/12-nonpara-reg.html#/title-slide)\n  + [__Smoothing splines__]{.green}\n  + [__General additive models (GAMs)__]{.green}\n  \n\n\n\n## Polynomial Regression\n\n- The **$d$th-order (degree)** polynomial model in **one** variable is\n$$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_dx^d + \\epsilon$$\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/07-splines/unnamed-chunk-2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n- We extend the simple linear regression by mapping $x$ to $(x, x^2, \\dots, x^d)$.\n\n::: notes\n- *\"Bayesian Deep Learning and a Probabilistic Perspective of Generalization\"* Wilson and Izmailov (2020) for the rationale of choosing a super high-order polynomial as the regression model.\n- Extra flexibility produces undesirable results at the boundaries.\nGenerally speaking, it is unusual to use d greater than 3 or 4 because for large values of d, the polynomial curve can become overly flexible and can take on some very strange shapes.\nThis is especially true near the boundary of the X variable.\n- centering, collinearity\n:::\n\n\n\n\n## [Basis Function Approach](https://www.cs.princeton.edu/courses/archive/fall18/cos324/files/basis-functions.pdf)\n\n- A set of **basis functions** or transformations that can be applied to a variable $x$:$$[b_1(x), b_2(x), \\dots , b_K(x)]$$\n- **(Adaptive) basis function model**:\n$$y = \\beta_0 + \\beta_1 b_1(x) + \\beta_2 b_2(x) + \\cdots + \\beta_K b_K(x) + \\epsilon$$\n\n:::{.question}\nIs polynomial regression a basis function approach?\n:::\n\n. . .\n\n- *Monomial basis*: $\\phi(x) = [1, x, x^2, \\dots, x^K, \\dots]$\n- *Fourier series basis*: $\\phi(x) = [1, \\cos(\\omega_1x + \\psi_1), \\cos(\\omega_2x + \\psi_2), \\dots]$\n- [*B-Spline basis*](https://en.wikipedia.org/wiki/B-spline#:~:text=In%20the%20mathematical%20subfield%20of,B%2Dsplines%20of%20that%20degree.)\n\n:::{.callout-important}\n:::{style=\"font-size: 1.2em;\"}\n$[b_1(x), b_2(x), \\dots , b_K(x)]$ are **fixed** and **known**.\n:::\n:::\n\n::: notes\n\n:::\n\n\n\n\n\n## Piecewise regression\n\n- Polynomial regression imposes a *global* structure on the non-linear function.\n\n- Piecewise regression allows *structural changes in different parts of the range of $x$*\n\n$$y=\\begin{cases}\n    \\beta_{01} + \\beta_{11}x+ \\beta_{21}x^2 +\\epsilon     & \\quad \\text{if } x < \\xi\\\\\n    \\beta_{02} + \\beta_{12}x+ \\beta_{22}x^2+\\beta_{32}x^3+\\epsilon      & \\quad \\text{if } x \\ge \\xi\n  \\end{cases}$$\n\n- The joint points of pieces are called **knots**.\n\n- Using more knots leads to a more flexible piecewise polynomial.\n  \n  \n:::{.question}\nWith $K$ different knots, how many different polynomials do we have?\n:::\n\n::: notes\n- A polynomial regression may provide a poor fit, and increasing the order does not improve the situation.\n- This may happen when the regression function behaves differently in different parts of the range of x\n.\n:::\n\n\n## Piecewise Constant and Linear Regression\n\n::: xsmall\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: ESL Fig. 5.1](./images/07-splines/piecewise_linear.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n## U.S. Birth Rate from 1917 to 2003^[The data is only for illustrating ideas of different methods. The methods introduced here are best for inference about relationship between variables of some physical or natural system, not mainly for time series forecasting.]\n\n:::: {.columns}\n\n::: {.column width=\"25%\"}\n\n::: midi\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n``` my_class1\n     Year Birthrate\n1917 1917       183\n1918 1918       184\n1919 1919       163\n1920 1920       180\n1921 1921       181\n1922 1922       173\n1923 1923       168\n1924 1924       177\n1925 1925       172\n1926 1926       170\n1927 1927       164\n1928 1928       152\n1929 1929       145\n1930 1930       145\n1931 1931       139\n1932 1932       132\n1933 1933       126\n1934 1934       130\n1935 1935       130\n```\n\n\n:::\n:::\n\n:::\n\n:::\n\n::: {.column width=\"75%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/07-splines/unnamed-chunk-5-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::::\n\n\n\n\n## A Polynomial Regression Provide a Poor Fit\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nlmfit3 <- lm(Birthrate ~ poly(Year - mean(Year), degree = 3, raw = TRUE),  \n             data = birthrates)\n```\n:::\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/07-splines/unnamed-chunk-7-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/07-splines/unnamed-chunk-8-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::::\n\n\n::: notes\n`raw` if true, use raw and not orthogonal polynomials.\nhttps://stackoverflow.com/questions/19484053/what-does-the-r-function-poly-really-do\n- A polynomial regression may provide a poor fit, and increasing the order does not improve the situation.\n- This may happen when the regression function behaves differently in different parts of the range of x.\n:::\n\n\n\n\n## Piecewise Polynomials: 3 knots at 1936, 60, 78\n\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n::: midi\n$$y=\\begin{cases}\n    \\beta_{01} + \\beta_{11}x+ \\epsilon & \\text{if } x < 1936\\\\\n    \\beta_{02} + \\beta_{12}x+\\epsilon  & \\text{if } 1936 \\le x < 1960 \\\\\n    \\beta_{03} + \\beta_{13}x+\\epsilon  & \\text{if } 1960 \\le x < 1978 \\\\\n    \\beta_{04} + \\beta_{14}x+\\epsilon  & \\text{if } 1978 \\le x\n  \\end{cases}$$\n:::\n:::\n\n\n::: {.column width=\"60%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/07-splines/unnamed-chunk-9-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::::\n\n:::{.question}\nAny issue of piecewise polynomials?\n:::\n\n::: notes\n- Discontinuity at knots.\n- Because the regression is too flexible in some sense, allowing two different estimates at knots.\n\n:::\n\n\n# Regression Splines\n\n\n:::{style=\"font-size: 1.5em;\"}\n\nPiecewise polynomials + Continuity/Differentiability at knots\n\n:::\n\n\n## Splines\n\n\n**Splines** of degree $d$ are _smooth piecewise polynomials_ of degree $d$ with **continuity in derivatives** (smoothing) up to degree $d-1$ *at each knot*.\n\n. . .\n\n\n#### Continuous Piecewise Linear Regression\n\n\n:::: {.columns}\n\n::: {.column width=\"33%\"}\n\n::: xsmall\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/07-splines/piecewise_conti_linear.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n:::\n\n::: {.column width=\"67%\"}\n\n::: xsmall\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Source: ESL Fig. 5.1](./images/07-splines/piecewise_linear.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n:::\n\n::::\n\n\n## Piecewise Linear to Continuous Piecewise Linear\n\nHow do we turn the piecewise regression of degree 1 into a regression spline?\n\n:::{.midi}\n$$y=\\begin{cases}\n    \\beta_{01} + \\beta_{11}x+ \\epsilon & \\quad \\text{if } x < 1936\\\\\n    \\beta_{02} + \\beta_{12}x+\\epsilon  & \\quad \\text{if } 1936 \\le x < 1960 \\\\\n    \\beta_{03} + \\beta_{13}x+\\epsilon  & \\quad \\text{if } 1960 \\le x < 1978 \\\\\n    \\beta_{04} + \\beta_{14}x+\\epsilon  & \\quad \\text{if } 1978 \\le x\n  \\end{cases}$$\n:::\n\n. . .\n\n- For splines of degree 1, we require _continuous_ piecewise linear function\n\n\n:::: clolumns\n\n::: column\n\n$$\\begin{align} \n\\beta_{01} + \\beta_{11}1936 &= \\beta_{02} + \\beta_{12}1936\\\\\n\\beta_{02} + \\beta_{12}1960 &= \\beta_{03} + \\beta_{13}1960\\\\\n\\beta_{03} + \\beta_{13}1978 &= \\beta_{04} + \\beta_{14}1978\n\\end{align}$$\n\n:::\n\n::: column\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/07-splines/unnamed-chunk-12-1.png){fig-align='center' width=60%}\n:::\n:::\n\n:::\n\n::::\n\n\n## `splines::bs()`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nlin_sp <- lm(Birthrate ~ splines::bs(Year, degree = 1, knots = c(1936, 1960, 1978)), \n             data = birthrates)\n```\n\n::: {.cell-output-display}\n![](images/07-splines/unnamed-chunk-13-1.png){fig-align='center' width=100%}\n:::\n:::\n\n- The function is continuous everywhere, also at knots $\\xi_1, \\xi_2,$ and $\\xi_3$, i.e. $f_{k}(\\xi_k^-) = f_{k+1}(\\xi_k^+)$.\n\n- Linear everywhere except $\\xi_1, \\xi_2,$ and $\\xi_3$.\n\n- Has a different slope for each region.\n\n\n\n## Splines as Basis Function Model {#sec-linearspline}\n\nFor linear splines with 3 knots,\n\n$$\\begin{align} \nb_1(x) &= x\\\\\nb_2(x) &= (x - \\xi_1)_+\\\\\nb_3(x) &= (x - \\xi_2)_+\\\\ \nb_4(x) &= (x - \\xi_3)_+\n\\end{align}$$\nwhere \n\n<!-- - $\\xi_k, k = 1, 2, 3$ are knots -->\n\n- $(x - \\xi_k)_+$ is a *truncated power basis function* (of power one) such that\n\n$$(x - \\xi_k)_+=\\begin{cases}\n    x - \\xi_k & \\quad \\text{if }x > \\xi_k\\\\\n    0  & \\quad \\text{otherwise} \n  \\end{cases}$$\n  \n  \n$$\\begin{align} y &= \\beta_0 + \\beta_1 b_1(x) + \\beta_2 b_2(x) + \\beta_3 b_3(x) + \\beta_4 b_4(x) + \\epsilon\\\\\n&=\\beta_0 + \\beta_1 x + \\beta_2 (x - \\xi_1)_+ + \\beta_3 (x - \\xi_2)_+ + \\beta_4 (x - \\xi_3)_+ + \\epsilon\n\\end{align}$$\n\n\n\n## Linear Splines Basis Functions\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/07-splines/unnamed-chunk-14-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n## Splines as Basis Function Model^[Besides truncated power basis, we can represent the splines using another set of basis functions, for example [B-Splines](https://en.wikipedia.org/wiki/B-spline#:~:text=A%20B%2Dspline%20function%20is,using%20a%20number%20of%20points.) which is more computationally convenient and numerically accurate.]\n\nWith degree $d$ and $K$ knots, the regression spline with _first $d-1$ derivatives being continuous_ at the knots can be represented as \n\n$$\\begin{align} y &= \\beta_0 + \\beta_1 b_1(x) + \\beta_2 b_2(x) + \\dots + \\beta_d b_d(x) + \\beta_{d+1}b_{d+1}(x) + \\dots + \\beta_{d+K}b_{d+K}(x) + \\epsilon\n\\end{align}$$\nwhere\n\n- $b_j(x) = x^j, j = 1, 2, \\dots, d$\n\n- $b_{d+k} = (x - \\xi_k)^{d}_+, k = 1, \\dots, K$ where \n$$(x - \\xi_k)^d_+=\\begin{cases}\n    (x - \\xi_k)^d & \\quad \\text{if }x > \\xi_k\\\\\n    0  & \\quad \\text{otherwise} \n  \\end{cases}$$\n\n\n::: {.question}\nCan you write the model with $d = 2$ and $K = 3$?\n:::\n\n\n\n\n\n## Cubic Splines^[Cubic splines are popular because most human eyes cannot detect the discontinuity at the knots.]\n\n- The **cubic spline** is a spline of *degree 3* with *first 2 derivatives are continuous at the knots*.\n\n$$\\begin{align} y &= \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\sum_{k = 1}^K \\beta_{k+3} (x - \\xi_k)_+^3 + \\epsilon\n\\end{align}$$\n\n- Smooth at knots because\n  + $f(\\xi_k^-) = f(\\xi_k^+)$\n  + $f'(\\xi_k^-) = f'(\\xi_k^+)$\n  + $f''(\\xi_k^-) = f''(\\xi_k^+)$\n  + But $f^{(3)}(\\xi_k^-) \\ne f^{(3)}(\\xi_k^+)$\n\n\n\n\n## Degrees of Freedom\n\n- In regression, **(effective) degrees of freedom (df)** can be viewed as *the number of regression coeffcients that are freely to move.*\n\n- It measures the model complexity/flexiblity. The larger df is, the more flexible the model is.\n\n\n. . .\n\n\n- [*Piecewise linear*]{.green}: *3 knots, 4 linear regressions, and 8 dfs.*\n\n::: {.midi}\n$$y=\\begin{cases}\n    \\beta_{01} + \\beta_{11}x+ \\epsilon & \\quad \\text{if } x < 1936\\\\\n    \\beta_{02} + \\beta_{12}x+\\epsilon  & \\quad \\text{if } 1936 \\le x < 1960 \\\\\n    \\beta_{03} + \\beta_{13}x+\\epsilon  & \\quad \\text{if } 1960 \\le x < 1978 \\\\\n    \\beta_{04} + \\beta_{14}x+\\epsilon  & \\quad \\text{if } 1978 \\le x\n  \\end{cases}$$\n:::\n\n\n. . .\n\n\n- [*Linear splines*]{.green} (continuous piecewise linear): *8 - 3 constraints = 5 dfs.* (Check its [basis representation](#sec-linearspline))\n\n::: {.midi}\n$$\\begin{align} \n\\beta_{01} + \\beta_{11}1936 &= \\beta_{02} + \\beta_{12}1936\\\\\n\\beta_{02} + \\beta_{12}1960 &= \\beta_{03} + \\beta_{13}1960\\\\\n\\beta_{03} + \\beta_{13}1978 &= \\beta_{04} + \\beta_{14}1978\n\\end{align}$$\n:::\n\n\n::: notes\ntalk about effective df, so that we know the model flexibility and complexity of a regression spline model\n:::\n\n\n\n\n## Degrees of Freedom\n\n- With degree $d$ and $K$ knots,\n\n$$\\text{df} = K + d + 1$$\n\n<!-- - This leads to a total of [(4 + # knots)]{.green} degrees of freedom. -->\n\n. . .\n\n:::{.question}\nSo what is the degrees of freedom of cubic splines?\n:::\n\n\n\n::: notes\n- With degree $d$ and $K$ knots,\n\n$$\\begin{align} \\text{df} &= \\text{(# regions)} \\times \\text{(# coefficients)} - \\text{(# constraints)} \\times \\text{(# knots)}\\\\\n&=(K + 1)(d + 1) - dK\\\\\n&= K + d + 1 \\end{align} $$\n\n<!-- - This leads to a total of [(4 + # knots)]{.green} degrees of freedom. -->\n\n. . .\n\n- [*Cubic splines*]{.green}:\n\n\nWe have 3 constraints:\n\n  + continuity $f(\\xi ^ {-}) = f(\\xi ^ {+})$\n  + 1st derivative $f'(\\xi ^ {-}) = f'(\\xi ^ {+})$\n  + 2nd derivative $f''(\\xi ^ {-}) = f''(\\xi ^ {+})$\n\n$\\text{df} = \\text{(# regions)} \\times \\text{(# coef)} - \\text{(# constraints)} \\times \\text{(# knots)} = (K + 1)4 - 3K = 4 + K$\n:::\n\n\n\n## Cubic Splines\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\ncub_sp <- lm(Birthrate ~ splines::bs(Year, degree = 3, knots = c(1936, 1960, 1978)), \n             data = birthrates)\n```\n\n::: {.cell-output-display}\n![](images/07-splines/unnamed-chunk-15-1.png){fig-align='center' width=49%}\n:::\n:::\n\n\n\n## Natural Splines\n\n- Splines^[Especially for high-degree polynomials.] tends to produce erratic and undesirable results near the boundaries, and huge variance at the outer range of the predictors.\n\n. . .\n\n- A **natural spline** is a regression spline with additional *boundary constraints*: \n  + The spline function is *linear at the boundary* (in the region where $X$ is smaller than the smallest knot, or larger than the largest knot).\n  \n  <!-- + $f^{(k)}(x) = 0$ for $x < \\xi_1$ and $x > \\xi_K$ where $f^{(k)}(\\cdot)$ is the $k$-th derivative of the spline function $f$. -->\n  \n. . .\n\n- Natural splines generally produce more stable estimates at the boundaries.\n\n- Assuming linearity near the boundary is reasonable since there is less information available there.\n\n<!-- - The natural cubic  -->\n\n- `splines::ns()`\n\n\n. . .\n\n- **Natural Cubic Spline (NCS)** forces the 2nd and 3rd derivatives to be zero at the boundaries.\n\n- The constraints frees up 4 dfs.\n\n- The df of NCS is $K$.\n\n\n\n## Spline and Natural Spline Comparison \n\n- Cubic Spline vs. Natural Cubic Spline with the same degrees of freedom 6.\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/07-splines/unnamed-chunk-16-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n::: notes\nhttps://stackoverflow.com/questions/17930140/extracting-knot-points-from-glm-when-using-bs-in-r-as-a-variable\nattr(fit.bs$terms, \"predvars\")\n:::\n\n\n\n## [B-Splines](https://en.wikipedia.org/wiki/B-spline#:~:text=A%20B%2Dspline%20function%20is,using%20a%20number%20of%20points.) Basis Functions {visibility=\"hidden\"}\n\n- B-splines use a different basis representation for regression splines.\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbs(x, df = NULL, knots = NULL, \n   degree = 3, intercept = FALSE)\n```\n:::\n\n- `df`: the number of basis\n- `knots`: the quantiles of $x$\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/07-splines/unnamed-chunk-18-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nns(x, df = NULL, knots = NULL, \n   degree = 3, intercept = FALSE)\n```\n:::\n\n- `df`: the number of basis\n- `knots`: the quantiles of $x$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/07-splines/unnamed-chunk-20-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::::\n\n\n::: notes\nB-splines is more computationally efficient\n:::\n\n\n## Practical Issues\n\n- [How many knots should we use]{.green}\n  + *As few knots as possible, with at least 5 data points per segment* (Wold, 1974)\n  + *Cross-validation: e.g., choose the $K$ giving the smallest $MSE_{CV}$*\n  \n. . .\n\n- [Where to place the knots]{.green}\n  + *No more than one extreme or inflection point per segment* (Wold, 1974)\n  + If possible, *the extreme points should be centered in the segment*\n  + *More knots in places where the function might vary most rapidly, and fewer knots where it seems more stable*\n  + *Place knots in a uniform fashion*\n  + *Specify the desired df, and have the software place the corresponding number of knots at uniform quantiles of the data* (`bs()`, `ns()`)\n  \n. . .\n\n- [Degree of functions in each region]{.green}\n  + *Use $d < 4$ to avoid overly flexible curve fitting*\n  + *Cubic spline is popular*\n  \n::: notes\nToo many knots can overfit\nRegression splines often give superior results to polynomial regression\n:::\n\n  \n\n\n# Smoothing Splines\n\n## Roughness Penalty Approach\n\n- Our goal is *inference or curve fitting* rather than out of range prediction or forecasting.\n\n- Is there a method that we can select the number and location of knots automatically?\n\nConsider this criterion for fitting a *smooth* function $g(x)$ to\nsome data:\n\n$$\\begin{equation} \\min_{g} \\sum_{i=1}^n \\left( y_i - g(x_i) \\right)^2 + \\lambda \\int g''(t)^2 \\, dt \\end{equation}$$\n\n- The first term is $SS_{res}$, and tries to make $g(x)$ match the\ndata at each $x_i$. ([Goodness-of-fit]{.green})\n\n- The second term with $\\lambda \\ge 0$ is a [*roughness penalty*]{.green} and controls how wiggly $g(x)$ is.\n\n<!-- - The tuning parameter $\\lambda \\ge 0$. -->\n\n. . .\n\n:::{style=\"font-size: 3.5em;\"}\n:::{style=\"text-align: center;\"}\n\n> **Loss + Penalty**\n\n:::\n:::\n\n\n::: notes\n- 2nd term: penalizes the variability in g\n- the second derivative of a function is a measure of its roughness\n- the integral simply a measure of the total change in the function g′(t), over its entire range\n:::\n\n\n## Roughness Penalty Approach\n\n$$\\begin{equation} \\min_{g} \\sum_{i=1}^n \\left( y_i - g(x_i) \\right)^2 + \\lambda \\int g''(t)^2 \\, dt \\end{equation}$$\n\n:::{.question}\nHow $\\lambda$ affects the shape of $g(x)$?\n:::\n\n\n+ The smaller $\\lambda$ is, the more wiggly $g(x)$ is, eventually\ninterpolating $y_i$ when $\\lambda = 0$.\n\n+ As $\\lambda \\rightarrow \\infty$, $g(x)$ becomes linear.\n\n- The function $g$ that minimizes the objective is known as a **smoothing spline**.\n\n. . .\n\n:::{.callout-note}\n\n:::{style=\"font-size: 1.2em;\"}\n\n- The solution $\\hat{g}$ is a [natural cubic spline]{.green}, with knots at $x_1, x_2, \\dots , x_n$!\n- But, it is not the same NCS gotten from the basis function approach.\n- It is a *shrunken* version where $\\lambda$ controls the level of shrinkage.\n- Its *effective* df is less than $n$.\n\n:::\n\n:::\n\n\n## Properties of Smoothing splines\n\n- Smoothing splines avoid the knot-selection issue, leaving a single $\\lambda$ to be chosen.\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n[**Linear regression**]{.green}\n\n- $\\hat{\\by} = \\bf H \\by$ where $\\bf H$ is the hat matrix\n\n- The degrees of freedom $(\\text{# coef})$ is $$p = \\sum_{i=1}^n {\\bf H}_{ii}$$\n\n- PRESS for model selection\n\n$$\\begin{align} PRESS &= \\sum_{i = 1}^n \\left( y_i - \\hat{y}_i^{(-i)}\\right)^2 \\\\\n&= \\sum_{i = 1}^n \\left[ \\frac{y_i - \\hat{y}_i}{1 - {\\bf H}_{ii}}\\right]^2\\end{align}$$\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n[**Smoothing splines**]{.green}\n\n- $\\hat{\\bg}_{\\lambda} = {\\bf S}_{\\lambda} \\by$ where $\\bf {\\bf S}_{\\lambda}$ is the **smoother matrix**.\n\n- The effective degrees of freedom $(\\text{# coef})$ is $$df_{\\lambda} = \\sum_{i=1}^n \\{{\\bf S}_{\\lambda}\\}_{ii} \\in [2, n]$$\n\n- LOOCV for choosing $\\lambda$^[GCV can be used too.]\n\n$$\\begin{align} (SS_{res})_{CV}(\\lambda) &= \\sum_{i = 1}^n \\left( y_i - \\hat{g}_{\\lambda}^{(-i)}(x_i)\\right)^2 \\\\\n&= \\sum_{i = 1}^n \\left[ \\frac{y_i - \\hat{g}_{\\lambda}(x_i)}{1 - \\{{\\bf S}_{\\lambda}\\}_{ii}}\\right]^2 \\end{align}$$\n\n:::\n\n::::\n\n\n## `smooth.spline()`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nsmooth.spline(x, y, df, lambda, cv = FALSE)\n```\n:::\n\n- `cv = TRUE` use LOOCV; `cv = FALSE` use GCV\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nfit <- smooth.spline(birthrates$Year, birthrates$Birthrate)\n```\n\n::: {.cell-output-display}\n![](images/07-splines/unnamed-chunk-22-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nfit$df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 60.8\n```\n\n\n:::\n:::\n\n::: notes\noverfitting since the birthrate data has little variation in adjacent years\n:::\n\n\n## `smooth.spline()`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nfit <- smooth.spline(birthrates$Year, birthrates$Birthrate, df = 15)\n```\n\n::: {.cell-output-display}\n![](images/07-splines/unnamed-chunk-24-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n# Generalized Additive Models\n\n\n\n## Extending Splines to Multiple Variables\n\n- All methods discussed so far are extensions of simple linear regression.\n\n- How to flexibly predict $y$ or nonlinear regression function on the basis of several predictors $x_1, \\dots, x_p$?\n\n. . .\n\nMaintaining the *additive* structure of linear models, but allowing nonlinear functions of each of the variables.\n\n$$y = \\beta_0 + f_1(x_1) + f_2(x_2) + \\dots + f_p(x_p) + \\epsilon$$\n\n- This is a **generalized additive model**.\n  + It's *general*: it can be modelling response with other distributions, binary, counts, positive values, for example.\n  + It's *additive*: calculate a separate $f_j$ for each $x_j$, and add together all of their contributions.\n\n. . .\n\n- $f_1(x_1) = x_1$; $f_2(x_2) = x_2 + x_2^2$; $f_3(x_3) = \\text{cubic splines}$\n\n## `Wage` data in ISL\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nlibrary(ISLR2)\nattach(Wage)\ndplyr::glimpse(Wage)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n``` my_class600\nRows: 3,000\nColumns: 11\n$ year       <int> 2006, 2004, 2003, 2003, 2005, 2008, 2009, 2008, 2006, 2004,…\n$ age        <int> 18, 24, 45, 43, 50, 54, 44, 30, 41, 52, 45, 34, 35, 39, 54,…\n$ maritl     <fct> 1. Never Married, 1. Never Married, 2. Married, 2. Married,…\n$ race       <fct> 1. White, 1. White, 1. White, 3. Asian, 1. White, 1. White,…\n$ education  <fct> 1. < HS Grad, 4. College Grad, 3. Some College, 4. College …\n$ region     <fct> 2. Middle Atlantic, 2. Middle Atlantic, 2. Middle Atlantic,…\n$ jobclass   <fct> 1. Industrial, 2. Information, 1. Industrial, 2. Informatio…\n$ health     <fct> 1. <=Good, 2. >=Very Good, 1. <=Good, 2. >=Very Good, 1. <=…\n$ health_ins <fct> 2. No, 2. No, 1. Yes, 1. Yes, 1. Yes, 1. Yes, 1. Yes, 1. Ye…\n$ logwage    <dbl> 4.32, 4.26, 4.88, 5.04, 4.32, 4.85, 5.13, 4.72, 4.78, 4.86,…\n$ wage       <dbl> 75.0, 70.5, 131.0, 154.7, 75.0, 127.1, 169.5, 111.7, 118.9,…\n```\n\n\n:::\n:::\n\n\n## `gam::gam()`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nlibrary(gam)\ngam.m3 <- gam(wage ~ s(year, df = 4) + s(age , df = 5) + education, data = Wage)\n```\n:::\n\n- `s()` for smoothing splines\n\n- Coefficients not that interesting; fitted functions are. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/07-splines/unnamed-chunk-27-1.png){fig-align='center' width=100%}\n:::\n:::\n\n::: notes\nupper and lower pointwise twice-standard-error curves are included\n\nThe left-hand panel indicates that\nholding age and education fixed, wage tends to increase slightly with year;\nthis may be due to inflation. The center panel indicates that holding education\nand year fixed, wage tends to be highest for intermediate values of age, and\nlowest for the very young and very old.\n\n- Because the model is additive, we can examine the effect of each $X_j$ on $Y$ individually while holding all of the other variables fixed.\n\n- Allow to fit a non-linear $f_j$ to each $X_j$, so that we can\nautomatically model non-linear relationships that standard linear regression will miss.\n\n- No need to manually try out many different transformations on each variable individually.\n\n- The non-linear fits can potentially make more accurate predictions for the response.\n\n- The smoothness of the function $f_j$ for the variable $X_j$ can be summarized via degrees of freedom.\n\n- The model is additive. For fully general models, we look for more flexible approaches such as random forests and boosting.\n\n:::\n\n\n<!-- ## GAM fitting {visibility=\"hidden\"} -->\n\n<!-- - Natural splines `lm(wage ~ ns(year, df = 5) + ns(age, df = 5) + education)` -->\n\n<!-- - Can mix terms, e.g., smoothing splines and local regression -->\n\n<!-- `gam(wage ~ s(year, df = 5) + lo(age, span = .5) + education)` -->\n\n<!-- - Can add interactions, e.g. `ns(age, df = 5):ns(year, df = 5)` -->\n\n<!-- - Use `anova()` to compare models. -->\n\n\n\n\n<!-- ## Summary of GAMs {visibility=\"hidden\"} -->\n\n<!-- - Allow to fit a non-linear $f_j$ to each $X_j$, so that we can -->\n<!-- automatically model non-linear relationships that standard linear regression will miss. -->\n\n<!-- - No need to manually try out many different transformations on each variable individually. -->\n\n<!-- - The non-linear fits can potentially make more accurate predictions for the response. -->\n\n<!-- - The smoothness of the function $f_j$ for the variable $X_j$ can be summarized via degrees of freedom. -->\n\n<!-- - The model is additive. For fully general models, we look for more flexible approaches such as random forests and boosting. -->\n\n\n<!-- ::: notes -->\n<!-- - Because the model is additive, we can examine the effect of each $X_j$ on $Y$ individually while holding all of the other variables fixed. -->\n\n<!-- - The smoothness of the function $f_j$ for the variable $X_j$ can be summarized via degrees of freedom. -->\n\n<!-- ::: -->\n\n\n\n<!-- ## Other Topics -->\n\n<!-- - [Basis spline (B-spline) definition (de Boor, 1978)](https://en.wikipedia.org/wiki/B-spline#:~:text=A%20B%2Dspline%20function%20is,using%20a%20number%20of%20points.): more computationally efficient -->\n\n<!-- - Algorithms for fitting GAMs using smoothing splines: [Backfitting](https://en.wikipedia.org/wiki/Backfitting_algorithm), ISL Sec 7.9 Exercise 11. -->\n\n<!-- - GAMs for binary response and classification. -->\n\n<!-- - [Multivariate Adaptive Regression Splines (MARS)](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_spline#:~:text=In%20statistics%2C%20multivariate%20adaptive%20regression,nonlinearities%20and%20interactions%20between%20variables.) -->\n\n\n<!-- ::: notes -->\n<!-- How many knots should be used -->\n<!-- Where to place the knots -->\n<!-- What is the degree of functions in each region -->\n\n<!-- we can control that using the df parameter. We use a total of 6 parameters, chosen by the function automatically. However, this does not seems to perform better than the knots we implemented. The choice of knots can be crucial. -->\n<!-- ::: -->\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
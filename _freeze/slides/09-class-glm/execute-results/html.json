{
  "hash": "3b1ad2d8a7c859637feac6e3edfdb168",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Logistic Regression `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 320 512\" style=\"height:1em;width:0.62em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M0 192C0 103.6 71.6 32 160 32s160 71.6 160 160V320c0 88.4-71.6 160-160 160S0 408.4 0 320V192zM160 96c-53 0-96 43-96 96V320c0 53 43 96 96 96s96-43 96-96V192c0-53-43-96-96-96z\"/></svg>`{=html} `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 256 512\" style=\"height:1em;width:0.5em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M160 64c0-11.8-6.5-22.6-16.9-28.2s-23-5-32.8 1.6l-96 64C-.5 111.2-4.4 131 5.4 145.8s29.7 18.7 44.4 8.9L96 123.8V416H32c-17.7 0-32 14.3-32 32s14.3 32 32 32h96 96c17.7 0 32-14.3 32-32s-14.3-32-32-32H160V64z\"/></svg>`{=html}'\nsubtitle: \"MSSC 6250 Statistical Machine Learning\"\nauthor: \"Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University\"\n# date: \"February 23 2025\"\n# macros: _macros.tex # import a list of TeX/LaTeX definitions\nformat: \n  revealjs:\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n    # include-in-header:\n    #     - \"macros.tex\"\n    highlight-style: github\n    code-block-bg: true\n    self-contained: false\n    slide-number: c/t\n    incremental: false\n    width: 1800\n    height: 1000\n    margin: 0.05\n    logo: \"https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg\"\n    footer: \"[mssc6250-s25.github.io/website](https://mssc6250-s25.github.io/website/)\"\n    theme: [\"simple\", \"styles.scss\"]\n    echo: false\n    multiplex: true\n    code-link: true\n    fig-cap-location: bottom\n    fig-align: center\n    transition: none ## fade slide convex concave zoom\n    code-line-numbers: false\n    title-slide-attributes:\n      data-background-color: \"#447099\"\n      # data-background-image: images/paper-texture.jpg\n      # data-background-size: cover\n      # data-background-color: \"#698ED5\"\neditor: source\nexecute:\n  freeze: true\n---\n\n\n# {visibility=\"hidden\"}\n\n\n\\def\\bx{\\mathbf{x}}\n\\def\\bg{\\mathbf{g}}\n\\def\\bbeta{\\boldsymbol \\beta}\n\\def\\bX{\\mathbf{X}}\n\\def\\by{\\mathbf{y}}\n\\def\\bH{\\mathbf{H}}\n\\def\\bI{\\mathbf{I}}\n\\def\\bS{\\mathbf{S}}\n\\def\\T{\\text{T}}\n\\def\\cov{\\mathrm{Cov}}\n\\def\\cor{\\mathrm{Corr}}\n\\def\\var{\\mathrm{Var}}\n\\def\\E{\\mathrm{E}}\n\\def\\bmu{\\boldsymbol \\mu}\n\\def\\btheta{\\boldsymbol \\theta}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\def\\Trace{\\text{Trace}}\n\n\n\n\n\n\n# Binary Logistic Regression\n\n## Non-Gaussian and Non-continuous Response\n- In many applications, the response or error term are **nonnormal**.\n\n  + [*Binary*]{.green} response: how an online banking system determine [*whether or not a transaction is fraudulent*]{.green} based on the user's IP address.\n  + [*Count*]{.green} response: how weather conditions affect [*the number of users of a bike sharing program*]{.green} during a particular hour of the day.\n  \n. . .\n\n- Besides normal, the response in a **generalized linear model** (GLM) can be Bernoulli, binomial, Poisson, etc.\n\n- There is *no assumption that $\\var(y_i)$ is homogeneous:* **Both $E(y_i)$ and $\\var(y_i)$ may vary with the regressors from data point to data point.**\n\n- Ordinary least squares *does not* apply when the response is not normal.\n\n\n## Classification\n\n- Linear regression assumes that the response $Y$ is *numerical*.\n- In many situations, $Y$ is **categorical**.\n\n. . .\n\n- A process of predicting categorical response is known as **classification**.  \n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n**Normal vs. COVID vs. Smoking**\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/09-class-glm/covid_lung.jpeg){fig-align='center' width=100%}\n:::\n:::\n:::\n\n\n\n::: {.column width=\"50%\"}\n\n**fake news vs. true news**\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/09-class-glm/fake_news.jpeg){fig-align='center' width=90%}\n:::\n:::\n:::\n::::\n\n \n    \n\n## Regression Function $f(x)$ vs. Classifier $C(x)$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/09-class-glm/regression.png){fig-align='center' width=100%}\n:::\n:::\n\n. . .\n\n::: xsmall\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![https://daviddalpiaz.github.io/r4sl/classification-overview.html](images/09-class-glm/classification.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n## Soft and Hard Classifiers\n\nTwo popular approaches when modeling binary data\n\n- **Soft classifiers**\n  + Estimate the conditional probabilities $Pr(Y = k \\mid {\\bf X})$ for each category $k$.\n  + Use $\\mathbf{1}\\{Pr(Y \\mid {\\bf X}) > c \\}$ for classification, where $c \\in(0, 1)$ is a threshold or cutoff.\n  + e.g. logistic regression\n  \n. . .\n\n- **Hard classifiers**\n  + Directly estimate the classification decision boundary\n  + e.g. support vector machines\n\n\n## Classification Example\n\nGiven the training data $\\{(x_i, y_i)\\}_{i=1}^n$, we build a classifier to predict whether people will default on their credit card payment $(Y)$ `yes` or `no`, based on monthly credit card balance $(X)$.\n\n\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/09-class-glm/unnamed-chunk-6-1.png){fig-align='center' width=100%}\n:::\n:::\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/09-class-glm/credit_card.jpeg){fig-align='center' width=100%}\n:::\n:::\n:::\n::::\n\n\n\n## Why Not Linear Regression\n\n$$Y =\\begin{cases}\n    0  & \\quad \\text{if not default}\\\\\n    1  & \\quad \\text{if default}\n     \\end{cases}$$\n     \n\n- $Y = \\beta_0 + \\beta_1X + \\epsilon$, $\\, X =$ credit card balance \n\n:::{.question}\nWhat are potential issues with this dummy variable approach?\n:::\n\n. . .\n\n\n- $\\hat{Y} = b_0 + b_1X$ estimates $P(Y = 1 \\mid X) = P(default = yes \\mid balance)$.\n\n. . .\n\n- However, \n\n<!-- which is equivalent to *linear discriminant analysis (LDA)* discussed later. -->\n\n## Why Not Linear Regression?\n\n- *Probability estimates can be outside $[0, 1]$.*\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/09-class-glm/lm-default-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Why Not Linear Regression?\n\n- Linear regression generally *cannot handle more than two categories*.\n\n$$Y =\\begin{cases}\n    1  & \\quad \\text{stroke}\\\\\n    2  & \\quad \\text{drug overdose} \\\\\n    3  & \\quad \\text{epileptic seisure}\n     \\end{cases}$$\n     \n     \n- The coding \n\n  + suggests an ordering `epileptic seisure` $>$ `drug overdose` $>$ `stroke`\n  + implies that `stroke` $-$ `drug overdose` $=$ `drug overdose` $-$ `epileptic seizure`.\n\n\n\n\n## Binary Logistic Regression\n\n- First predict the *probability* of each category of $Y$\n- Predict probability of `default` using a <span style=\"color:blue\">**S-shaped** curve</span>.\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/09-class-glm/glm-default-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n## Framing the Problem: Binary Responses\n\n:::{.question}\nWe use normal distribution for numerical $y$. What distribution we can use for binary $y$ that takes value 0 or 1?\n:::\n\n. . .\n\n- Each outcome `default` $(y = 1)$ and `not default` $(y = 0)$ is a Bernoulli variable. But,\n\n. . .\n\n- The probability of \"success\" $\\pi$ is not constant but *varies with predictor values!*\n\n$$ y_i \\mid x_i \\stackrel{indep}{\\sim} \\text{Bernoulli}(\\pi_i = \\pi(x_i)) = \\text{binomial}(m=1,\\pi = \\pi_i) $$\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n- $X =$ `balance`. $x_1 = 2000$ has a larger $\\pi_1 = \\pi(2000)$ than $\\pi_2 = \\pi(500)$ with $x_2 = 500$\n- Credit cards with a higher balance are more likely to be defaulted.\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/09-class-glm/unnamed-chunk-10-1.png){fig-align='center' width=85%}\n:::\n:::\n\n:::\n::::\n\n\n\n\n::: notes\nIn the credit card example, \n- do we have exactly two outcomes? \n- do we have constant probability? $P(y_1 = 1) = P(y_2 = 1) = \\cdots = P(y_n = 1) = \\pi?$\nExactly the same idea as linear regression, where the the mean response is not constant but varies with the value of predictors, $E(y_i) = \\beta_0+\\beta_1x_i$.\n:::\n\n\n## Bernoulli Variables {visibility=\"hidden\"}\n\n- $Y_i \\sim \\text{Bernoulli}(\\pi_i)$\n<!-- - $P(Y_i = 1) = \\pi_i$ -->\n<!-- - $P(Y_i = 0) = 1 - \\pi_i$ -->\n<!-- - $E[Y_i] = 1(\\pi_i) + 0(1-\\pi_i) = \\pi_i$ -->\n<!-- - $\\var(Y_i) = E[(Y_i - E(Y_i))^2] = (1 - \\pi_i)^2\\pi_i + (0 - \\pi_i)^2(1 - \\pi_i) = \\pi_i(1-\\pi_i)$ -->\n- $E[Y_i] = \\pi_i$\n- $\\var(Y_i) = \\pi_i(1-\\pi_i)$\n\nEven though fitting linear regression $y_i = \\bx_i'\\bbeta + \\epsilon_i$ predicts $P(Y = 1 \\mid X)$, it **does not** make sense because\n\n:::{.alert}\n- $\\epsilon_i$s only take two values, so they are not Gaussian.\n- The variance of $Y_i$ is a function of the mean $\\pi_i$ (not constant).\n- $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1+ \\cdots + \\hat{\\beta}_px_p$ could fall outside the $(0, 1)$ range.\n:::\n\n\n## Logistic Function\n\n:::{.center}\nInstead of predicting $y_i$ directly, we use the predictors to model its *probability* of success, $\\pi_i$.\n:::\n\n\n:::{.center}\nBut how?\n:::\n\n. . .\n\n- **Transform $\\pi \\in (0, 1)$ into a variable $\\eta \\in (-\\infty, \\infty)$. Then construct a linear predictor on $\\eta$:  $\\eta_i = \\bx_i'\\bbeta$**\n\n. . .\n\n- **Logit function:** For $0 < \\pi < 1$\n\n$$\\eta = \\text{logit}(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)$$\n\n\n## Logit function $\\eta = \\text{logit}(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)$\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/09-class-glm/unnamed-chunk-11-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n## Logistic Function\n\n- The *logit* function $\\eta = \\text{logit}(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)$ takes a value $\\pi \\in (0, 1)$ and maps it to a value $\\eta \\in (-\\infty, \\infty)$.\n- **Logistic function**:\n$$\\pi = \\text{logistic}(\\eta) = \\frac{1}{1+\\exp(-\\eta)} \\in (0, 1)$$\n- The *logistic (sigmoid)* function takes a value $\\eta \\in (-\\infty, \\infty)$ and maps it to a value $\\pi \\in (0, 1)$.\n\n. . .\n\n- Once $\\eta$ is estimated by the linear regression, we use the logistic function to transform $\\eta$ back to the probability.\n\n::: notes\n$$\\pi = \\text{logistic}(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)} = \\frac{1}{1+\\exp(-\\eta)} \\in (0, 1)$$\n:::\n\n\n\n\n## Logistic (Sigmoid) Function $\\pi = \\text{logistic}(\\eta) = \\frac{1}{1+\\exp(-\\eta)}$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/09-class-glm/unnamed-chunk-12-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n## Logistic Regression Model\n\nFor $i = 1, \\dots, n$ and with $p$ predictors:\n  $$Y_i \\mid \\pi_i({\\bf x}_i) \\stackrel{indep}{\\sim} \\text{Bernoulli}(\\pi_i), \\quad {\\bf x}_i' = (x_{i1}, \\dots, x_{ip})$$\n  $$\\text{logit}(\\pi_i) = \\ln \\left( \\frac{\\pi_i}{1 - \\pi_i} \\right) = \\eta_i = \\beta_0+\\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip} = {\\bf x}_i'\\bbeta$$\n  \n  \n- The $\\eta_i = \\text{logit}(\\pi_i)$ is a **link function** that *links* the linear predictor and the mean of $Y_i$.\n\n. . .\n\n$$\\small E(Y_i) = \\pi_i = \\frac{1}{1 + \\exp(-{\\bf x}_i'\\bbeta )}$$\n$$\\small \\hat{\\pi}_i = \\frac{1}{1 + \\exp(-{\\bf x}_i'\\hat{\\bbeta} )}$$\n\n\n\n\n## `Default` Data in `ISLR2`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(\"Default\"); head(Default, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n``` my_class500\n   default student balance income\n1       No      No     730  44362\n2       No     Yes     817  12106\n3       No      No    1074  31767\n4       No      No     529  35704\n5       No      No     786  38463\n6       No     Yes     920   7492\n7       No      No     826  24905\n8       No     Yes     809  17600\n9       No      No    1161  37469\n10      No      No       0  29275\n```\n\n\n:::\n\n```{.r .cell-code}\nstr(Default)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n``` my_class500\n'data.frame':\t10000 obs. of  4 variables:\n $ default: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ student: Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 2 1 1 ...\n $ balance: num  730 817 1074 529 786 ...\n $ income : num  44362 12106 31767 35704 38463 ...\n```\n\n\n:::\n:::\n\n\n\n## Fitting [^1]\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1-3|2\"}\nlogit_fit <- glm(default ~ balance, data = Default, \n                 family = binomial)\ncoef(summary(logit_fit))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept) -10.6513    0.36116     -29 3.6e-191\nbalance       0.0055    0.00022      25 2.0e-137\n```\n\n\n:::\n:::\n\n$\\hat{\\eta} = \\text{logit}(\\hat{\\pi}) = \\ln \\left( \\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\right) = -10.65 + 0.0055 \\times \\text{balance}$\n\n\n\n[^1]: In Python, use [statsmodels.formula.api.logit](https://www.statsmodels.org/stable/generated/statsmodels.formula.api.logit.html#statsmodels.formula.api.logit) or [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n\n\n<!-- . . . -->\n\n<!-- [statsmodels.formula.api](https://www.statsmodels.org/stable/generated/statsmodels.formula.api.logit.html#statsmodels.formula.api.logit) -->\n\n<!-- ```{python} -->\n<!-- #| eval: false -->\n<!-- #| echo: true -->\n<!-- from statsmodels.formula.api import logit -->\n<!-- ``` -->\n\n<!-- ```{python} -->\n<!-- #| eval: false -->\n<!-- #| echo: true -->\n<!-- from sklearn.linear_model import LogisticRegression -->\n<!-- ``` -->\n\n## Interpretation of Coefficients\n\nThe ratio $\\frac{\\pi}{1-\\pi} \\in (0, \\infty)$ is called the **odds** of some event.\n\n\n- Example: If 1 in 5 people will default, the odds is 1/4 since $\\pi = 0.2$ implies an odds of $0.2/(1−0.2) = 1/4$.\n\n\n\n$$\\ln \\left( \\frac{\\pi(x)}{1 - \\pi(x)} \\right)= \\beta_0 + \\beta_1x$$\n\n- Increasing $x$ by one unit \n  + changes the **log-odds** by $\\beta_1$\n  + multiplies the odds by $e^{\\beta_1}$\n\n\n\n:::{.alert}\n- $\\beta_1$ does *not* correspond to the change in $\\pi(x)$ associated with a one-unit\nincrease in $x$.\n- $\\beta_1$ is the change in *log odds* associated with one-unit increase in $x$.\n:::\n\n## Interpretation of Coefficients\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)  -10.651       0.36     -29        0\nbalance        0.005       0.00      25        0\n```\n\n\n:::\n:::\n\n- $\\hat{\\eta} = \\text{logit}(\\hat{\\pi}) = \\ln \\left( \\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\right) = -10.65 + 0.0055 \\times \\text{balance}$\n\n. . .\n\n- $\\hat{\\eta}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1x$\n- $\\hat{\\eta}(x+1) = \\hat{\\beta}_0 + \\hat{\\beta}_1(x+1)$\n- $\\hat{\\eta}(x+1) - \\hat{\\eta}(x) = \\hat{\\beta}_1 = \\ln(\\text{odds}_{x+1}) - \\ln(\\text{odds}_{x})$\n- One-unit increase in `balance` increases the *log odds* of `default` by 0.005 units.\n\n. . .\n\n- The **odds ratio**, $\\widehat{OR} = \\frac{\\text{odds}_{x+1}}{\\text{odds}_{x}} = e^{\\hat{\\beta}_1} = e^{0.0055} = 1.005515$.\n- The odds of `default` increases by 0.5% with additional one unit of credit card `balance`.\n\n\n## Pr(default) When Balance is 2000\n\n$$\\log\\left(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\right) = -10.65+0.0055\\times 2000$$\n\n$$ \\hat{\\pi} = \\frac{1}{1+\\exp(-(-10.65+0.0055 \\times 2000)} = 0.586$$\n\n. . .\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npi_hat <- predict(logit_fit, type = \"response\")\neta_hat <- predict(logit_fit, type = \"link\")  ## default gives us b0 + b1*x\npredict(logit_fit, newdata = data.frame(balance = 2000), type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   1 \n0.59 \n```\n\n\n:::\n:::\n\n\n## Probability Curve\n\n:::{.question}\nWhat is the probability of default when the balance is 500? What about balance 2500?\n:::\n\n:::: {.columns}\n\n::: {.column width=\"75%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/09-class-glm/default-predict-viz-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"25%\"}\n\n- [500 balance: Pr(default) = 0]{.pink}\n- [2000 balance: Pr(default) = 0.59]{.yellow}\n- [2500 balance: Pr(default) = 0.96]{.green}\n\n:::\n\n::::\n\n\n\n\n## Maximum Likelihood Estimation\n\n- `glm()` uses maximum likelihood to estimate the parameters $\\bbeta$.\n\nThe log-likelihood is given by \n\n$$\\ell(\\bbeta) = \\sum_{i=1}^n \\log \\, p(y_i \\mid x_i, \\bbeta).$$\n\nUsing Bernoulli probabilities, we have\n\n$$\\begin{align}\n\\ell(\\bbeta) =& \\sum_{i=1}^n \\log \\left\\{ \\pi(\\bx_i)^{y_i} [1-\\pi(\\bx_i)]^{1-y_i} \\right\\}\\\\\n    =& \\sum_{i=1}^n y_i \\log \\frac{\\pi(\\bx_i)}{1-\\pi(\\bx_i)} + \\log [1-\\pi(\\bx_i)] \\\\\n    =& \\sum_{i=1}^n y_i \\bx_i' \\bbeta - \\log [ 1 + \\exp(\\bx_i' \\bbeta)]\n\\end{align}$$\n\n<!-- Since this objective function is relatively simple, we can use Newton's method to update. The key is to derive the gradient and Hessian functions. For details, please see the [SMLR text book](https://teazrq.github.io/SMLR/logistic-regression.html#solving-a-logistic-regression). Instead of solving them ourselves, we can simply utilize the `optim()` function to perform the optimization for us. -->\n\n\n\n## Maximum Likelihood Estimation\n\n- We can use *Newton's method* that needs the gradient and Hessian matrix.\n\n$$\\bbeta^{\\text{new}} = \\bbeta^{\\text{old}} - \\left[ \\frac{\\partial ^2 \\ell(\\bbeta)}{\\partial \\bbeta \\partial \\bbeta'}\\right] ^{-1}\\frac{\\partial \\ell(\\bbeta)}{\\partial \\bbeta}$$\n\n. . .\n\n- Simply use `optim()`\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- as.matrix(cbind(\"intercept\" = 1, Default$balance))\ny <- as.matrix(as.numeric(Default$default) - 1)\nbeta <- rep(0, ncol(x))  ## start with 0\n\noptim(beta, fn = my_loglik, gr = my_gradient, method = \"BFGS\", \n      x = x, y = y, \n      control = list(\"fnscale\" = -1))$par # \"fnscale\" = -1 for maximization\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -10.6454   0.0055\n```\n\n\n:::\n:::\n\n\n## Evaluation^[More on [Wiki page](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).]\n\n- **Confusion Matrix**\n\n|                        | True 0               | True 1            |\n|------------------------|-------------------------------|-------------------------------|\n| **Predict 0** |  **True Negative  (TN)** | **False Negative (FN)**|\n| **Predict 1**  |  **False Positive (FP)**|  **True Positive  (TP)**           | \n\n\n\n- **Sensitivity (True Positive Rate)** $= P( \\text{predict 1} \\mid \\text{true 1}) = \\frac{TP}{TP+FN}$\n\n- **Specificity (True Negative Rate)** $= P( \\text{predict 0} \\mid \\text{true 0}) = \\frac{TN}{FP+TN}$ \n\n- **Accuracy** $= \\frac{TP + TN}{TP+FN+FP+TN} = \\frac{1}{m}\\sum_{j=1}^mI(y_j = \\hat{y}_j)$, where $y_j$s are true test labels and $\\hat{y}_j$s are their corresponding predicted label.\n\n. . .\n\nA *good* classifier can be the one which the *test accuracy rate is highest*, or *test error rate $\\frac{1}{m}\\sum_{j=1}^mI(y_j \\ne \\hat{y}_j)$ is smallest*.\n\n\n<!-- - **F1 score** $= \\frac{2TP}{2TP+FP+FN}$ -->\n\n\n## Confusion Matrix [^2]\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_prob <- predict(logit_fit, type = \"response\")\ntable(pred_prob > 0.5, Default$default)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       \n          No  Yes\n  FALSE 9625  233\n  TRUE    42  100\n```\n\n\n:::\n:::\n\n\n\n[^2]: For simplicity we use training set for demo. In real applications, we care about the classification result of the test set.\n\n\n\n\n::: notes\n\n- Packages:\n  + [`caret`](https://topepo.github.io/caret/) (**C**lassification **A**nd **RE**gression **T**raining)\n  + [`yardstick`](https://yardstick.tidymodels.org/index.html) of [`tidymodels`](https://www.tidymodels.org/)\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncaret::confusionMatrix()\nyardstick::conf_mat()\n```\n:::\n\n:::\n\n\n\n## Receiver Operating Characteristic (ROC) Curve\n\n- The **ROC curve** plots True Positive Rate (Sensitivity) vs. False Positive Rate (1 - Specificity)\n\n:::: {.panel-tabset}\n\n## ROC Plot\n<!-- :::: {.columns} -->\n\n<!-- ::: {.column width=\"50%\"} -->\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/09-class-glm/roc-1.png){fig-align='center' width=60%}\n:::\n:::\n\n<!-- ::: -->\n\n<!-- ::: {.column width=\"50%\"} -->\n\n## Code\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ROCR)\n# create an object of class prediction \npred <- ROCR::prediction(\n    predictions = pred_prob, \n    labels = Default$default)\n# calculates the ROC curve\nroc <- ROCR::performance(\n    prediction.obj = pred, \n    measure = \"tpr\",\n    x.measure = \"fpr\")\nplot(roc, colorize = TRUE, lwd = 3)\n```\n:::\n\n\n::::\n\n\n::: notes\n\n- Packages: [ROCR](http://ipa-tys.github.io/ROCR/), [pROC](https://web.expasy.org/pROC/), [yardstick::roc_curve()](https://yardstick.tidymodels.org/reference/roc_curve.html)\n\n:::\n\n\n\n## Area Under Curve (AUC)\n\nFind the area under the curve:\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## object of class 'performance'\nauc <- ROCR::performance(\n    prediction.obj = pred, \n    measure = \"auc\")\nauc@y.values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] 0.95\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/09-class-glm/unnamed-chunk-25-1.png){fig-align='center' width=100%}\n:::\n:::\n:::\n\n::::\n\n\n\n::: notes\nPrecision-recall curve\n:::\n\n\n\n## Threshold 0.5 Issue\n\n- Threshold 0.5 may be inappropriate if\n  + imbalanced data or skewed class distribution in training\n  + the cost of one type of misclassification is more important than another.\n\n\n\n::: notes\n\n- 0.5 threshold may misclassify too many minority class samples. Using smaller threshold\nExample: If only 1% of the data is fraud, a model predicting \"no fraud\" for everything would still achieve 99% accuracy, but it fails at detecting fraud.\n- If false positive is more costly (Marking a real email as spam (False Positive) can be frustrating), want to be more conservative, so use larger threshold.\n- If false negative is more costly (Medical Diagnosis: Missing cancer (False Negative) is much worse), want to be more aggressive, so use smaller threshold.\n\n:::\n\n\n\n## Optimal Threshold\n\nFrom the training set, choose the cut-off that maximizes\n\n- **G-Mean** $= \\sqrt{\\text{TPR} * \\text{TNR}}$ (geometric mean of TPR and TNR)\n\n- **Youden’s J index** $= \\text{TPR} + \\text{TNR} - 1$ ([the distance to the 45 degree identity line]{.green})\n\n\nor that minimizes \n\n- **D-optimal Threshold** $= \\sqrt{(1 - \\text{TPR})^2 + (1 - \\text{TNR})^2}$ ([the distance to the optimal point]{.green})\n\n\n\n::: notes\n\nG-mean used When dealing with imbalanced datasets (rare events).\n\nYouden’s J Index used when Equal importance of positives & negatives.\n\nD-optimal Threshold Does not assume class balance, unlike some other metrics. When minimizing both false positives and false negatives.\n\n:::\n\n## Optimal Threshold\n\nWith *Youden’s J index*, the optimal threshold is $3.18\\%$.\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/09-class-glm/unnamed-chunk-26-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: notes\n\n- The predicted probabilities are not calibrated, e.g. those predicted by an SVM or decision tree.\n- The metric used to train the model is different from the metric used to evaluate a final model.\n- The class distribution is severely skewed.\n- The cost of one type of misclassification is more important than another type of misclassification.\n- Learning from Imbalanced Data Sets 1st ed. 2018 Edition\n- https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\nThe index is represented graphically as the height above the chance line, and it is also equivalent to the area under the curve subtended by a single operating point.\nhttps://web.expasy.org/pROC/files/pROC_1.7.2_R_manual.pdf\n\n:::\n\n\n\n\n# Multinomial Logistic Regression\n\n\n## Multinomial Logistic Regression\n\n- When classifying $K > 2$ categories, we can consider **multinomial logistic regression**^[Sometimes it is known as **multiclass logistic regression**.].\n\n- The response should be **nominal**. If $Y_i$ is **ordinal**, we should consider the **ordinal regression**.\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/09-class-glm/covid_lung.jpeg){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](./images/09-class-glm/car_brand.jpeg){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::::\n\n\n\n\n## Multinomial Link\n\n- First select a single class to serve as the *baseline*, the $K$th class for example.^[The coefficient estimates will differ if a different baseline is used. But the fitted values, predictions, log odds between any pair of classes remain the same.] \n\n::: {style=\"font-size: 0.9em\"}\n\nFor $k = 1, 2, \\dots, K-1,$\n\n$$Pr(Y = k \\mid {\\bf x}) = \\dfrac{e^{\\beta_{k0}+\\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p}}{ \\sum_{l=1}^{K}e^{\\beta_{l0}+\\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}},$$ and $$Pr(Y = K \\mid {\\bf x}) = \\dfrac{1}{ \\sum_{l=1}^{K}e^{\\beta_{l0}+\\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}}.$$\n:::\n\n. . .\n\n::: {style=\"font-size: 0.9em\"}\n\nFor $k = 1, \\dots, K-1$, $$\\log\\left( \\frac{Pr(Y = k \\mid \\bx)}{Pr(Y = K \\mid \\bx)} \\right) = \\beta_{k0}+\\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p.$$\n\n:::\n\n## Multinomial Link\n\n- First select a single class to serve as the *baseline*, the $K$th class for example.^[The coefficient estimates will differ if a different baseline is used. But the fitted values, predictions, log odds between any pair of classes remain the same.]\n\n- $\\pi_{ik} = Pr(Y_i = k \\mid {\\bf x}_i)$: the probability that the $i$-th response falls in the $k$-th category.\n\n- $\\sum_{k = 1}^K \\pi_{ik} = 1$ for each observation $i$.\n\n. . .\n\n- For $k = 1, \\dots, K-1$, the link $\\eta_{ik} = \\log\\left( \\frac{\\pi_{ik}}{\\pi_{iK}}\\right) = \\beta_{k0}+\\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p = {\\bf x}_i'\\bbeta_k$, where $\\bbeta_k = (\\beta_{k0}, \\beta_{k1}, \\dots, \\beta_{kp})'$\n\n:::{.question}\nWhat is the value of $\\eta_{iK}$?\n:::\n\n. . .\n\nFor $k = 1, \\dots, K$,\n\n$$\\pi_{ik} = \\dfrac{e^{\\eta_{ik}}}{\\sum_{l=1}^{K}e^{\\eta_{il}}};  \\left(\\pi_{iK} = \\dfrac{1}{\\sum_{l=1}^{K}e^{\\eta_{il}}} \\right)$$\n\n\n\n## [Multinomial Data](https://stats.oarc.ucla.edu/r/dae/multinomial-logistic-regression/)\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 200\nColumns: 3\n$ prog  <fct> vocation, general, vocation, vocation, vocation, general, vocati…\n$ ses   <fct> low, middle, high, low, middle, high, middle, middle, middle, mi…\n$ write <dbl> 35, 33, 39, 37, 31, 36, 36, 31, 41, 37, 44, 33, 31, 44, 35, 44, …\n```\n\n\n:::\n:::\n\n- Response:\n  + `prog` (program type: `general`, `academic`, `vocation`)\n  \n- Predictors:\n  + `ses` (social economic status: `low`, `middle`, `high`)\n  + `write` (writing score)\n  \n::: notes\nEntering high school students make program choices among general program, vocational program and academic program. Their choice might be modeled using their writing score and their social economic status.\n:::\n\n\n\n## Multinomial Logit - Variable Association\n\n*Program* type is affected by *social economic status*\n\n::: {style=\"font-size: 1.2em\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n        prog\nses      general academic vocation\n  low         16       19       12\n  middle      20       44       31\n  high         9       42        7\n```\n\n\n:::\n:::\n\n:::\n\n. . .\n\nand *writing score*\n\n::: {style=\"font-size: 1.2em\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n          M  SD\ngeneral  51 9.4\nacademic 56 7.9\nvocation 47 9.3\n```\n\n\n:::\n:::\n\n:::\n\nThe level (coding order) of `prog` is\n\n::: {style=\"font-size: 1.2em\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"general\"  \"academic\" \"vocation\"\n```\n\n\n:::\n:::\n\n:::\n\n\n\n\n\n\n\n## Multinomial Logit - Setting Baseline\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmultino_data$prog2 <- relevel(multino_data$prog, ref = \"academic\")\nlevels(multino_data$prog2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"academic\" \"general\"  \"vocation\"\n```\n\n\n:::\n:::\n\n- `academic` = 1 (baseline), `general` = 2, `vocation` = 3\n- For `ses`, `low` = 1 (baseline), `middle` = 2, `high` = 3\n\n. . .\n\n\n::: {style=\"font-size: 0.8em\"}\n\n$$\\log \\left( \\frac{Pr(prog = general)}{Pr(prog = academic)}\\right) = b_{10} + b_{11}I(ses = 2) + b_{12}I(ses = 3) + b_{13}write$$\n\n$$\\log \\left( \\frac{Pr(prog = vocation)}{Pr(prog = academic)}\\right) = b_{20} + b_{21}I(ses = 2) + b_{22}I(ses = 3) + b_{23}write$$\n\n:::\n\n*All others held constant*,\n\n- $b_{13}$: the amount changed in the *log odds* of being in `general` vs. `academic` program as `write` increases one unit.\n\n- $b_{21}$ the amount changed in the *log odds* of being in `vocation` vs. `academic` program if moving from `ses = \"low\"` to `ses = \"middle\"`.\n\n\n\n::: notes\n\nset \"academic\" as the baseline category\nfactor(multino_data$prog, levels = c(\"general\", \"vocation\", \"academic\")\nb13 A one-unit increase in the variable write is associated with the decrease in the log odds of being in general program vs. academic program in the amount of .058 .\nb23 A one-unit increase in the variable write is associated with the decrease in the log odds of being in vocation program vs. academic program. in the amount of .1136 .\nb12 The log odds of being in general program vs. in academic program will decrease by 1.163 if moving from ses=\"low\" to ses=\"high\".\nb11 The log odds of being in general program vs. in academic program will decrease by 0.533 if moving from ses=\"low\"to ses=\"middle\", although this coefficient is not significant.\nb22 The log odds of being in vocation program vs. in academic program will decrease by 0.983 if moving from ses=\"low\" to ses=\"high\".\nb21 The log odds of being in vocation program vs. in academic program will increase by 0.291 if moving from ses=\"low\" to ses=\"middle\", although this coefficient is not significant.\n\n:::\n\n\n## Multinomial Logit - [`nnet::multinom()`](https://cran.r-project.org/web/packages/nnet/nnet.pdf)\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmultino_fit <- nnet::multinom(prog2 ~ ses + write, data = multino_data, trace = FALSE)\nsumm <- summary(multino_fit)\nsumm$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         (Intercept) sesmiddle seshigh  write\ngeneral          2.9     -0.53   -1.16 -0.058\nvocation         5.2      0.29   -0.98 -0.114\n```\n\n\n:::\n:::\n\n. . .\n\n<br>\n\n- `glmnet()` uses the **softmax coding** that treats *all* $K$ classes *symmetrically*, and coefficients have a different meaning. (ISL eq. (4.13), (4.14))\n\n- The fitted values, predictions, log odds between any pair of classes, and other key model outputs remain the same.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglmnet(x = input_matrix, y = categorical_vector, family = \"multinom\", \n       family = \"multinomial\", lambda = 0)\n```\n:::\n\n\n\n::: notes\n- Estimate coefficients for all $K$ classes.\n- The fitted values, predictions, log odds between any pair of classes, and other key model outputs remain the same.\n:::\n\n\n## Multinomial Logit - Odds (Relative Risk) {visibility=\"hidden\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n         (Intercept) sesmiddle seshigh write\ngeneral           17      0.59    0.31  0.94\nvocation         185      1.34    0.37  0.89\n```\n\n\n:::\n:::\n\n- The relative risk ratio for a one-unit increase in the variable write is .9437 for being in general program vs. academic program.\n- The relative risk ratio switching from ses = 1 to 3 is .3126 for being in general program vs. academic program.\n\n\n## Multinomial Logit - Estimated Probability\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(fitted(multino_fit))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  academic general vocation\n1     0.15    0.34     0.51\n2     0.12    0.18     0.70\n3     0.42    0.24     0.34\n4     0.17    0.35     0.48\n5     0.10    0.17     0.73\n6     0.35    0.24     0.41\n```\n\n\n:::\n:::\n\n- Hold `write` at its *mean* and examine the predicted probabilities for each level of `ses`.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndses <- data.frame(ses = c(\"low\", \"middle\", \"high\"), \n                   write = mean(multino_data$write))\npredict(multino_fit, newdata = dses, type = \"probs\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  academic general vocation\n1     0.44    0.36     0.20\n2     0.48    0.23     0.29\n3     0.70    0.18     0.12\n```\n\n\n:::\n:::\n\n\n::: notes\nhttps://stackoverflow.com/questions/22905807/how-does-the-function-multinom-from-r-package-nnet-compute-the-multinomial-proba\n:::\n\n\n\n## Multinomial Logit - Estimated Probability {visibility=\"hidden\"}\n\n- Probabilities for different values of `write` within each level of `ses`.\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\npred_prob_write$ses: high\nacademic  general vocation \n    0.62     0.18     0.20 \n------------------------------------------------------------ \npred_prob_write$ses: low\nacademic  general vocation \n    0.40     0.33     0.27 \n------------------------------------------------------------ \npred_prob_write$ses: middle\nacademic  general vocation \n    0.43     0.20     0.37 \n```\n\n\n:::\n:::\n\n\n## Multinomial Logit - Probability Curve\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/09-class-glm/unnamed-chunk-41-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Softmax Coding^[The fitted values, predictions, log odds between any pair of classes, and other key model outputs remain the same.] {visibility=\"hidden\"}\n\n\n- Treat *all* $K$ classes *symmetrically*,\n\n$$\\pi_{ik} = Pr(Y_i = k \\mid {\\bf x}) = \\dfrac{e^{\\beta_{k0}+\\beta_{k1}x_{i1} + \\cdots + \\beta_{kp}x_{ip}}}{ \\sum_{l=1}^{K}e^{\\beta_{l0}+\\beta_{l1}x_{i1} + \\cdots + \\beta_{lp}x_{ip}}}; \\quad \\pi_{iK} \\ne \\dfrac{1}{ \\sum_{l=1}^{K}e^{\\beta_{l0}+\\beta_{l1}x_{i1} + \\cdots + \\beta_{lp}x_{ip}}}$$\n\n\n\n\nFor any classes $k$ and $k' \\in\\{1, 2, \\dots, K \\}$, $$\\eta_{ikk'} = \\log\\left( \\frac{\\pi_{ik}}{\\pi_{ik'}} \\right) = (\\beta_{k0} - \\beta_{k'0}) + (\\beta_{k1} - \\beta_{k'1}) x_{i1} + \\cdots + (\\beta_{kp} - \\beta_{k'p})x_{ip}$$\n\n\n- `glmnet()` uses the softmax coding. \n\n\n::: notes\n- Estimate coefficients for all $K$ classes.\n:::\n\n\n# Generalized Linear Models\n\n## [Generalized Linear Models](https://en.wikipedia.org/wiki/Generalized_linear_model) in Greater Generality\n\n- *Conditional on $\\bX$, the response $Y$ belongs to a certain family of distribution.*^[Usually the distribution of $Y$ belongs to the [exponential family](https://en.wikipedia.org/wiki/Exponential_family).]\n  + Linear regression: Gaussian\n  + Logistic regression: Bernoulli, binomial, multinomial\n\n  <!-- + Poisson regression: Poisson -->\n\n. . .\n\n- *Model the mean of $Y$ as a function of the predictors.*\n  + Linear regression: $E(Y \\mid \\bx) = \\bx'\\bbeta$\n  + Logistic regression: $E(Y \\mid \\bx) = P(Y=1 \\mid \\bx) = \\pi(\\bx) = \\frac{e^{\\bx'\\bbeta}}{1 + e^{\\bx'\\bbeta}}$\n  \n  <!-- + Poisson regression: $E(Y \\mid \\bx) = \\lambda(\\bx) = e^{\\bx'\\bbeta}$ -->\n\n. . .\n\n- In general, with $E(Y \\mid \\bx) = \\mu$, the link function $\\eta(\\mu) = {\\bf x}'\\bbeta$ transforms $\\mu$ so that the transformed mean is a linear function of predictors!\n  + Linear regression: $\\eta(\\mu) = \\mu$\n  + Logistic regression: $\\eta(\\mu) = \\log(\\mu/(1-\\mu))$\n\n\n::: notes\nAny regression approach that follows this general recipe is\nknown as a generalized linear model (GLM).\n:::\n\n\n## Link Function\n\n- Logistic regression uses the logit function $\\eta = g(\\pi) = \\ln\\left( \\dfrac{\\pi}{1-\\pi}\\right)$ as the link function.\n- $\\pi = \\dfrac{1}{1 + e^{-\\eta}} \\in (0, 1)$ is the CDF of the logistic distribution, whose PDF is of the from\n$$f(\\eta) = \\frac{e^{-\\eta}}{(1 + e^{-\\eta})^2}, \\quad \\eta \\in(-\\infty, \\infty)$$\n\n\n- Could use a different link function and hence CDF to describe the probability curve.\n\n\n## Probit and Complementary log-log (cloglog) Link\n\n- If $\\pi = \\Phi(\\eta) \\in (0, 1)$, $\\Phi(\\cdot)$ is the CDF of standard normal distribution, the link function $$\\eta = g(\\pi) = \\Phi^{-1}(\\pi)$$ is called **probit**, and the model is called probit model or probit regression.\n\n. . .\n\n- Another popular link function is the **complementary log-log link**: \n$$\\eta = g(\\pi) = \\log\\left( -\\log(1-\\pi)\\right)$$\nwhere $\\pi = 1 - \\exp(-\\exp(\\eta))$ is the CDF of the so called Gumbel distribution.^[The complementary log-log link is actually defined by a *negative* Gumbel random variable. No worries about why it is used at this moment.]\n\n\n## Link Function Summary\n\n| Distribution | Link Function $\\eta = g(\\pi)$ | CDF $\\pi = g^{-1}(\\eta)$ | PDF\n|:-------:|:-------:|:-------:|:-------:|\n| Logistic | logit: $\\ln\\left( \\dfrac{\\pi}{1-\\pi}\\right)$ | $\\dfrac{1}{1 + e^{-\\eta}}$ | $\\dfrac{e^{-\\eta}}{(1 + e^{-\\eta})^2}$\n| Normal | probit: $\\Phi^{-1}(\\pi)$ | $\\Phi(\\eta)$ | $\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{\\eta^2}{2}}$\n| Gumbel | cloglog: $\\log\\left( -\\log(1-\\pi)\\right)$ | $1 - \\exp(-\\exp(\\eta))$| $e^{-(\\eta + e ^ {-\\eta})}$\n\n. . .\n\nAny transformation that *maps probabilities into the real line* could be used to produce a GLM for binary classification, as long as the transformation is *one-to-one continuous and differentiable*.\n\n$$\\pi = F(\\eta)$$\n$$\\eta = F^{-1}(\\pi)$$\n\n\n\n## Link Function Summary\n\n| Name | Link Function $\\eta = g(\\pi)$ \n|:-------:|:-------:|\n| logit | $\\ln\\left( \\dfrac{\\pi}{1-\\pi}\\right)$ \n| probit | $\\Phi^{-1}(\\pi)$\n| cloglog | $\\log\\left( -\\log(1-\\pi)\\right)$ \n\n- Logit and probit links produce similar curve. For simple regression, both estimate $\\pi = 1/2$ when $x = -\\beta_0/\\beta_1$ and exhibit symmetric behavior.\n- The complementary log-log link is not symmetric.\n\n\n\n## PDF\n\n- Normal and logistic density are symmetric, while Gumbel is right skewed.\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/09-class-glm/unnamed-chunk-42-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n## Probability Curve\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/09-class-glm/unnamed-chunk-43-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\n## Probit and Complementary log-log Links\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprobit_fit <- glm(default ~ balance, data = Default, family = binomial(link = \"probit\"))\ncloglog_fit <- glm(default ~ balance, data = Default, family = binomial(link = \"cloglog\"))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/09-class-glm/unnamed-chunk-45-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n## Other Topics\n\n- Repeated measures and binomial logistic regression\n\n- Regression diagnostics for binary data\n\n- Penalized logistic regression\n\n- Exponential family\n\n- Poisson regression\n\n::: notes\n\nhttps://grodri.github.io/glms/\n\nhttps://stats.oarc.ucla.edu/r/dae/multinomial-logistic-regression/\n\nhttps://glmnet.stanford.edu/articles/glmnet.html\n\n<!-- # Poisson Regression {background-color=\"#447099\"} -->\n<!-- ##  -->\n\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
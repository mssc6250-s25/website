---
title: "Overview of Statistical Machine Learning `r emo::ji('computer')`"
subtitle: "MSSC 6250 Statistical Machine Learning"
author: "Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University"
# date: "`r format(Sys.time(), '%B %d %Y')`"
format: 
  revealjs:
    html-math-method: katex
    highlight-style: github
    code-block-bg: true
    self-contained: false
    slide-number: c/t
    incremental: false
    width: 1800
    height: 1000
    margin: 0.05
    logo: "https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg"
    footer: "[mssc6250-s25.github.io/website](https://mssc6250-s25.github.io/website/)"
    theme: ["simple", "styles.scss"]
    echo: false
    multiplex: true
    code-link: true
    fig-cap-location: bottom
    fig-align: center
    transition: none ## fade slide convex concave zoom
    title-slide-attributes:
      data-background-color: "#447099"
      # data-background-image: images/paper-texture.jpg
      # data-background-size: cover
      # data-background-color: "#698ED5"
editor: source
execute:
  freeze: auto
---


# {visibility="hidden"}


\def\bx{\mathbf{x}}
\def\bg{\mathbf{g}}
\def\bw{\mathbf{w}}
\def\bbeta{\boldsymbol \beta}
\def\bX{\mathbf{X}}
\def\by{\mathbf{y}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\T{\text{T}}
\def\cov{\mathrm{Cov}}
\def\cor{\mathrm{Corr}}
\def\var{\mathrm{Var}}
\def\E{\mathrm{E}}
\def\bmu{\boldsymbol \mu}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\Trace{\text{Trace}}


```{r}
#| label: setup
#| include: false
#| eval: true
# library(countdown)
library(emo)
library(knitr)
library(kableExtra)
# library(gt)
# library(gtExtras)
# library(ggplot2)
# library(tidyverse)
# library(tidymodels)
# library(ISLR2)
# library(genridge)
# library(glmnet)
# library(gam)
# library(splines)
# library(MASS)

# library(ElemStatLearn)
knitr::opts_chunk$set(
    fig.asp = 0.618,
    fig.align = "center",
    out.width = "100%",
    fig.retina = 10,
    fig.path = "images/02-overview/",
    message = FALSE,
    global.par = TRUE
)
options(
  htmltools.dir.version = FALSE,
  dplyr.print_min = 6, 
  dplyr.print_max = 6,
  tibble.width = 80,
  width = 80,
  digits = 3
  )
hook_output <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
```



## Image/Object Recognition
:::: r-stack
![](./images/02-overview/maru1.JPG){.absolute top="100" left="0" width="500"}
![](./images/02-overview/fatty2.JPG){.absolute top="100" left="550" width="500"}
![](./images/02-overview/fatty3.JPG){.absolute top="100" left="1150" width="500"}
![](./images/02-overview/maru5.JPG){.absolute bottom="50" right="50" width="500"}

![](./images/02-overview/maru2.JPG){.absolute bottom="10" right="650" width="500"}

![](./images/02-overview/fatty1.JPG){.absolute bottom="10" right="1550" height="400"}

![](./images/02-overview/maru4.JPG){.absolute bottom="10" right="1250" height="400"}


![](./images/02-overview/cifar.png){.fragment width="1600"}

![](./images/02-overview/traffic-recog.jpg){.fragment width="1600"}
::::


::: notes
- Machine learning is everywhere.
- Can we train a machine to have an ability to recognize dogs and cats?
- hundreds of thousands of objects?
:::


## Recommender System
![](https://upload.wikimedia.org/wikipedia/commons/5/52/Collaborative_filtering.gif){fig-align="center" width="1600"}


::: notes
- watch one video on YouTube, then lots of other videos that never show up pop up.
- Check some items on Amazon, then it recommends some other items saying "you may also like XXX"
- This kind of recommendation relies on machine learning.
- How does the model know you may also like those items. Well the model learn this from other customer surfing and purchasing history.
:::



## COVID Detection
![](./images/02-overview/covid_lung.jpeg){fig-align="center" width="1600"}


## Stock Price Forecasting
![](./images/02-overview/stock.jpeg){fig-align="center" width="1600"}

## Risk Factors for Cancer

```{r}
#| out-width: 100%
#| fig-link: https://www.verywellhealth.com/cancer-causes-513773
knitr::include_graphics("./images/02-overview/cancer-risk.webp")
```




<!-- ## Credit Card Fraud Detection -->

## How We Solve Real-Life Problems?
- We describe and formulate our problems/questions by **models**.

. . .

- Solve problems/Answer questions by doing **inference** and/or **predictions** from the built model using the information from the data and computer algorithms.

. . .

::: r-fit-text
[So what is (Statistical) Machine Learning?]{.highlight-red}
:::



::: notes
- Mathematical/statistical or machine learning models.
- we know the eating habits from a data of 100 american people. How do we know the the eating habits of all the people in the united states.
- If we know someone's GPA and major, how can we predict his salary after graduation?
:::



## Machine Learning vs. Statistical Learning
- **Machine learning** (ML) is a field in Computer Science (CS).

> _"A **computer program** is said to learn from **experience** [(*E*)]{.red} with respect to some class of tasks [(*T*)]{.red} and performance measure [(*P*)]{.red} if its performance at tasks in *T*, as measured by *P*, [improves with experience]{style="color: red;"} *E*."_
-- [Tom Mitchell](http://www.cs.cmu.edu/~tom/mlbook.html), Professor of ML at Carnegie Mellon University

- *computer programs = learning software and system*

- *experience = data*

- *tasks = problems*

. . .

- The ML algorithms are mainly for predictive modeling problems, and many have been borrowed from Statistics, for example, linear regression.

- ML is a CS perspective on modeling data with a focus on algorithmic methods.

. . .

- Wait, I am a statistical modeler (computational statistician), and this is exactly what I am doing!



::: notes
Machine Learning is the study of computer algorithms that improve automatically through experience.
:::




## Machine Learning vs. Statistical Learning 
- **Statistical learning**, used to be called Applied Statistics (what a "fancy" name, ðŸ¤£), arose as a subfield of Statistics.

. . .

> _Statistical learning refers to a set of **tools for modeling and understanding complex datasets**._ -- [An Introduction to Statistical Learning](https://www.statlearning.com/)

> _...to extract important patterns and trends, and understand "what the data says." We call this **learning from data**._ -- [The Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/)

- *tools = mathematics, computing hardware/software/architecture, programming languages, algorithms, etc.*

- Statistical learning is a mathematical perspective on modeling data with a focus on goodness of fit.



::: notes
https://machinelearningmastery.com/relationship-between-applied-statistics-and-machine-learning/
:::




## Machine Learning vs. Statistical Learning 
- Machine learning emphasizes **algorithms** and **automation**.

- Statistical learning emphasizes **modeling**, **interpretability**, and **uncertainty**.

. . .

- The distinction is blur:
    + A machine learner needs a well-built statistical model that quantifies uncertainty about prediction
    + A statistical learner needs a computationally efficient algorithm to deal with large complex data.

. . .

- BUT, one thing for sure. Machine learning has the upper hand in marketing!

## Fancy Terms and Larger Grant!

![Source: http://statweb.stanford.edu/~tibs/stat315a/glossary.pdf](./images/02-overview/glossary.png)

##

```{r}
#| out-width: "100%"
#| fig-cap: "Source: All of Statistics"
#| cap-location: top
knitr::include_graphics("./images/02-overview/dictionary.png")
```


## Types of Learning 

```{r}
#| out-width: 82%
#| fig-cap: "Source: https://towardsdatascience.com/machine-learning-algorithms-in-laymans-terms-part-1-d0368d769a7b"
#| fig-align: center
knitr::include_graphics("./images/02-overview/ml_type.png")
```

## Supervised Learning
- Response $Y$ (output, outcome, target, label, dependent/endogenous variable)

- Vector of $p$ predictors $X = (X_1, X_2, \dots, X_p)$ (inputs, features, regressors, covariates, explanatory/exogenous/independent variable).

. . .

- **Regression**: $Y$ is numeric (e.g price, blood pressure). (if $p$ = 1, simple regression; if $p$ > 1, multiple regression)

- **Classification**: $Y$ is categorical (e.g survived/died, digit 0-9 ([MNIST](https://yann.lecun.com/exdb/mnist/)), cancer class of tissue sample).

. . .


- Training data $\mathcal{D} = \{(x_1, y_1), \dots ,(x_n , y_n)\} =\{(x_i, y_i)\}_{i=1}^n$, $x_i \in \mathbf{R}^p$, $y_i \in \mathbf{R}^d$.

. . .

Goal: Use training data ([*E*]{.red}) to *train* our model for better (w.r.t. [*P*]{.red}) inference/prediction ([*T*]{.red}) on the response.

- Learn a **mapping** from inputs to outputs $Y = f(X)$.




## Regression Example
- Goal: Establish the relationship between salary and demographic variables.

```{r}
#| fig-cap: "Source: ISL Fig. 1.1"
knitr::include_graphics("./images/02-overview/regression_wage.png")
```


## Statistics Nah... Machine Learning Neat!
```{r}
#| out-width: "78%"
#| fig-cap: "Source: https://towardsdatascience.com/the-actual-difference-between-statistics-and-machine-learning-64b49f07ea3"
knitr::include_graphics("./images/02-overview/regression2019.jpeg")
```

## Statistics Nah... Machine Learning Neat!
```{r}
#| out-width: "80%"
knitr::include_graphics("./images/02-overview/regression_tweet.png")
```


## Classification Example
- Goal: Build a customized spam filtering system

```{r}
#| out-width: "55%"
#| fig-cap: "Source: http://penplusbytes.org/strategies-for-dealing-with-e-mail-spam/"
knitr::include_graphics("./images/02-overview/spam_filter.jpg")
```


## Classification Example
```{r}
#| out-width: "50%"
#| fig-cap: "Source: https://www.yesware.com/blog/email-spam/"
knitr::include_graphics("./images/02-overview/spam.png")
```

- Data: 4601 emails sent to George at HP, before 2000. Each is labeled as spam or email.
- Inputs: relative frequencies of 57 of the commonly occurring words and punctuation marks.


```{r}
#| out-width: "50%"
#| fig-cap: "Source: ESL Tbl. 1.1"
knitr::include_graphics("./images/02-overview/spam_data.png")
```




## Objectives of Supervised Learning
Based on the training data ([*E*]{.red}) we'd like to :

- ([*T*]{.red}) **Prediction**: [Accurately predict unseen test cases.]{style="color:green"}
    + <span style="color:blue">Given a new test input (age, year, education), what is predicted salary level?</span>
    + <span style="color:blue">Given a new bunch of words in the email, the email is a spam or normal message?</span>

. . .

- ([*T*]{.red}) **Inference** [Understand which inputs affect the outcome, and how.]{style="color:green"}
    + <span style="color:blue">If education is up one level, how much salary will increase on average?</span>
    + <span style="color:blue">If "!" increases one more time, how much percentage does the probability of being labeled as spam go up?</span>
    
. . .

- ([*P*]{.red}) [Assess the quality of our predictions and inferences.]{style="color:green"}
    + Use evalution metrics to assess the performance of a machine learning model/algorithm.
    
    
## Unsupervised Learning
- No outcome variable, just a set of features measured on a set of samples. 
- Our training sample is $\mathcal{D} = \{x_1, \dots, x_n\}$, $x_i \in \mathbf{R}^p$.

. . .

- Objective is more fuzzy
    + find groups of samples or features that behave similarly **(Clustering)**
    + find linear combinations of features with the most variation **(Dimension Reduction)**.
    
. . .

- Difficult to know how well you are doing.
- Can be useful as a pre-processing step for supervised learning.




## Clustering Example
- Customer Segmentation: dividing customers into groups or clusters on the basis of common characteristics. 

```{r}
#| out-width: "56%"
#| fig-cap: "Source: https://www.datacamp.com/community/tutorials/introduction-customer-segmentation-python"

knitr::include_graphics("./images/02-overview/clustering.png")
```

## Deep Learning
- An (artifical) **neural network** is a machine learning model inspired by the biological neural networks that constitute animal brains.

::: columns
::: {.column width="70%"}
![Source: Wiki](https://upload.wikimedia.org/wikipedia/commons/4/44/Neuron3.png){width="1000"}
:::

:::{.column width="30%"}
![](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg){width="600"}
:::

:::


##
A neural network with several hidden layers is called a deep neural network, or **deep learning**.

![Source: ISL Ch 10](./images/02-overview/deeplearning.png){fig-align="center" width="1600"}

## Accuracy and Interpretability Trade-Off
<!-- <iframe width="560" height="315" src="./images/02-overview/2_7.pdf"> -->

<!-- </iframe> -->


![Source: ISL Fig. 2.7](./images/02-overview/2_7.png){fig-align="center" width="1500"}


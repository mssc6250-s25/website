---
title: 'Ridge Regression and Cross Validation `r fontawesome::fa("wave-square")`'
subtitle: "MSSC 6250 Statistical Machine Learning"
author: "Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University"
# date: "`r format(Sys.time(), '%B %d %Y')`"
# macros: _macros.tex # import a list of TeX/LaTeX definitions
format: 
  revealjs:
    html-math-method:
      method: mathjax
      url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    # include-in-header:
    #     - "macros.tex"
    highlight-style: arrow
    code-block-bg: true
    code-block-border-left: "#31BAE9"
    self-contained: false
    slide-number: c/t
    incremental: false
    width: 1800
    height: 1000
    margin: 0.05
    logo: "https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg"
    footer: "[mssc6250-s25.github.io/website](https://mssc6250-s25.github.io/website/)"
    theme: ["simple", "styles.scss"]
    echo: false
    multiplex: true
    code-link: true
    fig-cap-location: bottom
    fig-align: center
    transition: none ## fade slide convex concave zoom
    title-slide-attributes:
      data-background-color: "#447099"
      # data-background-image: images/paper-texture.jpg
      # data-background-size: cover
      # data-background-color: "#698ED5"
editor: source
execute:
  freeze: true
  echo: false
---


# {visibility="hidden"}


\def\bx{\mathbf{x}}
\def\bg{\mathbf{g}}
\def\bw{\mathbf{w}}
\def\bbeta{\boldsymbol \beta}
\def\bX{\mathbf{X}}
\def\by{\mathbf{y}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\T{\text{T}}
\def\cov{\mathrm{Cov}}
\def\cor{\mathrm{Corr}}
\def\var{\mathrm{Var}}
\def\E{\mathrm{E}}
\def\bmu{\boldsymbol \mu}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\Trace{\text{Trace}}





```{r}
#| label: setup
#| include: false
#| eval: true
library(countdown)
library(knitr)
library(gt)
library(gtExtras)
library(ggplot2)
library(ISLR2)
library(genridge)
library(glmnet)
# library(ElemStatLearn)
knitr::opts_chunk$set(
    fig.asp = 0.618,
    fig.align = "center",
    out.width = "100%",
    fig.retina = 10,
    fig.path = "images/05-ridge-cv/",
    message = FALSE,
    global.par = TRUE
)
options(
  htmltools.dir.version = FALSE,
  dplyr.print_min = 6, 
  dplyr.print_max = 6,
  tibble.width = 80,
  width = 80,
  digits = 3
  )
hook_output <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
```





# Ridge Regression {background-color="#A7D5E8"}



## When Ordinary Least Squares (OLS) Doesn't Work Well: Collinearity
- When predictors are highly correlated, $\var(b_j)$ is much inflated.^[Although $\var(b_j)$ is still the smallest among all linear unbiased estimators.]
  + A tiny change in the training set causes a large change in $b_j$, leading to *unreliable* estimation and possibly prediction.

. . .

- ${\bf X'X} = \begin{bmatrix} 1 & 0.99 \\ 0.99 & 1 \end{bmatrix}$ $\quad ({\bf X'X})^{-1} = \begin{bmatrix}  50.3 & -49.7 \\ -49.7 & 50.3 \end{bmatrix}$

- $\var(b_j) = 50.3\sigma^2$
  + An increase in 50-fold over the ideal case when the two regressors are orthogonal.


```{r}
#| echo: true
solve(matrix(c(1, 0.99, 0.99, 1), 2, 2))
```


::: notes
the eigen vector direction with the smallest eigen value
https://online.stat.psu.edu/stat857/node/155/
https://robjhyndman.com/hyndsight/loocv-linear-models/
:::

## When OLS Doesn't Work Well: Optimization Perspective

- $\beta_1 = \beta_2 = 1$ and $\cor(x_1, x_2) = 0.99$

- The relatively "flat" valley in the objective function walks along the eigen-vector of ${\bf X}'{\bf X}$ having the smallest eigen-value.

:::: columns
::: {.column width="60%"}
```{r}
#| out-width: "100%"
library(MASS)
set.seed(2023)
n <- 50
# create highly correlated variables and a linear model
X <- mvrnorm(n, c(0, 0), matrix(c(1, 0.99, 0.99, 1), 2, 2))
y <- rnorm(n, mean = X[, 1] + X[, 2])
beta1 <- seq(-1, 3, 0.005)
beta2 <- seq(-1, 3, 0.005)
allbeta <- data.matrix(expand.grid(beta1, beta2))
rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2), X, y),
              length(beta1), length(beta2))
  
# quantile levels for drawing contour
quanlvl = c(0.01, 0.025, 0.05, 0.2, 0.5, 0.75)
par(mar = c(3, 3, 0, 0))
# plot the contour
contour(beta1, beta2, rss, levels = quantile(rss, quanlvl), las = 1,lwd = 3, 
        xlab = "beta 1", ylab = "beta 2")
box()
  
# the truth
points(1, 1, pch = 19, col = "red", cex = 2)
  
# the data 
betahat <- coef(lm(y~X-1))
points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 2)
legend("topright", c("true beta value", "OLS estimates"), 
       col = c("red", "blue"), pch = 19, bty = "n")
```
:::

::: {.column width="40%"}
```{r}
#| echo: true
eigen(
    matrix(c(1, 0.99, 0.99, 1), 
             2, 2)
    )
```
:::
::::


::: notes

- From optimization point of view, the objective function ($\ell_2$ loss) is "flat" along certain directions in the parameter domain.
- a small eigen-value in XTX makes the corresponding eigen-value large in the inverse.
:::


## When OLS Doesn't Work Well: Optimization Perspective

- A little change in the training set perturbs the objective function. The LSEs lie on a valley centered around the truth.

```{r}
#| out-width: "100%"
#| cache: true
par(mfrow = c(2, 2))
par(mar = c(2, 2, 0, 0))
set.seed(2024)
  for (i in 1:4)
  {
    # create highly correlated variables and a linear model
    X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2))
    y = rnorm(n, mean = X[,1] + X[,2])
  
    beta1 <- seq(-1, 3, 0.005)
    beta2 <- seq(-1, 3, 0.005)
    allbeta <- data.matrix(expand.grid(beta1, beta2))
    rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2), X, y), 
                    length(beta1), length(beta2))
      
    # quantile levels for drawing contour
    quanlvl = c(0.01, 0.025, 0.05, 0.2, 0.5, 0.75)
      
    # plot the contour
    contour(beta1, beta2, rss, levels = quantile(rss, quanlvl), las = 1)
    box()
      
    # the truth
    points(1, 1, pch = 19, col = "red", cex = 2)
      
    # the data 
    betahat <- coef(lm(y~X-1))
    points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 2)
  }
```


## When OLS Doesn't Work Well: High Variance

- The optimizer could land anywhere along the valley, leading to *large variance* of the LSE.

- Over many simulation runs, the LSE lies around the line of $\beta_1 + \beta_2 = 2$, the direction of the eigen-vector of the smallest eigen-value.


:::: columns
::: column
```{r}
par(mar = c(2, 2, 2, 0))
  # the truth
  plot(1, 1, xlim = c(-1, 3), ylim = c(-1, 3), xlab = "", ylab = "", las = 1,
       pch = 19, col = "red", cex = 2, main = "200 LSEs w/ highly correlated predictors",
       cex.main = 2)
  
  # generate many datasets in a simulation 
  for (i in 1:200)
  {
    X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2))
    y = rnorm(n, mean = X[,1] + X[,2])
    
    betahat <- coef(lm(y~X-1))
    points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 0.5)
    points(1, 1, pch = 19, col = "red", cex = 2)
  }
  
```
:::

::: column
```{r}
par(mar = c(2, 2, 2, 0))
  # the truth
  plot(1, 1, xlim = c(-1, 3), ylim = c(-1, 3), 
       pch = 19, col = "red", cex = 2, xlab = "", ylab = "", las = 1,
       main = "200 LSEs w/ orthogonal predictors", cex.main = 2)
  
  # generate many datasets in a simulation 
  for (i in 1:200)
  {
    X = mvrnorm(n, c(0, 0), matrix(c(1,0, 0, 1), 2,2))
    y = rnorm(n, mean = X[,1] + X[,2])
    
    betahat <- coef(lm(y~X-1))
    points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 0.5)
    points(1, 1, pch = 19, col = "red", cex = 2)
  }
```

:::

::::




## When OLS Doesn't Work Well: Large-$p$-small-$n$ (High Dimensional Data)
- OLS stays well in the world of "large-$n$-small-$p$".

- When $p > n$, ${\bf X}'{\bf X}$ is not invertible.

- There is no unique $\bbeta$ estimate.

. . .


[**Intuition**: *Too many degrees of freedom ($p$) to specify a model, but not enough information ($n$) to decide which one is the one.*]{style="font-size: 1.2em"}

- Too flexible and ends up with overfitting



## Remedy for Large Variance and Large-$p$-small-$n$

- Make ${\bf X}'{\bf X}$ invertible when $p > n$ by [regularizing]{.blue} coefficient behavior! 

- A good estimator *balances bias and variance well*, or *minimizes the mean square error* $$\text{MSE}(\hat{\beta}) = E[(\hat{\beta} - \beta)^2] = \var(\hat{\beta}) + \text{Bias}(\hat{\beta})^2$$

- A slightly **biased** estimator $\hat{\bbeta}$ that has *much smaller variance and MSE* than the LSE ${\bf b}$.

:::: columns
::: column
```{r}
#| out-width: "90%"
#| fig-asp: 0.6
par(mar = c(2, 0, 2, 0), mgp = c(0.5, 0.5, 0), las = 1)
x <- seq(-6, 6, by = 0.1)
plot(x, dnorm(x), type = "l", lwd = 3, axes = F, xlab = "", ylab = "",
     xlim = c(-8, 8))
title(main = list(expression(paste("Sampling distribution a biased estimator ", hat(beta))), cex = 2))
axis(1, at = c(-8, -1, 0, 8), 
     labels = c("", expression(bold(beta)), expression(E(hat(beta))), ""), 
     tick = T, tck = -0.01, cex.axis=1.5)
segments(x0 = 0, y0 = 0, x1 = 0, y1 = dnorm(0), lty = 2)
segments(x0 = -1, y0 = 0, x1 = -1, y1 = dnorm(-1), lty = 2)
text(3.5, dnorm(1), expression(paste(E(hat(beta)) != bold(beta), " (biased)")),
     cex = 1.5)
text(4, dnorm(1.5), expression(paste(Var(hat(beta)), " small")),
     cex = 1.5)
```
:::

::: column
```{r}
#| out-width: "100%"
#| fig-asp: 0.3
par(mar = c(2, 0, 2, 0), mgp = c(0.5, 0.5, 0), las = 1)
x <- seq(-3, 3, by = 0.1)
plot(x, dnorm(x), type = "l", lwd = 3, axes = F, xlab = "", ylab = "")
title(main = list("Sampling distribution of LSE b", cex = 2))
axis(1, at = c(-3, 0.1, 3), labels = c("", expression(bold(beta) == E(b)), ""), 
     tick = T, tck = -0.01, cex.axis=1.5)
segments(x0 = 0, y0 = 0, x1 = 0, y1 = dnorm(0), lty = 2)
text(2, dnorm(1), expression(paste("E(b) = ", bold(beta), " (unbiased)")),
     cex = 1.5)
text(2.5, dnorm(1.5), paste("Var(b) large"), cex = 1.5)
```
:::
::::

## Ridge Regression

- Idea: Add a *ridge* (diagonal matrix) to ${\bf X} ' {\bf X}$.^[This is a special case of the [Tikhonov regularization](https://en.wikipedia.org/wiki/Ridge_regression#Tikhonov_regularization).]
$$\widehat{\bbeta}^\text{r} = (\bX' \bX + n \lambda \bI)^{-1} \bX' \by,$$

. . .

- To regularize coefficients, add an $\ell_2$ **penalty** to the residual sum of squares, for some *tuning parameter* $\lambda > 0$. 



$$
\begin{align}
\widehat{\bbeta}^\text{r} =& \, \argmin_{\bbeta} \underbrace{\lVert \by - \bX \bbeta\rVert_2^2}_{SS_{res}} + n \lambda \lVert\bbeta\rVert_2^2\\
=& \, \argmin_{\bbeta} \underbrace{\frac{1}{n} \sum_{i=1}^n (y_i - x_i'\bbeta)^2}_{\text{MSE}_{\texttt{Tr}}} + \lambda \sum_{j=1}^p \beta_j^2\\
=& \, \argmin_{\bbeta} \color{green} - \text{ goodness of fit } + \text{ model complexity/flexibility}
\end{align}
$$


## Properties of Ridge Regression

$$
\begin{align}
\widehat{\bbeta}^\text{r} =& \argmin_{\bbeta} \frac{1}{n} \sum_{i=1}^n (y_i - x_i'\bbeta)^2 + \lambda \sum_{j=1}^p \beta_j^2,
\end{align}
$$

Properties of ridge regression:

- Has *less* degrees of freedom in the sense that [_the cost gets higher when larger coefficients are used_]{.green}.

. . .

- Favors small-magnitude coefficient estimates (**Shrinkage**) to avoid cost penalty. 

. . .

- The shrinkage parameter $\lambda$ controls the degree of penalty.
  + $\lambda \rightarrow 0$: No penalty, and $\widehat{\bbeta}^\text{r} = \bf b$.
  + $\lambda \rightarrow \infty$: Unbearable penalty, and $\widehat{\bbeta}^\text{r} \rightarrow \mathbf{0}$
  
  
  
  
## Ridge Penalty

:::: {.columns}

::: {.column width="50%"}


$$\lambda \lVert \bbeta \rVert^2_2 = \lambda \bbeta' \bI \bbeta$$

- The penalty contour is *circle-shaped*

- Force the objective function to be more *convex*

- A more stable or less varying solution
:::

::: {.column width="50%"}
```{r}
#| fig-asp: 1
pen <- matrix(apply(allbeta, 1, function(b) 3 * b %*% b),
              length(beta1), length(beta2))
par(mar = c(5, 5, 0, 0))
contour(beta1, beta2, pen, levels = quantile(pen, quanlvl), las = 1, lwd = 3, 
        xlab = "Beta 1", ylab = "Beta 2", cex.lab = 3, cex.axis = 3)
points(1, 1, pch = 19, col = "red", cex = 3)
box()
```
:::

::::






## Geometrical Meaning of Ridge Regression

```{r}
#| out-width: 52%
#| fig-cap: "Source: https://online.stat.psu.edu/stat508/lessons/Lesson06"
knitr::include_graphics("./images/05-ridge-cv/ridge_pc.png")
```



::: notes
- Perform PCA of X
- Project y onto the PCs
- Shrinks the projection
- Reassemble the PCs using all the shrunken length
:::


## More Convex Loss Function

- Adding a ridge penalty forces the objective to be more convex due to the added eigenvalues.

```{r}
#| echo: true
eigen(matrix(c(1, 0.99, 0.99, 1), 2, 2) + diag(2))[1]
```


```{r fig.dim = c(18, 6)}
#| out-width: "100%"

par(mfrow = c(1, 3))
set.seed(2024)
n <- 50
# create highly correlated variables and a linear model
X <- mvrnorm(n, c(0, 0), matrix(c(1, 0.99, 0.99, 1), 2, 2))
y <- rnorm(n, mean = X[, 1] + X[, 2])
beta1 <- seq(-1, 3, 0.005)
beta2 <- seq(-1, 3, 0.005)
allbeta <- data.matrix(expand.grid(beta1, beta2))
rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2), X, y),
              length(beta1), length(beta2))
  
# quantile levels for drawing contour
quanlvl = c(0.01, 0.025, 0.05, 0.2, 0.5, 0.75)
par(mar = c(2, 2, 2, 0))
# plot the contour
contour(beta1, beta2, rss, levels = quantile(rss, quanlvl), las = 1,lwd = 3, 
        xlab = "beta 1", ylab = "beta 2", main = "Lambda = 0 OLS", cex.main = 3)
box()
  
# the truth
points(1, 1, pch = 19, col = "red", cex = 3)
  
# the data 
betahat <- coef(lm(y~X-1))
points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 3)

    # adding a L2 penalty to the objective function
    rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2) + b %*% b, X, y),
                  length(beta1), length(beta2))
    
    # the ridge solution
    bh = solve(t(X) %*% X + diag(2)) %*% t(X) %*% y
    
    contour(beta1, beta2, rss, levels = quantile(rss, quanlvl), las = 1,lwd = 3, 
            main = "Lambda = 1", cex.main = 3)
    points(1, 1, pch = 19, col = "red", cex = 3)
    points(bh[1], bh[2], pch = 19, col = "blue", cex = 3)
    box()
    
    # adding a larger penalty
    rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2) + 10*b %*% b, X, y),
                  length(beta1), length(beta2))
    
    bh = solve(t(X) %*% X + 10*diag(2)) %*% t(X) %*% y
    
    # the ridge solution
    contour(beta1, beta2, rss, levels = quantile(rss, quanlvl), las = 1,lwd = 3, 
            main = "Lambda = 10", cex.main = 3)
    points(1, 1, pch = 19, col = "red", cex = 3)
    points(bh[1], bh[2], pch = 19, col = "blue", cex = 3)
    box()
```



::: notes
Hence, by adding this to the OLS objective function, the solution is more stable, in the sense that each time we observe a new set of data, this contour looks pretty much the same. This may be interpreted in several different ways such as: 1) the objective function is more convex and less affected by the random samples; 2) the variance of the estimator is smaller because the eigenvalues of $\bX^\T \bX + n \lambda \bI$ are large. 
:::


## Bias Caused by the Ridge Penalty # {visibility="hidden"}

Now, let's apply the same analysis on the ridge regression estimator. For the theoretical justification of this analysis, please read the [SMLR textbook](https://teazrq.github.io/SMLR/ridge-regression.html#bias-and-variance-of-ridge-regression). 
We will set up a simulation study with the following steps, with both $\widehat{\bbeta}^\text{ridge}$ and $\widehat{\bbeta}^\text{ols}$:

  1) Generate a set of $n = 100$ observations
  2) Estimate the ridge estimator $\widehat{\bbeta}^\text{ridge}$ with $\lambda = 0.3$. Hence, $n \lambda = 30$.
  3) Repeat steps 1) and 2) $\text{nsim} = 200$ runs
  4) Average all estimations and compare that with the truth $\bbeta$
  5) Compute the variation of these estimates across all runs

```{r}
#| eval: false
  set.seed(1)
  # number of researchers
  nsim = 200
  # number of observations
  n = 100
  
  # lambda
  lambda = 0.3
  
  # save all estimated variance in a vector 
  allridgebeta = matrix(NA, nsim, 2)
  allolsbeta = matrix(NA, nsim, 2)
  
  for (i in 1:nsim)
  {
    # create highly correlated variables and a linear model
    X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2))
    y = rnorm(n, mean = X[,1] + X[,2])
    
    allridgebeta[i, ] = solve(t(X) %*% X + lambda*n*diag(2)) %*% t(X) %*% y
    allolsbeta[i, ] = solve(t(X) %*% X) %*% t(X) %*% y
  }

  # compare the mean of these estimates
  colMeans(allridgebeta)
  colMeans(allolsbeta)
  
  # compare the var of these estimates
  apply(allridgebeta, 2, var)
  apply(allolsbeta, 2, var)
```

::: notes

:::



## The Bias-variance Tradeoff
  * As $\lambda \downarrow$, bias $\downarrow$ and variance $\uparrow$
  * As $\lambda \uparrow$, bias $\uparrow$ and variance $\downarrow$


```{r}
#| out-width: "100%"
#| fig-asp: 0.3
    par(mfrow = c(1, 3))
par(mar = c(2, 2, 2, 0))
  # the truth
  plot(1, 1, xlim = c(-1, 3), ylim = c(-1, 3), xlab = "", ylab = "", las = 1,
       pch = 19, col = "red", cex = 2, main = "OLS lambda = 0",
       cex.main = 2)
  
  # generate many datasets in a simulation 
  for (i in 1:200)
  {
    X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2))
    y = rnorm(n, mean = X[,1] + X[,2])
    
    betahat <- coef(lm(y~X-1))
    points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 0.5)
    points(1, 1, pch = 19, col = "red", cex = 2)
  }
  
  # the truth
  plot(1, 1, xlim = c(-1, 3), ylim = c(-1, 3), xlab = "", ylab = "", las = 1,
       pch = 19, col = "red", cex = 2, main = "n*lambda = 2",
       cex.main = 2)
  
  # generate many datasets in a simulation 
  for (i in 1:200)
  {
    X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2))
    y = rnorm(n, mean = X[,1] + X[,2])
    
    betahat <- lm.ridge(y ~ X - 1, lambda = 2)$coef
    points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 0.5)
  }
  
  points(1, 1, pch = 19, col = "red", cex = 2)
  
  # a plot
  plot(1, 1, xlim = c(-1, 3), ylim = c(-1, 3), xlab = "", ylab = "", las = 1,
       pch = 19, col = "red", cex = 2, main = "n*lambda = 30",
       cex.main = 2)
  
  # generate many datasets in a simulation 
  for (i in 1:200)
  {
    X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2))
    y = rnorm(n, mean = X[,1] + X[,2])
    
    # betahat <- solve(t(X) %*% X + 30*diag(2)) %*% t(X) %*% y
    betahat <- lm.ridge(y ~ X - 1, lambda = 30)$coef
    points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 0.5)
  }
  
  points(1, 1, pch = 19, col = "red", cex = 2)
```





::: notes
This effect is gradually changing as we increase the penalty level. The following simulation shows how the variation of $\bbeta$ changes. We show this with two penalty values, and see how the estimated parameters are away from the truth. 

Now, we may ask the question: is it worth it? In fact, this bias and variance will be then carried to the predicted values $x^\T \widehat{\bbeta}^\text{ridge}$. Hence, we can judge if this is beneficial from the prediction accuracy. And we need some procedure to do this. 

__Remark__: The bias-variance trade-off will appear frequently in this course. And the way to evaluate the benefit of this is to see if it eventually reduces the prediction error ($\text{Bias}^2 + \text{Variance}$ plus a term called __irreducible error__, which will be introduced in later chapter). 
:::

## Lower Test MSE



:::: {.columns}

::: {.column width="38%"}

- When $b_j$ has large variance or $p > n$, ridge regression could have lower test MSE and better predictive performance.

- [$\text{MSE}_{\texttt{Tr}}$ (purple)]{.purple}

- **Squared bias (black)**

- [**Variance (green)**]{.green}



:::

::: {.column width="62%"}
::: small

```{r}
#| out-width: "100%"
#| fig-cap: "Source: ISL Figure 6.5"
#| fig-align: left
knitr::include_graphics("./images/05-ridge-cv/6_5.png")
```

:::
:::
::::


## `MASS::lm.ridge()`

- The `lambda` parameter in `lm.ridge()` specifies the $n\lambda$ in our notation.

- OLS is *scale equivalent*: $X_jb_j$ remains the same regardless of how $X_j$ is scaled.

- Ridge coefficient estimates can *change substantially* when multiplying a given predictor by a constant, i.e., $X_j\hat{\beta}^{r}_{j, \lambda}$ depends on $\lambda$, the scaling of $X_j$, and even the scaling of *other* predictors.

- [__Standardize all predictors!__]{.green}^[In practice, if the intercept is not our interest, we also standardize the response.]

. . .


```{r}
#| echo: true
head(mtcars, 3)
(fit <- MASS::lm.ridge(mpg ~ ., data = mtcars, lambda = 1)) ## ridge fit
```




::: notes

:::

## Scaling Issues of `lm.ridge()`

- `coef(fit)` transforms these back to the *original* scale. 

- `fit$coef` shows the coefficients of the standardized predictors.


```{r}
#| echo: true
coef(fit)
fit$coef
```


## Scaling Issues of `lm.ridge()`

```{r}
#| echo: true
fit$coef
```

- `lm.ridge()` uses $n$ instead of $n-1$ when calculating the standard deviation.

```{r}
#| echo: true
# each column has mean 0 and var 1
X <- scale(data.matrix(mtcars[, -1]), center = TRUE, scale = TRUE)

# center y but not scaling
y <- scale(mtcars$mpg, center = TRUE, scale = FALSE)
```


<br>

```{r}
#| echo: true

# lambda = 1
my_ridge_coef <- solve(t(X) %*% X + diag(ncol(X))) %*% t(X) %*% y
t(my_ridge_coef)

```

## Scaling Issues of `lm.ridge()`


```{r}
#| echo: true
fit$coef
```

<br>

```{r}
#| echo: true
# use n instead of (n-1) for standardization
n <- nrow(X); X <- X * sqrt(n / (n - 1))
```


```{r}
my_ridge_coef <- solve(t(X) %*% X + diag(ncol(X))) %*% t(X) %*% y
t(my_ridge_coef)
```

::: notes
More discussion at https://stats.stackexchange.com/questions/288754/lm-ridge-returns-different-results-that-are-from-manual-calculation
:::


## Ridge Trace

```{r, echo=-1}
#| out-width: "100%"
par(mar = c(4, 4, 0, 0))
ridge_fit <- lm.ridge(mpg ~ ., data = mtcars, lambda = 0:40)
matplot(coef(ridge_fit)[, -1], type = "l", xlab = "Lambda", ylab = "Coefficients")
text(rep(1, 10), coef(ridge_fit)[1,-1], colnames(mtcars)[2:11])
```

- Select a value of $\lambda$ at which the ridge estimates are stable.



::: notes

:::




## Methods for Choosing $\lambda$
```{r}
#| echo: true
MASS::select(ridge_fit)
```

- Hoerl, Kennard, and Baldwin (1975): $\lambda \approx \frac{p \hat{\sigma}^2}{{\bf b}'{\bf b}}$

- Lawless and Wang (1976): $\lambda \approx \frac{np \hat{\sigma}^2}{{\bf b'X}'{\bf Xb}}$

- [Golub, Health, and Wahba (1979)](https://pages.stat.wisc.edu/~wahba/ftp1/oldie/golub.heath.wahba.pdf): Generalized Cross Validation


## Cross Validation

- Cross Validation (CV) is a **resampling** method. 

- Resampling methods refit a model of interest to data sampled from the training set.

- CV can be used to 
  + estimate the test error when there is no test data. _(Model assessment)_
  + select the tuning parameters that controls the model complexity/flexibility. _(Model selection)_




::: notes
Hence, by adding this to the OLS objective function, the solution is more stable, in the sense that each time we observe a new set of data, this contour looks pretty much the same. This may be interpreted in several different ways such as: 1) the objective function is more convex and less affected by the random samples; 2) the variance of the estimator is smaller because the eigenvalues of $\bX^\T \bX + n \lambda \bI$ are large. 
:::


## 

::: {.algo} 

**$k$-Fold Cross Validation**

1. Randomly divide the training set into $k$ *folds*, of approximately equal size.

2. Use 1 fold for validation to compute MSE, and the remaining $k - 1$ partitions for training.

3. Repeat $k$ times. Each time a different fold is treated as a validation set.

4. Average $k$ metrics, $\text{MSE}_{CV} = \frac{1}{k}\sum_{i=1}^k\text{MSE}_i$.

5. Use the CV estimate $\text{MSE}_{CV}$ to select the "best" tuning parameter.
:::



::: small

```{r}
#| out-width: 45%
#| fig-cap: "Five-fold cross validation. Source: Data science in a box"
knitr::include_graphics("./images/05-ridge-cv/cross-validation.png")
```

:::

::: notes
Can compute other performance measures.
:::

## $k$-Fold Cross Validation in `R` {visibility="hidden"}

- [`caret`](https://topepo.github.io/caret/index.html) package: **C**lassification **A**nd **RE**gression **T**raining ^[Be careful that not all models are available in `caret`. `caret` do not use `MASS::lm.ridge`, but the `elasticnet` package to fit ridge regression.]


```{r}
#| eval: false
library(caret)
library(elasticnet)

# set cross-validation type
ctrl <- caret::trainControl(method = "cv", number = 10)
    
# set tuning parameter range
lambdaGrid <- data.frame("lambda" = seq(0, 0.4, length = 20))
    
# perform cross-validation
ridge_cvfit <- caret::train(x = mtcars[, -1], mtcars$mpg,
                            method = ,
                            tuneGrid = lambdaGrid,
                            trControl = ctrl,
                            ## center and scale predictors
                            preProc = c("center", "scale"))
ridge_cvfit
```



::: notes
Cross-validation can be setup using the `caret` package. However, you should be careful that not all models are available in `caret`, and you should always check the documentation to see how to implement them. For example, if you use `method = "ridge"`, they do not use `lm.ridge` to fit the model, instead, they use a package called `elasticnet`, which can do the same job. However, the definition of parameters may vary. Hence, it is always good to check the main [help pages](https://topepo.github.io/caret/) for the package. We will use the caret package later for other models. 
https://rsample.tidymodels.org/reference/vfold_cv.html
:::

## [`glmnet`](https://glmnet.stanford.edu/index.html) package  ![](images/05-ridge-cv/glmnet_logo.png){.absolute left="450" top="0" width="70"}

- The parameter `alpha` controls the ridge (`alpha = 0`) and lasso (`alpha = 1`) penalties.

- Supply a *decreasing* sequence of `lambda` values.

- `lm.ridge()` use $SS_{res}$ and $n\lambda$, while `glmnet()` use $\text{MSE}_{\texttt{Tr}}$ and $\lambda$.

- Argument `x` should be a matrix.


:::: columns
::: column
```{r}
#| echo: true
#| eval: false
glmnet(x = data.matrix(mtcars[, -1]), 
       y = mtcars$mpg, 
       alpha = 0, 
       lambda = 5:0/nrow(mtcars))
```
:::


::: column
```{r}
#| echo: true
#| eval: false
lm.ridge(formula = mpg ~ ., 
         data = mtcars, 
         lambda = 5:0/nrow(mtcars))
```
:::


:::{.callout-note style="font-size: 1.2em"}
- `lm.ridge()` and `glmnet()` coefficients do not match exactly, specially when transforming back to original scale.
- No need to worry too much as we focus on predictive performance.
:::

::::



::: notes
https://stats.stackexchange.com/questions/74206/ridge-regression-results-different-in-using-lm-ridge-and-glmnet
:::



## 

```{r}
#| fig-asp: 0.4
X <- scale(data.matrix(mtcars[, -1]), center = TRUE, scale = TRUE)
y <- scale(mtcars$mpg, center = TRUE, scale = TRUE)
lam = 10^seq(-1, 3, 0.1)

fit2 <- glmnet(X, y, alpha = 0, lambda = rev(lam) / nrow(X))
# X <- X * sqrt(n / (n - 1))
fit1 <- lm.ridge(y ~ X, lambda = lam)
par(mar = c(2, 2, 2, 1))
par(mfrow = c(1, 2))
matplot(coef(fit1), type = "l", main = "Standardized lm.ridge", ylab = "")
matplot(apply(t(as.matrix(coef(fit2))), 2, rev), type = "l", 
        main = "Standardized glmnet",
        ylab = "")
```


##

```{r}
#| fig-asp: 0.4
ridge_lm <- MASS::lm.ridge(mpg ~ ., data = mtcars, lambda = 0:500)
ridge_net <- glmnet(data.matrix(mtcars[, -1]), mtcars$mpg, alpha = 0, 
       lambda = 500:0 / nrow(mtcars))
par(mar = c(2, 2, 2, 1))
par(mfrow = c(1, 2))
matplot(coef(ridge_lm), type = "l", main = "Original-scale lm.ridge",
        ylab = "")
matplot(apply(t(as.matrix(coef(ridge_net))), 2, rev), type = "l", 
        main = "Original-scale glmnet", ylab = "")

```



## $k$-Fold Cross Validation using `cv.glmnet()`^[There are other ways to do CV for ridge regression in `R`, for example, the [`caret`](https://topepo.github.io/caret/) (**C**lassification **A**nd **RE**gression **T**raining) package and the [`rsample`](https://rsample.tidymodels.org/reference/vfold_cv.html) package in [`tidymodels`](https://www.tidymodels.org/) ecosystem.]

- The $\lambda$ values are automatically selected, on the *$\log_{e}$* scale.

```{r}
#| echo: true
#| out-width: "50%"
ridge_cv_fit <- cv.glmnet(x = data.matrix(mtcars[, -1]), y = mtcars$mpg, alpha = 0,
                          nfolds = 10, type.measure = "mse")
plot(ridge_cv_fit$glmnet.fit, "lambda")
```



::: notes
- Why s and not lambda? In case we want to allow one to specify the model size in other ways in the future.
s: Value(s) of the penalty parameter lambda at which predictions are required. Default is the entire sequence used to create the model.

:::



## Determine $\lambda$

:::: columns
::: column


```{r}
#| echo: true
#| out-width: "100%"
plot(ridge_cv_fit)
```

:::


::: column
```{r}
#| echo: true

ridge_cv_fit$lambda.min
# largest lambda s.t. error is within 1 s.e of the min
ridge_cv_fit$lambda.1se 
coef(ridge_cv_fit, s = "lambda.min")
```
:::
::::


::: notes
coef(fit2, s = "lambda.1se")
This plots the cross-validation curve (red dotted line) along with upper and lower standard deviation curves along the ðœ†
 sequence (error bars). Two special values along the ðœ†
 sequence are indicated by the vertical dotted lines. lambda.min is the value of ðœ†
 that gives minimum mean cross-validated error, while lambda.1se is the value of ðœ†
 that gives the most regularized model such that the cross-validated error is within one standard error of the minimum.
:::



## Generalized Cross-Validation
- The generalized cross-validation (GCV) is a modified version of the **leave-one-out CV** ($n$-fold CV).

- The GCV criterion is given by $$\text{GCV}(\lambda) = \frac{1}{n}\sum_{i=1}^n \left[ \frac{y_i - x_i' \widehat{\bbeta}^\text{r}_\lambda}{1 - \frac{\Trace(\bS_\lambda)}{n}} \right]^2$$

where $\bS_\lambda$ is the _hat matrix_ corresponding to the ridge regression:

$$\bS_\lambda = \bX (\bX' \bX + n\lambda \bI)^{-1} \bX'$$




::: notes
The interesting fact about leave-one-out CV in the linear regression setting is that we do not need to explicitly fit all leave-one-out models.

- ESL p. 244

- lm.ridge code
:::


## Generalized Cross-Validation

:::: columns 
::: {.column width="49%"}

```{r, echo=-1}
#| out-width: "100%"
par(mar = c(4, 4, 0, 0))
plot(ridge_fit$lambda, 
     ridge_fit$GCV, 
     type = "l", col = "darkgreen", 
     ylab = "GCV", xlab = "Lambda", 
     lwd = 3)
```
:::

::: {.column width="49%"}

```{r}
#| echo: true
idx <- which.min(ridge_fit$GCV)
ridge_fit$lambda[idx]
round(coef(ridge_fit)[idx, ], 2)
```
:::
::::


Select the best $\lambda$ that produces the smallest GCV.


::: notes
You can clearly see that the GCV decreases initially, as $\lambda$ increases, this is because the reduced variance is more beneficial than the increased bias. However, as $\lambda$ increases further, the bias term will eventually dominate and causing the overall prediction error to increase. The fitted MSE under this model is 

```{r}
#| eval: false
  pred1 = data.matrix(cbind(1, mtcars[, -1])) %*% coef(fit1)[which.min(fit1$GCV), ]
  mean((pred1 - mtcars$mpg)^2)
```
:::




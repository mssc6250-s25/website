---
title: 'Ridge Regression and Cross Validation `r fontawesome::fa("wave-square")`'
subtitle: "MSSC 6250 Statistical Machine Learning"
author: "Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University"
# date: "`r format(Sys.time(), '%B %d %Y')`"
# macros: _macros.tex # import a list of TeX/LaTeX definitions
format: 
  revealjs:
    html-math-method:
      method: mathjax
      url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    # include-in-header:
    #     - "macros.tex"
    highlight-style: arrow
    code-block-bg: true
    code-block-border-left: "#31BAE9"
    self-contained: false
    slide-number: c/t
    incremental: false
    width: 1800
    height: 1000
    margin: 0.05
    logo: "https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg"
    footer: "[mssc6250-s25.github.io/website](https://mssc6250-s25.github.io/website/)"
    theme: ["simple", "styles.scss"]
    echo: false
    multiplex: true
    code-link: true
    fig-cap-location: bottom
    fig-align: center
    transition: none ## fade slide convex concave zoom
    title-slide-attributes:
      data-background-color: "#447099"
      # data-background-image: images/paper-texture.jpg
      # data-background-size: cover
      # data-background-color: "#698ED5"
editor: source
execute:
  freeze: true
  echo: false
---


# {visibility="hidden"}


\def\bx{\mathbf{x}}
\def\bg{\mathbf{g}}
\def\bw{\mathbf{w}}
\def\bbeta{\boldsymbol \beta}
\def\bX{\mathbf{X}}
\def\by{\mathbf{y}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\T{\text{T}}
\def\cov{\mathrm{Cov}}
\def\cor{\mathrm{Corr}}
\def\var{\mathrm{Var}}
\def\E{\mathrm{E}}
\def\bmu{\boldsymbol \mu}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\Trace{\text{Trace}}





```{r}
#| label: setup
#| include: false
#| eval: true
library(countdown)
library(knitr)
library(gt)
library(gtExtras)
library(ggplot2)
library(ISLR2)
library(genridge)
library(glmnet)
# library(ElemStatLearn)
knitr::opts_chunk$set(
    fig.asp = 0.618,
    fig.align = "center",
    out.width = "100%",
    fig.retina = 10,
    fig.path = "images/05-ridge-cv/",
    message = FALSE,
    global.par = TRUE
)
options(
  htmltools.dir.version = FALSE,
  dplyr.print_min = 6, 
  dplyr.print_max = 6,
  tibble.width = 80,
  width = 80,
  digits = 3
  )
hook_output <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
```





# Ridge Regression {background-color="#A7D5E8"}



## When Ordinary Least Squares (OLS) Doesn't Work Well: Collinearity
- When predictors are highly correlated, $\var(b_j)$ is much inflated.^[Although $\var(b_j)$ is still the smallest among all linear unbiased estimators.]
  + A tiny change in the training set causes a large change in $b_j$, leading to *unreliable* estimation and possibly prediction.

. . .

- ${\bf X'X} = \begin{bmatrix} 1 & 0.99 \\ 0.99 & 1 \end{bmatrix}$ $\quad ({\bf X'X})^{-1} = \begin{bmatrix}  50.3 & -49.7 \\ -49.7 & 50.3 \end{bmatrix}$

- $\var(b_j) = 50.3\sigma^2$
  + An increase in 50-fold over the ideal case when the two regressors are orthogonal.


```{r}
#| echo: true
solve(matrix(c(1, 0.99, 0.99, 1), 2, 2))
```


::: notes
the eigen vector direction with the smallest eigen value
https://online.stat.psu.edu/stat857/node/155/
https://robjhyndman.com/hyndsight/loocv-linear-models/
:::

## When OLS Doesn't Work Well: Optimization Perspective

- $\beta_1 = \beta_2 = 1$ and $\cor(x_1, x_2) = 0.99$

- The relatively "flat" valley in the objective function walks along the eigen-vector of ${\bf X}'{\bf X}$ having the smallest eigen-value.

:::: columns
::: {.column width="60%"}
```{r}
#| out-width: "100%"
library(MASS)
set.seed(2023)
n <- 50
# create highly correlated variables and a linear model
X <- mvrnorm(n, c(0, 0), matrix(c(1, 0.99, 0.99, 1), 2, 2))
y <- rnorm(n, mean = X[, 1] + X[, 2])
beta1 <- seq(-1, 3, 0.005)
beta2 <- seq(-1, 3, 0.005)
allbeta <- data.matrix(expand.grid(beta1, beta2))
rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2), X, y),
              length(beta1), length(beta2))
  
# quantile levels for drawing contour
quanlvl = c(0.01, 0.025, 0.05, 0.2, 0.5, 0.75)
par(mar = c(3, 3, 0, 0))
# plot the contour
contour(beta1, beta2, rss, levels = quantile(rss, quanlvl), las = 1,lwd = 3, 
        xlab = "beta 1", ylab = "beta 2")
box()
  
# the truth
points(1, 1, pch = 19, col = "red", cex = 2)
  
# the data 
betahat <- coef(lm(y~X-1))
points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 2)
legend("topright", c("true beta value", "OLS estimates"), 
       col = c("red", "blue"), pch = 19, bty = "n")
```
:::

::: {.column width="40%"}
```{r}
#| echo: true
eigen(
    matrix(c(1, 0.99, 0.99, 1), 
             2, 2)
    )
```
:::
::::


::: notes

- From optimization point of view, the objective function ($\ell_2$ loss) is "flat" along certain directions in the parameter domain.
- a small eigen-value in XTX makes the corresponding eigen-value large in the inverse.
:::


## When OLS Doesn't Work Well: Optimization Perspective

- A little change in the training set perturbs the objective function. The LSEs lie on a valley centered around the truth.

```{r}
#| out-width: "100%"
#| cache: true
par(mfrow = c(2, 2))
par(mar = c(2, 2, 0, 0))
set.seed(2024)
  for (i in 1:4)
  {
    # create highly correlated variables and a linear model
    X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2))
    y = rnorm(n, mean = X[,1] + X[,2])
  
    beta1 <- seq(-1, 3, 0.005)
    beta2 <- seq(-1, 3, 0.005)
    allbeta <- data.matrix(expand.grid(beta1, beta2))
    rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2), X, y), 
                    length(beta1), length(beta2))
      
    # quantile levels for drawing contour
    quanlvl = c(0.01, 0.025, 0.05, 0.2, 0.5, 0.75)
      
    # plot the contour
    contour(beta1, beta2, rss, levels = quantile(rss, quanlvl), las = 1)
    box()
      
    # the truth
    points(1, 1, pch = 19, col = "red", cex = 2)
      
    # the data 
    betahat <- coef(lm(y~X-1))
    points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 2)
  }
```


## When OLS Doesn't Work Well: High Variance

- The optimizer could land anywhere along the valley, leading to *large variance* of the LSE.

- Over many simulation runs, the LSE lies around the line of $\beta_1 + \beta_2 = 2$, the direction of the eigen-vector of the smallest eigen-value.


:::: columns
::: column
```{r}
par(mar = c(2, 2, 2, 0))
  # the truth
  plot(1, 1, xlim = c(-1, 3), ylim = c(-1, 3), xlab = "", ylab = "", las = 1,
       pch = 19, col = "red", cex = 2, main = "200 LSEs w/ highly correlated predictors",
       cex.main = 2)
  
  # generate many datasets in a simulation 
  for (i in 1:200)
  {
    X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2))
    y = rnorm(n, mean = X[,1] + X[,2])
    
    betahat <- coef(lm(y~X-1))
    points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 0.5)
    points(1, 1, pch = 19, col = "red", cex = 2)
  }
  
```
:::

::: column
```{r}
par(mar = c(2, 2, 2, 0))
  # the truth
  plot(1, 1, xlim = c(-1, 3), ylim = c(-1, 3), 
       pch = 19, col = "red", cex = 2, xlab = "", ylab = "", las = 1,
       main = "200 LSEs w/ orthogonal predictors", cex.main = 2)
  
  # generate many datasets in a simulation 
  for (i in 1:200)
  {
    X = mvrnorm(n, c(0, 0), matrix(c(1,0, 0, 1), 2,2))
    y = rnorm(n, mean = X[,1] + X[,2])
    
    betahat <- coef(lm(y~X-1))
    points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 0.5)
    points(1, 1, pch = 19, col = "red", cex = 2)
  }
```

:::

::::




## When OLS Doesn't Work Well: Large-$p$-small-$n$ (High Dimensional Data)
- OLS stays well in the world of "large-$n$-small-$p$".

- When $p > n$, ${\bf X}'{\bf X}$ is not invertible.

- There is no unique $\bbeta$ estimate.

. . .


[**Intuition**: *Too many degrees of freedom ($p$) to specify a model, but not enough information ($n$) to decide which one is the one.*]{style="font-size: 1.2em"}

- Too flexible and ends up with overfitting



## Remedy for Large Variance and Large-$p$-small-$n$

- Make ${\bf X}'{\bf X}$ invertible when $p > n$ by [regularizing]{.blue} coefficient behavior! 

- A good estimator *balances bias and variance well*, or *minimizes the mean square error* $$\text{MSE}(\hat{\beta}) = E[(\hat{\beta} - \beta)^2] = \var(\hat{\beta}) + \text{Bias}(\hat{\beta})^2$$

- A slightly **biased** estimator $\hat{\bbeta}$ that has *much smaller variance and MSE* than the LSE ${\bf b}$.

:::: columns
::: column
```{r}
#| out-width: "90%"
#| fig-asp: 0.6
par(mar = c(2, 0, 2, 0), mgp = c(0.5, 0.5, 0), las = 1)
x <- seq(-6, 6, by = 0.1)
plot(x, dnorm(x), type = "l", lwd = 3, axes = F, xlab = "", ylab = "",
     xlim = c(-8, 8))
title(main = list(expression(paste("Sampling distribution a biased estimator ", hat(beta))), cex = 2))
axis(1, at = c(-8, -1, 0, 8), 
     labels = c("", expression(bold(beta)), expression(E(hat(beta))), ""), 
     tick = T, tck = -0.01, cex.axis=1.5)
segments(x0 = 0, y0 = 0, x1 = 0, y1 = dnorm(0), lty = 2)
segments(x0 = -1, y0 = 0, x1 = -1, y1 = dnorm(-1), lty = 2)
text(3.5, dnorm(1), expression(paste(E(hat(beta)) != bold(beta), " (biased)")),
     cex = 1.5)
text(4, dnorm(1.5), expression(paste(Var(hat(beta)), " small")),
     cex = 1.5)
```
:::

::: column
```{r}
#| out-width: "100%"
#| fig-asp: 0.3
par(mar = c(2, 0, 2, 0), mgp = c(0.5, 0.5, 0), las = 1)
x <- seq(-3, 3, by = 0.1)
plot(x, dnorm(x), type = "l", lwd = 3, axes = F, xlab = "", ylab = "")
title(main = list("Sampling distribution of LSE b", cex = 2))
axis(1, at = c(-3, 0.1, 3), labels = c("", expression(bold(beta) == E(b)), ""), 
     tick = T, tck = -0.01, cex.axis=1.5)
segments(x0 = 0, y0 = 0, x1 = 0, y1 = dnorm(0), lty = 2)
text(2, dnorm(1), expression(paste("E(b) = ", bold(beta), " (unbiased)")),
     cex = 1.5)
text(2.5, dnorm(1.5), paste("Var(b) large"), cex = 1.5)
```
:::
::::

## Ridge Regression

- Idea: Add a *ridge* (diagonal matrix) to ${\bf X} ' {\bf X}$.^[This is a special case of the [Tikhonov regularization](https://en.wikipedia.org/wiki/Ridge_regression#Tikhonov_regularization).]
$$\widehat{\bbeta}^\text{r} = (\bX' \bX + n \lambda \bI)^{-1} \bX' \by,$$

. . .

- To regularize coefficients, add an $\ell_2$ **penalty** to the residual sum of squares, for some *tuning parameter* $\lambda > 0$. 



$$
\begin{align}
\widehat{\bbeta}^\text{r} =& \, \argmin_{\bbeta} \underbrace{\lVert \by - \bX \bbeta\rVert_2^2}_{SS_{res}} + n \lambda \lVert\bbeta\rVert_2^2\\
=& \, \argmin_{\bbeta} \underbrace{\frac{1}{n} \sum_{i=1}^n (y_i - x_i'\bbeta)^2}_{\text{MSE}_{\texttt{Tr}}} + \lambda \sum_{j=1}^p \beta_j^2\\
=& \, \argmin_{\bbeta} \color{green} - \text{ goodness of fit } + \text{ model complexity/flexibility}
\end{align}
$$


## Properties of Ridge Regression

$$
\begin{align}
\widehat{\bbeta}^\text{r} =& \argmin_{\bbeta} \frac{1}{n} \sum_{i=1}^n (y_i - x_i'\bbeta)^2 + \lambda \sum_{j=1}^p \beta_j^2,
\end{align}
$$

Properties of ridge regression:

- Has *less* degrees of freedom in the sense that [_the cost gets higher when larger coefficients are used_]{.green}.

. . .

- Favors small-magnitude coefficient estimates (**Shrinkage**) to avoid cost penalty. 

. . .

- The shrinkage parameter $\lambda$ controls the degree of penalty.
  + $\lambda \rightarrow 0$: No penalty, and $\widehat{\bbeta}^\text{r} = \bf b$.
  + $\lambda \rightarrow \infty$: Unbearable penalty, and $\widehat{\bbeta}^\text{r} \rightarrow \mathbf{0}$
  
  
  
  
## Ridge Penalty

:::: {.columns}

::: {.column width="50%"}


$$\lambda \lVert \bbeta \rVert^2_2 = \lambda \bbeta' \bI \bbeta$$

- The penalty contour is *circle-shaped*

- Force the objective function to be more *convex*

- A more stable or less varying solution
:::

::: {.column width="50%"}
```{r}
#| fig-asp: 1
pen <- matrix(apply(allbeta, 1, function(b) 3 * b %*% b),
              length(beta1), length(beta2))
par(mar = c(5, 5, 0, 0))
contour(beta1, beta2, pen, levels = quantile(pen, quanlvl), las = 1, lwd = 3, 
        xlab = "Beta 1", ylab = "Beta 2", cex.lab = 3, cex.axis = 3)
points(1, 1, pch = 19, col = "red", cex = 3)
box()
```
:::

::::






## Geometrical Meaning of Ridge Regression

```{r}
#| out-width: 52%
#| fig-cap: "Source: https://online.stat.psu.edu/stat508/lessons/Lesson06"
knitr::include_graphics("./images/05-ridge-cv/ridge_pc.png")
```



::: notes
- Perform PCA of X
- Project y onto the PCs
- Shrinks the projection
- Reassemble the PCs using all the shrunken length
:::


## More Convex Loss Function

- Adding a ridge penalty forces the objective to be more convex due to the added eigenvalues.

```{r}
#| echo: true
eigen(matrix(c(1, 0.99, 0.99, 1), 2, 2) + diag(2))[1]
```


```{r fig.dim = c(18, 6)}
#| out-width: "100%"

par(mfrow = c(1, 3))
set.seed(2024)
n <- 50
# create highly correlated variables and a linear model
X <- mvrnorm(n, c(0, 0), matrix(c(1, 0.99, 0.99, 1), 2, 2))
y <- rnorm(n, mean = X[, 1] + X[, 2])
beta1 <- seq(-1, 3, 0.005)
beta2 <- seq(-1, 3, 0.005)
allbeta <- data.matrix(expand.grid(beta1, beta2))
rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2), X, y),
              length(beta1), length(beta2))
  
# quantile levels for drawing contour
quanlvl = c(0.01, 0.025, 0.05, 0.2, 0.5, 0.75)
par(mar = c(2, 2, 2, 0))
# plot the contour
contour(beta1, beta2, rss, levels = quantile(rss, quanlvl), las = 1,lwd = 3, 
        xlab = "beta 1", ylab = "beta 2", main = "Lambda = 0 OLS", cex.main = 3)
box()
  
# the truth
points(1, 1, pch = 19, col = "red", cex = 3)
  
# the data 
betahat <- coef(lm(y~X-1))
points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 3)

    # adding a L2 penalty to the objective function
    rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2) + b %*% b, X, y),
                  length(beta1), length(beta2))
    
    # the ridge solution
    bh = solve(t(X) %*% X + diag(2)) %*% t(X) %*% y
    
    contour(beta1, beta2, rss, levels = quantile(rss, quanlvl), las = 1,lwd = 3, 
            main = "Lambda = 1", cex.main = 3)
    points(1, 1, pch = 19, col = "red", cex = 3)
    points(bh[1], bh[2], pch = 19, col = "blue", cex = 3)
    box()
    
    # adding a larger penalty
    rss <- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2) + 10*b %*% b, X, y),
                  length(beta1), length(beta2))
    
    bh = solve(t(X) %*% X + 10*diag(2)) %*% t(X) %*% y
    
    # the ridge solution
    contour(beta1, beta2, rss, levels = quantile(rss, quanlvl), las = 1,lwd = 3, 
            main = "Lambda = 10", cex.main = 3)
    points(1, 1, pch = 19, col = "red", cex = 3)
    points(bh[1], bh[2], pch = 19, col = "blue", cex = 3)
    box()
```



::: notes
Hence, by adding this to the OLS objective function, the solution is more stable, in the sense that each time we observe a new set of data, this contour looks pretty much the same. This may be interpreted in several different ways such as: 1) the objective function is more convex and less affected by the random samples; 2) the variance of the estimator is smaller because the eigenvalues of $\bX^\T \bX + n \lambda \bI$ are large. 
:::


## Bias Caused by the Ridge Penalty # {visibility="hidden"}

Now, let's apply the same analysis on the ridge regression estimator. For the theoretical justification of this analysis, please read the [SMLR textbook](https://teazrq.github.io/SMLR/ridge-regression.html#bias-and-variance-of-ridge-regression). 
We will set up a simulation study with the following steps, with both $\widehat{\bbeta}^\text{ridge}$ and $\widehat{\bbeta}^\text{ols}$:

  1) Generate a set of $n = 100$ observations
  2) Estimate the ridge estimator $\widehat{\bbeta}^\text{ridge}$ with $\lambda = 0.3$. Hence, $n \lambda = 30$.
  3) Repeat steps 1) and 2) $\text{nsim} = 200$ runs
  4) Average all estimations and compare that with the truth $\bbeta$
  5) Compute the variation of these estimates across all runs

```{r}
#| eval: false
  set.seed(1)
  # number of researchers
  nsim = 200
  # number of observations
  n = 100
  
  # lambda
  lambda = 0.3
  
  # save all estimated variance in a vector 
  allridgebeta = matrix(NA, nsim, 2)
  allolsbeta = matrix(NA, nsim, 2)
  
  for (i in 1:nsim)
  {
    # create highly correlated variables and a linear model
    X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2))
    y = rnorm(n, mean = X[,1] + X[,2])
    
    allridgebeta[i, ] = solve(t(X) %*% X + lambda*n*diag(2)) %*% t(X) %*% y
    allolsbeta[i, ] = solve(t(X) %*% X) %*% t(X) %*% y
  }

  # compare the mean of these estimates
  colMeans(allridgebeta)
  colMeans(allolsbeta)
  
  # compare the var of these estimates
  apply(allridgebeta, 2, var)
  apply(allolsbeta, 2, var)
```

::: notes

:::



## The Bias-variance Tradeoff
  * As $\lambda \downarrow$, bias $\downarrow$ and variance $\uparrow$
  * As $\lambda \uparrow$, bias $\uparrow$ and variance $\downarrow$


```{r}
#| out-width: "100%"
#| fig-asp: 0.3
    par(mfrow = c(1, 3))
par(mar = c(2, 2, 2, 0))
  # the truth
  plot(1, 1, xlim = c(-1, 3), ylim = c(-1, 3), xlab = "", ylab = "", las = 1,
       pch = 19, col = "red", cex = 2, main = "OLS lambda = 0",
       cex.main = 2)
  
  # generate many datasets in a simulation 
  for (i in 1:200)
  {
    X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2))
    y = rnorm(n, mean = X[,1] + X[,2])
    
    betahat <- coef(lm(y~X-1))
    points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 0.5)
    points(1, 1, pch = 19, col = "red", cex = 2)
  }
  
  # the truth
  plot(1, 1, xlim = c(-1, 3), ylim = c(-1, 3), xlab = "", ylab = "", las = 1,
       pch = 19, col = "red", cex = 2, main = "n*lambda = 2",
       cex.main = 2)
  
  # generate many datasets in a simulation 
  for (i in 1:200)
  {
    X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2))
    y = rnorm(n, mean = X[,1] + X[,2])
    
    betahat <- lm.ridge(y ~ X - 1, lambda = 2)$coef
    points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 0.5)
  }
  
  points(1, 1, pch = 19, col = "red", cex = 2)
  
  # a plot
  plot(1, 1, xlim = c(-1, 3), ylim = c(-1, 3), xlab = "", ylab = "", las = 1,
       pch = 19, col = "red", cex = 2, main = "n*lambda = 30",
       cex.main = 2)
  
  # generate many datasets in a simulation 
  for (i in 1:200)
  {
    X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2))
    y = rnorm(n, mean = X[,1] + X[,2])
    
    # betahat <- solve(t(X) %*% X + 30*diag(2)) %*% t(X) %*% y
    betahat <- lm.ridge(y ~ X - 1, lambda = 30)$coef
    points(betahat[1], betahat[2], pch = 19, col = "blue", cex = 0.5)
  }
  
  points(1, 1, pch = 19, col = "red", cex = 2)
```





::: notes
This effect is gradually changing as we increase the penalty level. The following simulation shows how the variation of $\bbeta$ changes. We show this with two penalty values, and see how the estimated parameters are away from the truth. 

Now, we may ask the question: is it worth it? In fact, this bias and variance will be then carried to the predicted values $x^\T \widehat{\bbeta}^\text{ridge}$. Hence, we can judge if this is beneficial from the prediction accuracy. And we need some procedure to do this. 

__Remark__: The bias-variance trade-off will appear frequently in this course. And the way to evaluate the benefit of this is to see if it eventually reduces the prediction error ($\text{Bias}^2 + \text{Variance}$ plus a term called __irreducible error__, which will be introduced in later chapter). 
:::

## Lower Test MSE



:::: {.columns}

::: {.column width="38%"}

- When $b_j$ has large variance or $p > n$, ridge regression could have lower test MSE and better predictive performance.

- [$\text{MSE}_{\texttt{Tr}}$ (purple)]{.purple}

- **Squared bias (black)**

- [**Variance (green)**]{.green}



:::

::: {.column width="62%"}
::: small

```{r}
#| out-width: "100%"
#| fig-cap: "Source: ISL Figure 6.5"
#| fig-align: left
knitr::include_graphics("./images/05-ridge-cv/6_5.png")
```

:::
:::
::::


## `MASS::lm.ridge()`

- The `lambda` parameter in `lm.ridge()` specifies the $n\lambda$ in our notation.

- OLS is *scale equivalent*: $X_jb_j$ remains the same regardless of how $X_j$ is scaled.

- Ridge coefficient estimates can *change substantially* when multiplying a given predictor by a constant, i.e., $X_j\hat{\beta}^{r}_{j, \lambda}$ depends on $\lambda$, the scaling of $X_j$, and even the scaling of *other* predictors.

- [__Standardize all predictors!__]{.green}^[In practice, if the intercept is not our interest, we also standardize the response.]

. . .


```{r}
#| echo: true
head(mtcars, 3)
(fit <- MASS::lm.ridge(mpg ~ ., data = mtcars, lambda = 1)) ## ridge fit
```




::: notes

:::

## Scaling Issues of `lm.ridge()`

- `coef(fit)` transforms these back to the *original* scale. 

- `fit$coef` shows the coefficients of the standardized predictors.


```{r}
#| echo: true
coef(fit)
fit$coef
```


## Scaling Issues of `lm.ridge()`

```{r}
#| echo: true
fit$coef
```

- `lm.ridge()` uses $n$ instead of $n-1$ when calculating the standard deviation.

```{r}
#| echo: true
# each column has mean 0 and var 1
X <- scale(data.matrix(mtcars[, -1]), center = TRUE, scale = TRUE)

# center y but not scaling
y <- scale(mtcars$mpg, center = TRUE, scale = FALSE)
```


<br>

```{r}
#| echo: true

# lambda = 1
my_ridge_coef <- solve(t(X) %*% X + diag(ncol(X))) %*% t(X) %*% y
t(my_ridge_coef)

```

## Scaling Issues of `lm.ridge()`


```{r}
#| echo: true
fit$coef
```

<br>

```{r}
#| echo: true
# use n instead of (n-1) for standardization
n <- nrow(X); X <- X * sqrt(n / (n - 1))
```


```{r}
my_ridge_coef <- solve(t(X) %*% X + diag(ncol(X))) %*% t(X) %*% y
t(my_ridge_coef)
```

::: notes
More discussion at https://stats.stackexchange.com/questions/288754/lm-ridge-returns-different-results-that-are-from-manual-calculation
:::


## Ridge Trace

```{r, echo=-1}
#| out-width: "100%"
par(mar = c(4, 4, 0, 0))
ridge_fit <- lm.ridge(mpg ~ ., data = mtcars, lambda = 0:40)
matplot(coef(ridge_fit)[, -1], type = "l", xlab = "Lambda", ylab = "Coefficients")
text(rep(1, 10), coef(ridge_fit)[1,-1], colnames(mtcars)[2:11])
```

- Select a value of $\lambda$ at which the ridge estimates are stable.



::: notes

:::




## Methods for Choosing $\lambda$
```{r}
#| echo: true
MASS::select(ridge_fit)
```

- Hoerl, Kennard, and Baldwin (1975): $\lambda \approx \frac{p \hat{\sigma}^2}{{\bf b}'{\bf b}}$

- Lawless and Wang (1976): $\lambda \approx \frac{np \hat{\sigma}^2}{{\bf b'X}'{\bf Xb}}$

- [Golub, Health, and Wahba (1979)](https://pages.stat.wisc.edu/~wahba/ftp1/oldie/golub.heath.wahba.pdf): Generalized Cross Validation


## Cross Validation

- Cross Validation (CV) is a **resampling** method. 

- Resampling methods refit a model of interest to data sampled from the training set.

- CV can be used to 
  + estimate the test error when there is no test data. _(Model assessment)_
  + select the tuning parameters that controls the model complexity/flexibility. _(Model selection)_




::: notes
Hence, by adding this to the OLS objective function, the solution is more stable, in the sense that each time we observe a new set of data, this contour looks pretty much the same. This may be interpreted in several different ways such as: 1) the objective function is more convex and less affected by the random samples; 2) the variance of the estimator is smaller because the eigenvalues of $\bX^\T \bX + n \lambda \bI$ are large. 
:::


## 

::: {.algo} 

**$k$-Fold Cross Validation**

1. Randomly divide the training set into $k$ *folds*, of approximately equal size.

2. Use 1 fold for validation to compute MSE, and the remaining $k - 1$ partitions for training.

3. Repeat $k$ times. Each time a different fold is treated as a validation set.

4. Average $k$ metrics, $\text{MSE}_{CV} = \frac{1}{k}\sum_{i=1}^k\text{MSE}_i$.

5. Use the CV estimate $\text{MSE}_{CV}$ to select the "best" tuning parameter.
:::



::: small

```{r}
#| out-width: 45%
#| fig-cap: "Five-fold cross validation. Source: Data science in a box"
knitr::include_graphics("./images/05-ridge-cv/cross-validation.png")
```

:::

::: notes
Can compute other performance measures.
:::

## $k$-Fold Cross Validation in `R` {visibility="hidden"}

- [`caret`](https://topepo.github.io/caret/index.html) package: **C**lassification **A**nd **RE**gression **T**raining ^[Be careful that not all models are available in `caret`. `caret` do not use `MASS::lm.ridge`, but the `elasticnet` package to fit ridge regression.]


```{r}
#| eval: false
library(caret)
library(elasticnet)

# set cross-validation type
ctrl <- caret::trainControl(method = "cv", number = 10)
    
# set tuning parameter range
lambdaGrid <- data.frame("lambda" = seq(0, 0.4, length = 20))
    
# perform cross-validation
ridge_cvfit <- caret::train(x = mtcars[, -1], mtcars$mpg,
                            method = ,
                            tuneGrid = lambdaGrid,
                            trControl = ctrl,
                            ## center and scale predictors
                            preProc = c("center", "scale"))
ridge_cvfit
```



::: notes
Cross-validation can be setup using the `caret` package. However, you should be careful that not all models are available in `caret`, and you should always check the documentation to see how to implement them. For example, if you use `method = "ridge"`, they do not use `lm.ridge` to fit the model, instead, they use a package called `elasticnet`, which can do the same job. However, the definition of parameters may vary. Hence, it is always good to check the main [help pages](https://topepo.github.io/caret/) for the package. We will use the caret package later for other models. 
https://rsample.tidymodels.org/reference/vfold_cv.html
:::

## [`glmnet`](https://glmnet.stanford.edu/index.html) package  ![](images/05-ridge-cv/glmnet_logo.png){.absolute left="450" top="0" width="70"}

- The parameter `alpha` controls the ridge (`alpha = 0`) and lasso (`alpha = 1`) penalties.

- Supply a *decreasing* sequence of `lambda` values.

- `lm.ridge()` use $SS_{res}$ and $n\lambda$, while `glmnet()` use $\text{MSE}_{\texttt{Tr}}$ and $\lambda$.

- Argument `x` should be a matrix.


:::: columns
::: column
```{r}
#| echo: true
#| eval: false
glmnet(x = data.matrix(mtcars[, -1]), 
       y = mtcars$mpg, 
       alpha = 0, 
       lambda = 5:0/nrow(mtcars))
```
:::


::: column
```{r}
#| echo: true
#| eval: false
lm.ridge(formula = mpg ~ ., 
         data = mtcars, 
         lambda = 5:0/nrow(mtcars))
```
:::


:::{.callout-note style="font-size: 1.2em"}
- `lm.ridge()` and `glmnet()` coefficients do not match exactly, specially when transforming back to original scale.
- No need to worry too much as we focus on predictive performance.
:::

::::



::: notes
https://stats.stackexchange.com/questions/74206/ridge-regression-results-different-in-using-lm-ridge-and-glmnet
:::



## 

```{r}
#| fig-asp: 0.4
X <- scale(data.matrix(mtcars[, -1]), center = TRUE, scale = TRUE)
y <- scale(mtcars$mpg, center = TRUE, scale = TRUE)
lam = 10^seq(-1, 3, 0.1)

fit2 <- glmnet(X, y, alpha = 0, lambda = rev(lam) / nrow(X))
# X <- X * sqrt(n / (n - 1))
fit1 <- lm.ridge(y ~ X, lambda = lam)
par(mar = c(2, 2, 2, 1))
par(mfrow = c(1, 2))
matplot(coef(fit1), type = "l", main = "Standardized lm.ridge", ylab = "")
matplot(apply(t(as.matrix(coef(fit2))), 2, rev), type = "l", 
        main = "Standardized glmnet",
        ylab = "")
```


##

```{r}
#| fig-asp: 0.4
ridge_lm <- MASS::lm.ridge(mpg ~ ., data = mtcars, lambda = 0:500)
ridge_net <- glmnet(data.matrix(mtcars[, -1]), mtcars$mpg, alpha = 0, 
       lambda = 500:0 / nrow(mtcars))
par(mar = c(2, 2, 2, 1))
par(mfrow = c(1, 2))
matplot(coef(ridge_lm), type = "l", main = "Original-scale lm.ridge",
        ylab = "")
matplot(apply(t(as.matrix(coef(ridge_net))), 2, rev), type = "l", 
        main = "Original-scale glmnet", ylab = "")

```



## $k$-Fold Cross Validation using `cv.glmnet()`^[There are other ways to do CV for ridge regression in `R`, for example, the [`caret`](https://topepo.github.io/caret/) (**C**lassification **A**nd **RE**gression **T**raining) package and the [`rsample`](https://rsample.tidymodels.org/reference/vfold_cv.html) package in [`tidymodels`](https://www.tidymodels.org/) ecosystem.]

- The $\lambda$ values are automatically selected, on the *$\log_{e}$* scale.

```{r}
#| echo: true
#| out-width: "50%"
ridge_cv_fit <- cv.glmnet(x = data.matrix(mtcars[, -1]), y = mtcars$mpg, alpha = 0,
                          nfolds = 10, type.measure = "mse")
plot(ridge_cv_fit$glmnet.fit, "lambda")
```



::: notes
- Why s and not lambda? In case we want to allow one to specify the model size in other ways in the future.
s: Value(s) of the penalty parameter lambda at which predictions are required. Default is the entire sequence used to create the model.

:::



## Determine $\lambda$

:::: columns
::: column


```{r}
#| echo: true
#| out-width: "100%"
plot(ridge_cv_fit)
```

:::


::: column
```{r}
#| echo: true

ridge_cv_fit$lambda.min
# largest lambda s.t. error is within 1 s.e of the min
ridge_cv_fit$lambda.1se 
coef(ridge_cv_fit, s = "lambda.min")
```
:::
::::


::: notes
coef(fit2, s = "lambda.1se")
This plots the cross-validation curve (red dotted line) along with upper and lower standard deviation curves along the 𝜆
 sequence (error bars). Two special values along the 𝜆
 sequence are indicated by the vertical dotted lines. lambda.min is the value of 𝜆
 that gives minimum mean cross-validated error, while lambda.1se is the value of 𝜆
 that gives the most regularized model such that the cross-validated error is within one standard error of the minimum.
:::



## Generalized Cross-Validation
- The generalized cross-validation (GCV) is a modified version of the **leave-one-out CV** ($n$-fold CV).

- The GCV criterion is given by $$\text{GCV}(\lambda) = \frac{1}{n}\sum_{i=1}^n \left[ \frac{y_i - x_i' \widehat{\bbeta}^\text{r}_\lambda}{1 - \frac{\Trace(\bS_\lambda)}{n}} \right]^2$$

where $\bS_\lambda$ is the _hat matrix_ corresponding to the ridge regression:

$$\bS_\lambda = \bX (\bX' \bX + n\lambda \bI)^{-1} \bX'$$




::: notes
The interesting fact about leave-one-out CV in the linear regression setting is that we do not need to explicitly fit all leave-one-out models.

- ESL p. 244

- lm.ridge code
:::


## Generalized Cross-Validation

:::: columns 
::: {.column width="49%"}

```{r, echo=-1}
#| out-width: "100%"
par(mar = c(4, 4, 0, 0))
plot(ridge_fit$lambda, 
     ridge_fit$GCV, 
     type = "l", col = "darkgreen", 
     ylab = "GCV", xlab = "Lambda", 
     lwd = 3)
```
:::

::: {.column width="49%"}

```{r}
#| echo: true
idx <- which.min(ridge_fit$GCV)
ridge_fit$lambda[idx]
round(coef(ridge_fit)[idx, ], 2)
```
:::
::::


Select the best $\lambda$ that produces the smallest GCV.


::: notes
You can clearly see that the GCV decreases initially, as $\lambda$ increases, this is because the reduced variance is more beneficial than the increased bias. However, as $\lambda$ increases further, the bias term will eventually dominate and causing the overall prediction error to increase. The fitted MSE under this model is 

```{r}
#| eval: false
  pred1 = data.matrix(cbind(1, mtcars[, -1])) %*% coef(fit1)[which.min(fit1$GCV), ]
  mean((pred1 - mtcars$mpg)^2)
```
:::




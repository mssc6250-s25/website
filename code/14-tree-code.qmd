---
title: "14 - Tree Methods Code Demo"
author: 'Dr. Cheng-Han Yu'
format: 
  html:
    toc: true
    code-link: true
    code-fold: show
    # code-summary: "Show/Hide"
    code-tools: true
    number-sections: true
# filters: 
#   - include-code-files
---

# R implementation

```{r}
# generate some data 
set.seed(2025)
n <- 1000
x1 <- runif(n, -1, 1)
x2 <- runif(n, -1, 1)
y <- rbinom(n, size = 1, prob = ifelse((x1 + x2 > -0.5) & (x1 + x2 < 0.5) , 0.8, 0.2))
xgrid <- expand.grid(x1 = seq(-1, 1, 0.01), x2 = seq(-1, 1, 0.01))
```

## CART

### [rpart::rpart()](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf)

```{r}
#| fig-asp: 1
library(rpart)
rpart_fit <- rpart(as.factor(y) ~ x1 + x2, data = data.frame(x1, x2, y))
pred <- matrix(predict(rpart_fit, xgrid, type = "class") == 1, 201, 201)
contour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = "",
        lwd = 3)
points(xgrid, pch = ".", cex = 0.2, col = ifelse(pred, "lightblue", "pink"))
points(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt = "n", xaxt = "n")
box()
title(main = list("CART"))
```

```{r}
#| out-width: 100%
par(mar = c(0.5, 0, 0.5, 0))
plot(rpart_fit)
text(rpart_fit, cex = 0.8)
```

-   `rpart()` uses the 10-fold CV (`xval` in `rpart.control()`)
-   `cp` is the complexity parameter

```{r}
#| eval: false
#| echo: true
rpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, 
              maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10,
              surrogatestyle = 0, maxdepth = 30, ...)
```

```{r}
rpart_fit$cptable
```

```{r}
plotcp(rpart_fit)
```

```{r}
prunedtree <- prune(rpart_fit, cp = 0.012)
prunedtree
```

```{r}
rpart.plot::rpart.plot(prunedtree)
```

### tree::tree()

Read ISLR Sec 8.3 for `tree()` demo.

## Bagging

```{r}
#| fig-asp: 1
library(ipred)
bag_fit <- bagging(as.factor(y) ~ x1 + x2, data = data.frame(x1, x2, y), 
                   nbagg = 200, ns = 400)
pred <- matrix(predict(prune(bag_fit), xgrid) == 1, 201, 201)
contour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = "", 
        lwd = 3)
points(xgrid, pch = ".", cex = 0.2, col = ifelse(pred, "lightblue", "pink"))
points(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt = "n", xaxt = "n")
box()
title(main = list("Bagging"))
```

## Random Forests

```{r}
#| fig-asp: 1
library(randomForest)
rf_fit <- randomForest(cbind(x1, x2), as.factor(y), ntree = 200, mtry = 1, 
                      nodesize = 20, sampsize = 400)
pred <- matrix(predict(rf_fit, xgrid) == 1, 201, 201)
contour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = "",
        lwd = 3)
points(xgrid, pch = ".", cex = 0.2, col = ifelse(pred, "lightblue", "pink"))
points(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt="n", xaxt = "n")
box()
title(main = list("Random Forests", cex = 2))
```

## Boosting

```{r}
#| fig-asp: 1
library(gbm)
gbm_fit <- gbm(y ~ ., data = data.frame(x1, x2, y), distribution = "bernoulli", 
               n.trees = 10000, shrinkage = 0.01, bag.fraction = 0.6, 
               interaction.depth = 2, cv.folds = 10)
usetree <- gbm.perf(gbm_fit, method = "cv", plot.it = FALSE)
Fx <- predict(gbm_fit, xgrid, n.trees=usetree)
pred <- matrix(1 / (1 + exp(-2 * Fx)) > 0.5, 201, 201)
contour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels = 0.5, labels = "",
        lwd = 3)
points(xgrid, pch = ".", cex = 0.2, col = ifelse(pred, "lightblue", "pink"))
points(x1, x2, col = ifelse(y == 1, 4, 2), pch = 19, yaxt = "n", xaxt = "n")
box()
title(main = list("Boosting"))
```

# Python implementation

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
```

```{python}
np.random.seed(2025)
n = 1000
x1 = np.random.uniform(-1, 1, n)
x2 = np.random.uniform(-1, 1, n)
y = np.random.binomial(1, p = np.where((x1 + x2 > -0.5) & (x1 + x2 < 0.5), 0.8, 0.2))
x1_grid, x2_grid = np.meshgrid(np.linspace(-1, 1, 201), np.linspace(-1, 1, 201))
xgrid = np.c_[x1_grid.ravel(), x2_grid.ravel()]
```

## CART

```{python}
#| fig-asp: 1
dt = DecisionTreeClassifier(random_state=2025)
param_grid = {
    "max_depth": [3, 5, 7, 10],  # Control tree depth
    "min_samples_split": [2, 5, 10],  # Minimum samples to split a node
    "min_samples_leaf": [1, 5, 10]  # Minimum samples in a leaf node
}
grid_search = GridSearchCV(dt, param_grid, cv=10, scoring="accuracy")
grid_search.fit(np.c_[x1, x2], y)

best_dt = grid_search.best_estimator_
best_pred = best_dt.predict(xgrid).reshape(201, 201)

plt.figure()
plt.contourf(x1_grid, x2_grid, best_pred, cmap="coolwarm", alpha=0.3)
plt.scatter(x1, x2, c=y, cmap="bwr", s=15, alpha=0.7)
plt.xlabel("x1")
plt.ylabel("x2")
plt.title("CART")
plt.show()
```

## Bagging

```{python}
from sklearn.ensemble import BaggingClassifier
```

```{python}
#| fig-asp: 1
bagging_clf = BaggingClassifier(
    estimator=DecisionTreeClassifier(),
    n_estimators=200,  # Same as nbagg = 200 in R
    max_samples=0.4,   # Equivalent to ns = 400 in R (approx)
    random_state=2025
)
bagging_clf.fit(np.c_[x1, x2], y)
bagging_pred = bagging_clf.predict(xgrid).reshape(201, 201)
plt.figure()
plt.contourf(x1_grid, x2_grid, bagging_pred, cmap="coolwarm", alpha=0.3)
plt.scatter(x1, x2, c=y, cmap="bwr", s=15, alpha=0.7)
plt.xlabel("x1")
plt.ylabel("x2")
plt.title("Bagging")
plt.show()
```

## Random Forests

```{python}
from sklearn.ensemble import RandomForestClassifier
```

```{python}
#| fig-asp: 1
rf_clf = RandomForestClassifier(
    n_estimators=200,  # Equivalent to ntree = 200 in R
    max_features=1,    # Equivalent to mtry = 1 in R
    min_samples_leaf=20,  # Equivalent to nodesize = 20 in R
    max_samples=400,  # Equivalent to sampsize = 400 in R
    random_state=2025
)
rf_clf.fit(np.c_[x1, x2], y)
rf_pred = rf_clf.predict(xgrid).reshape(201, 201)
plt.figure()
plt.contourf(x1_grid, x2_grid, rf_pred, cmap="coolwarm", alpha=0.3)
plt.scatter(x1, x2, c=y, cmap="bwr", s=15, alpha=0.7)
plt.xlabel("x1")
plt.ylabel("x2")
plt.title("Random Forests")
plt.show()
```

## Boosting

```{python}
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_val_score
```

```{python}
#| fig-asp: 1
gbm_clf = GradientBoostingClassifier(
    n_estimators=10000,  # Equivalent to n.trees = 10000
    learning_rate=0.01,  # Equivalent to shrinkage = 0.01
    subsample=0.6,       # Equivalent to bag.fraction = 0.6
    max_depth=2,         # Equivalent to interaction.depth = 2
    random_state=2025
)
cv_scores = []
for n_trees in range(100, 2000, 100):  # Testing tree counts
    gbm_clf.set_params(n_estimators=n_trees)
    score = np.mean(cross_val_score(gbm_clf, np.c_[x1, x2], y, cv=10, scoring='accuracy'))
    cv_scores.append((n_trees, score))

best_n_trees = max(cv_scores, key=lambda x: x[1])[0]
gbm_clf.set_params(n_estimators=best_n_trees)
gbm_clf.fit(np.c_[x1, x2], y)
Fx = gbm_clf.decision_function(xgrid)  # Raw output before sigmoid transformation
pred = (1 / (1 + np.exp(-2 * Fx)) > 0.5).reshape(201, 201)
plt.figure()
plt.contourf(x1_grid, x2_grid, pred, cmap="coolwarm", alpha=0.3)
plt.scatter(x1, x2, c=y, cmap="bwr", s=15, alpha=0.7)
plt.xlabel("x1")
plt.ylabel("x2")
plt.title("Gradient Boosting")
plt.show()
```

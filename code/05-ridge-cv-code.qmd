---
title: "05-Ridge Regression Code Demo"
author: 'Dr. Cheng-Han Yu'
format: 
  html:
    toc: true
    code-link: true
    code-fold: show
    code-tools: true
    number-sections: true
---

## R implementation

### MASS::lm.ridge()

```{r}
library(MASS)
fit <- MASS::lm.ridge(mpg ~ ., data = mtcars, lambda = 1) ## ridge fit
coef(fit) ## original scale
fit$coef ## standardized scale
```


```{r}
X <- scale(data.matrix(mtcars[, -1]), center = TRUE, scale = TRUE)
```


```{r}
df <- data.frame(cbind(mtcars[, 1, drop=FALSE], X))
ridge_fit <- lm.ridge(mpg ~ ., data = df, lambda = 0:40)
MASS::select(ridge_fit)
```

```{r}
par(mar = c(4, 4, 0, 0))
plot(ridge_fit$lambda, ridge_fit$GCV, type = "l", col = "darkgreen", 
     ylab = "GCV", xlab = "Lambda", lwd = 3)
```


### glmnet::glmnet()

By defualt, `glmnet()` standardizes the `x` variables with `standardize = TRUE`, and does not standardize the response (`standardize.response = FALSE`).

`glmnet()` always return the coefficients at the original scale.


#### Use standardized `X` as the original scale

```{r}
library(glmnet)
ridge_cv_fit <- cv.glmnet(x = X, y = mtcars$mpg, alpha = 0,
                          nfolds = 10, type.measure = "mse")
plot(ridge_cv_fit$glmnet.fit, "lambda")
```

```{r}
plot(ridge_cv_fit)
```

```{r}
ridge_cv_fit$lambda.min
# largest lambda s.t. error is within 1 s.e of the min
ridge_cv_fit$lambda.1se 
coef(ridge_cv_fit, s = "lambda.min")
```

#### Use non-standardized `X` as the original scale

```{r}
library(glmnet)
ridge_cv_fit_ori <- cv.glmnet(x = data.matrix(mtcars[, -1]), y = mtcars$mpg, alpha = 0,
                          nfolds = 10, type.measure = "mse")
plot(ridge_cv_fit_ori$glmnet.fit, "lambda")
```

```{r}
plot(ridge_cv_fit_ori)
```

```{r}
ridge_cv_fit_ori$lambda.min
# largest lambda s.t. error is within 1 s.e of the min
ridge_cv_fit_ori$lambda.1se 
coef(ridge_cv_fit_ori, s = "lambda.min")
```


## Python implementation

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge, RidgeCV
from sklearn.preprocessing import StandardScaler
```


```{python}
mtcars = pd.read_csv("../data/mtcars.csv")
X = mtcars.drop(columns=["mpg"])
y = mtcars["mpg"]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
ridge_model = Ridge(alpha=1) ## alpha is the n*lambda
ridge_model.fit(X_scaled, y)
ridge_model.coef_
```

```{python}
lambdas = np.arange(1, 41) ## alpha must be > 0
# Ridge regression with cross-validation to select the best lambda
# Enable storing CV values (can only use LOOCV)
ridge_cv = RidgeCV(alphas=lambdas, store_cv_values=True)  
ridge_cv.fit(X_scaled, y)

# Optimal lambda and corresponding coefficients
optimal_lambda = ridge_cv.alpha_
optimal_lambda
optimal_coefficients = ridge_cv.coef_
optimal_coefficients
ridge_cv.intercept_

# Cross-validation mean squared error for each lambda
cv_mse = np.mean(ridge_cv.cv_values_, axis=0)

# Plot the CV MSE vs Lambda
plt.plot(lambdas, cv_mse, marker="o", linestyle="-")
plt.axvline(optimal_lambda, color="red", linestyle="--", 
            label=f"Optimal Lambda = {optimal_lambda}")
plt.xlabel("Lambda (Alpha)")
plt.ylabel("Mean Squared Error (MSE)")
plt.title("Cross-Validation MSE vs Lambda")
plt.legend()
plt.show()
```


Here we transform the coefficients back to the original scale when non-standardized `X` is used.


```{python}
# Reverse standardization
std_devs = scaler.scale_  # Feature standard deviations
means = scaler.mean_      # Feature means

coef_original = optimal_coefficients / std_devs
coef_original
intercept_original = ridge_cv.intercept_ - np.sum(optimal_coefficients * means / std_devs)
intercept_original
```

```{python}
## the lambda size is matching the size used in cv.glmnet(). Should it be multiplied by 32?
lambdas = 5 * 10 ** np.linspace(-1, 3, 100)

# RidgeCV with 10-fold cross-validation
ridge_cv = RidgeCV(alphas=lambdas, scoring="neg_mean_squared_error", cv=10)
ridge_cv.fit(X_scaled, y)

best_lambda = ridge_cv.alpha_

coefficients = []
for lam in lambdas:
    ridge = Ridge(alpha=lam)
    ridge.fit(X_scaled, y)
    coefficients.append(ridge.coef_)

coefficients = np.array(coefficients)
for i in range(coefficients.shape[1]):
    plt.plot(np.log(lambdas), coefficients[:, i])
plt.xlabel("Log(Lambda)")
plt.ylabel("Coefficients")
plt.show()
```

